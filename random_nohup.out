/home/user/miniconda/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/home/user/miniconda/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
Arguments:
  f: 
  bert_config_file: ./mnt/bert/uncased_L-12_H-768_A-12/bert_config.json
  vocab_file: ./mnt/bert/uncased_L-12_H-768_A-12/vocab.txt
  init_checkpoint: ./11_turns_output/output/model_58000.ckpt
  output_dir: random_output/
  coqa_train_file: ./mnt/coqa_extractive_gt/coqa-train-v1.0.json
  coqa_predict_file: ./mnt/coqa_extractive_gt/coqa-dev-v1.0.json
  quac_train_file: ./mnt/quac_original/train_v0.2.json
  quac_predict_file: ./mnt/quac_original/val_v0.2.json
  do_lower_case: True
  max_seq_length: 384
  doc_stride: 128
  max_query_length: 64
  do_train: True
  do_predict: True
  train_batch_size: 12
  predict_batch_size: 12
  learning_rate: 3e-05
  num_train_epochs: 2.0
  warmup_proportion: 0.1
  save_checkpoints_steps: 1000
  evaluation_steps: 1000
  evaluate_after: 50000
  iterations_per_loop: 1000
  n_best_size: 20
  max_answer_length: 50
  max_answer_threshold: 40
  use_tpu: False
  tpu_name: None
  tpu_zone: None
  gcp_project: None
  master: None
  num_tpu_cores: 8
  verbose_logging: False
  history: 6
  use_new_attetion: True
  only_history_answer: False
  use_history_answer_marker: True
  load_small_portion: False
  use_RL: False
  dataset: quac
  max_history_turns: 11
  example_batch_size: 4
  cache_dir: cache_4_turns/
  max_considered_history_turns: 11
  train_steps: 58000
  better_hae: False
  history_selection: previous_j
  more_history: 0
  max_question_len_for_matching: 20
  max_answer_len_for_matching: 40
  glove: ./mnt/glove/glove.840B.300d.pkl
  embedding_dim: 300
  kernel_size: 3
  kernel_count: 16
  pool_size: 3
  rl_learning_rate: 0.0001
  MTL: False
  MTL_lambda: 0.1
  MTL_mu: 0.8
  ideal_selected_num: 1
  aux: False
  aux_lambda: 0.0
  aux_shared: False
  disable_attention: False
  history_attention_hidden: False
  history_attention_input: reduce_mean
  mtl_input: reduce_mean
  history_ngram: 1
  reformulate_question: False
  front_padding: False
  freeze_bert: False
  fine_grained_attention: True
  append_self: False
  null_score_diff_threshold: 0.0
  bert_hidden: 768
====
output_dir random_output/
attempting to load train features from cache
attempting to load val features from cache
Traceback (most recent call last):
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py", line 527, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py", line 527, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/compat.py", line 61, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got 12

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "cqa_run_his_atten.py", line 231, in <module>
    use_one_hot_embeddings=False
  File "/data/shibo/qa/attention/cqa_model.py", line 55, in bert_rep
    use_one_hot_embeddings=use_one_hot_embeddings)
  File "/data/shibo/qa/attention/modeling.py", line 221, in __init__
    dropout_prob=config.hidden_dropout_prob)
  File "/data/shibo/qa/attention/modeling.py", line 334, in new_attention
    tmp = tf.Variable(tf.random_uniform([12, dim2, dim3], minval=0.0, maxval=1.0, dtype=tf.float32, seed=1234))
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py", line 235, in random_uniform
    shape = _ShapeTensor(shape)
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py", line 44, in _ShapeTensor
    return ops.convert_to_tensor(shape, dtype=dtype, name="shape")
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1050, in convert_to_tensor
    as_ref=False)
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py", line 229, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py", line 208, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File "/home/user/miniconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py", line 531, in make_tensor_proto
    "supported type." % (type(values), values))
TypeError: Failed to convert object of type <class 'list'> to Tensor. Contents: [12, Dimension(384), Dimension(114)]. Consider casting elements to a supported type.
