2020-06-03 17:48:37.885823: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-06-03 17:48:38.945362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:3b:00.0
totalMemory: 31.72GiB freeMemory: 14.78GiB
2020-06-03 17:48:38.945417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-06-03 17:48:41.310037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-06-03 17:48:41.310082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-06-03 17:48:41.310100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-06-03 17:48:41.310532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12992 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2020-06-03 17:48:42.712934: W tensorflow/core/framework/allocator.cc:122] Allocation of 93763584 exceeds 10% of system memory.
2020-06-03 17:48:42.753705: W tensorflow/core/framework/allocator.cc:122] Allocation of 93763584 exceeds 10% of system memory.
2020-06-03 17:48:43.124885: W tensorflow/core/framework/allocator.cc:122] Allocation of 93763584 exceeds 10% of system memory.
2020-06-03 17:48:43.201572: W tensorflow/core/framework/allocator.cc:122] Allocation of 93763584 exceeds 10% of system memory.
2020-06-03 17:48:45.023967: W tensorflow/core/framework/allocator.cc:122] Allocation of 93763584 exceeds 10% of system memory.
Arguments:
  f: 
  bert_config_file: ./mnt/bert/uncased_L-12_H-768_A-12/bert_config.json
  vocab_file: ./mnt/bert/uncased_L-12_H-768_A-12/vocab.txt
  init_checkpoint: ./test_output/model_36000.ckpt
  output_dir: test_output/
  coqa_train_file: ./mnt/coqa_extractive_gt/coqa-train-v1.0.json
  coqa_predict_file: ./mnt/coqa_extractive_gt/coqa-dev-v1.0.json
  quac_train_file: ./mnt/quac_original/train_v0.2.json
  quac_predict_file: ./mnt/quac_original/val_v0.2.json
  do_lower_case: True
  max_seq_length: 384
  doc_stride: 128
  max_query_length: 64
  do_train: True
  do_predict: True
  train_batch_size: 12
  predict_batch_size: 12
  learning_rate: 3e-05
  num_train_epochs: 20.0
  warmup_proportion: 0.1
  save_checkpoints_steps: 1000
  evaluation_steps: 1000
  evaluate_after: 1000
  iterations_per_loop: 1000
  n_best_size: 20
  max_answer_length: 50
  max_answer_threshold: 40
  use_tpu: False
  tpu_name: None
  tpu_zone: None
  gcp_project: None
  master: None
  num_tpu_cores: 8
  verbose_logging: False
  history: 6
  use_new_attetion: True
  only_history_answer: False
  use_history_answer_marker: True
  load_small_portion: False
  use_RL: False
  dataset: quac
  max_history_turns: 11
  example_batch_size: 4
  cache_dir: cache/
  max_considered_history_turns: 11
  train_steps: 22000
  better_hae: False
  history_selection: previous_j
  more_history: 0
  max_question_len_for_matching: 20
  max_answer_len_for_matching: 40
  glove: ./mnt/glove/glove.840B.300d.pkl
  embedding_dim: 300
  kernel_size: 3
  kernel_count: 16
  pool_size: 3
  rl_learning_rate: 0.0001
  MTL: False
  MTL_lambda: 0.1
  MTL_mu: 0.8
  ideal_selected_num: 1
  aux: False
  aux_lambda: 0.0
  aux_shared: False
  disable_attention: False
  history_attention_hidden: False
  history_attention_input: reduce_mean
  mtl_input: reduce_mean
  history_ngram: 1
  reformulate_question: False
  front_padding: False
  freeze_bert: False
  fine_grained_attention: True
  append_self: False
  null_score_diff_threshold: 0.0
  bert_hidden: 768
====
output_dir test_output/
attempting to load train features from cache
attempting to load val features from cache
***** Running training *****
  Num orig examples = %d 83568
  Num train_features = %d 440164
  Batch size = %d 12
  Num steps = %d 22000
training step: 36000, total_loss: 3.689951181411743
training step: 36001, total_loss: 2.254359245300293
training step: 36002, total_loss: 2.3971128463745117
training step: 36003, total_loss: 2.00838041305542
training step: 36004, total_loss: 2.9183156490325928
training step: 36005, total_loss: 2.4933645725250244
training step: 36006, total_loss: 2.088991641998291
training step: 36007, total_loss: 2.2158823013305664
training step: 36008, total_loss: 4.272381782531738
training step: 36009, total_loss: 3.5150530338287354
training step: 36010, total_loss: 1.5091416835784912
training step: 36011, total_loss: 2.683159112930298
training step: 36012, total_loss: 2.925283670425415
training step: 36013, total_loss: 3.0598816871643066
training step: 36014, total_loss: 0.925027072429657
training step: 36015, total_loss: 3.6600658893585205
training step: 36016, total_loss: 1.6737909317016602
training step: 36017, total_loss: 1.533137559890747
training step: 36018, total_loss: 2.339216470718384
training step: 36019, total_loss: 1.7948625087738037
training step: 36020, total_loss: 1.852309226989746
training step: 36021, total_loss: 3.410130262374878
training step: 36022, total_loss: 3.646559476852417
training step: 36023, total_loss: 2.8587615489959717
training step: 36024, total_loss: 3.582033395767212
training step: 36025, total_loss: 3.264294385910034
training step: 36026, total_loss: 2.495227813720703
training step: 36027, total_loss: 2.271542549133301
training step: 36028, total_loss: 1.1119099855422974
training step: 36029, total_loss: 3.1035263538360596
training step: 36030, total_loss: 3.1566219329833984
training step: 36031, total_loss: 1.0700300931930542
training step: 36032, total_loss: 0.43113887310028076
training step: 36033, total_loss: 2.4058947563171387
training step: 36034, total_loss: 4.563845634460449
training step: 36035, total_loss: 2.438176393508911
training step: 36036, total_loss: 2.9610629081726074
training step: 36037, total_loss: 0.9990127086639404
training step: 36038, total_loss: 1.5935121774673462
training step: 36039, total_loss: 3.421154022216797
training step: 36040, total_loss: 1.6258115768432617
training step: 36041, total_loss: 2.912675619125366
training step: 36042, total_loss: 3.6753227710723877
training step: 36043, total_loss: 2.210341691970825
training step: 36044, total_loss: 1.4456369876861572
training step: 36045, total_loss: 3.0798745155334473
training step: 36046, total_loss: 2.0105855464935303
training step: 36047, total_loss: 2.669708728790283
training step: 36048, total_loss: 4.424440860748291
training step: 36049, total_loss: 1.7090122699737549
training step: 36050, total_loss: 3.9178762435913086
training step: 36051, total_loss: 2.194888114929199
training step: 36052, total_loss: 4.084534168243408
training step: 36053, total_loss: 3.4944000244140625
training step: 36054, total_loss: 1.6143856048583984
training step: 36055, total_loss: 3.831111431121826
training step: 36056, total_loss: 1.379145860671997
training step: 36057, total_loss: 2.6105966567993164
training step: 36058, total_loss: 2.7829062938690186
training step: 36059, total_loss: 2.8710105419158936
training step: 36060, total_loss: 3.8968896865844727
training step: 36061, total_loss: 4.48382043838501
training step: 36062, total_loss: 3.3340585231781006
training step: 36063, total_loss: 1.4363515377044678
training step: 36064, total_loss: 1.1126480102539062
training step: 36065, total_loss: 2.9796996116638184
training step: 36066, total_loss: 2.5326828956604004
training step: 36067, total_loss: 3.001997232437134
training step: 36068, total_loss: 1.5141334533691406
training step: 36069, total_loss: 0.5204175710678101
training step: 36070, total_loss: 2.3435842990875244
training step: 36071, total_loss: 2.6369271278381348
training step: 36072, total_loss: 2.393906593322754
training step: 36073, total_loss: 1.6718720197677612
training step: 36074, total_loss: 4.5119099617004395
training step: 36075, total_loss: 2.7230615615844727
training step: 36076, total_loss: 2.7317724227905273
training step: 36077, total_loss: 4.4396071434021
training step: 36078, total_loss: 2.0445573329925537
training step: 36079, total_loss: 6.175556659698486
training step: 36080, total_loss: 2.307887315750122
training step: 36081, total_loss: 2.018197536468506
training step: 36082, total_loss: 2.41923451423645
training step: 36083, total_loss: 3.0542538166046143
training step: 36084, total_loss: 0.6571362018585205
training step: 36085, total_loss: 2.0058786869049072
training step: 36086, total_loss: 5.886635780334473
training step: 36087, total_loss: 1.6021208763122559
training step: 36088, total_loss: 1.906296730041504
training step: 36089, total_loss: 1.4143321514129639
training step: 36090, total_loss: 3.2532076835632324
training step: 36091, total_loss: 1.1189093589782715
training step: 36092, total_loss: 2.993940830230713
training step: 36093, total_loss: 2.299398899078369
training step: 36094, total_loss: 2.4976654052734375
training step: 36095, total_loss: 1.036102533340454
training step: 36096, total_loss: 2.715851068496704
training step: 36097, total_loss: 2.479612112045288
training step: 36098, total_loss: 1.1283185482025146
training step: 36099, total_loss: 4.355907917022705
training step: 36100, total_loss: 1.7888847589492798
training step: 36101, total_loss: 2.709770917892456
training step: 36102, total_loss: 2.0640511512756348
training step: 36103, total_loss: 1.9725037813186646
training step: 36104, total_loss: 2.5677502155303955
training step: 36105, total_loss: 1.2895870208740234
training step: 36106, total_loss: 1.7696551084518433
training step: 36107, total_loss: 3.0002896785736084
training step: 36108, total_loss: 1.381070613861084
training step: 36109, total_loss: 3.5414276123046875
training step: 36110, total_loss: 2.4780120849609375
training step: 36111, total_loss: 2.5425071716308594
training step: 36112, total_loss: 5.220196723937988
training step: 36113, total_loss: 1.8289331197738647
training step: 36114, total_loss: 1.0396783351898193
training step: 36115, total_loss: 2.3709325790405273
training step: 36116, total_loss: 1.7408418655395508
training step: 36117, total_loss: 2.848595380783081
training step: 36118, total_loss: 1.127790927886963
training step: 36119, total_loss: 1.4472429752349854
training step: 36120, total_loss: 2.075725555419922
training step: 36121, total_loss: 1.5252691507339478
training step: 36122, total_loss: 0.2199784219264984
training step: 36123, total_loss: 4.4680681228637695
training step: 36124, total_loss: 2.757573366165161
training step: 36125, total_loss: 2.6262850761413574
training step: 36126, total_loss: 2.877427577972412
training step: 36127, total_loss: 3.4413325786590576
training step: 36128, total_loss: 3.0663461685180664
training step: 36129, total_loss: 3.0672969818115234
training step: 36130, total_loss: 2.1275386810302734
training step: 36131, total_loss: 1.6145284175872803
training step: 36132, total_loss: 0.9086878299713135
training step: 36133, total_loss: 4.20412540435791
training step: 36134, total_loss: 1.5023694038391113
training step: 36135, total_loss: 2.7853150367736816
training step: 36136, total_loss: 2.343784809112549
training step: 36137, total_loss: 1.309786319732666
training step: 36138, total_loss: 3.52506685256958
training step: 36139, total_loss: 2.907534122467041
training step: 36140, total_loss: 4.159799098968506
training step: 36141, total_loss: 2.7981374263763428
training step: 36142, total_loss: 1.9093363285064697
training step: 36143, total_loss: 2.015549421310425
training step: 36144, total_loss: 2.8474600315093994
training step: 36145, total_loss: 2.7118217945098877
training step: 36146, total_loss: 3.214874744415283
training step: 36147, total_loss: 1.7633448839187622
training step: 36148, total_loss: 1.478076457977295
training step: 36149, total_loss: 2.5450406074523926
training step: 36150, total_loss: 5.008663177490234
training step: 36151, total_loss: 2.079270839691162
training step: 36152, total_loss: 1.867783784866333
training step: 36153, total_loss: 2.5914487838745117
training step: 36154, total_loss: 0.6045677661895752
training step: 36155, total_loss: 1.391892671585083
training step: 36156, total_loss: 3.3778605461120605
training step: 36157, total_loss: 2.7209858894348145
training step: 36158, total_loss: 1.852615475654602
training step: 36159, total_loss: 2.6322546005249023
training step: 36160, total_loss: 1.92703378200531
training step: 36161, total_loss: 2.5732855796813965
training step: 36162, total_loss: 0.9922633171081543
training step: 36163, total_loss: 3.0139107704162598
training step: 36164, total_loss: 1.6527130603790283
training step: 36165, total_loss: 2.9100124835968018
training step: 36166, total_loss: 4.157079696655273
training step: 36167, total_loss: 2.7787370681762695
training step: 36168, total_loss: 0.773780345916748
training step: 36169, total_loss: 3.5696539878845215
training step: 36170, total_loss: 2.428299903869629
training step: 36171, total_loss: 1.4861080646514893
training step: 36172, total_loss: 2.6815006732940674
training step: 36173, total_loss: 1.9679515361785889
training step: 36174, total_loss: 3.068516969680786
training step: 36175, total_loss: 4.1158223152160645
training step: 36176, total_loss: 1.4445788860321045
training step: 36177, total_loss: 3.5482540130615234
training step: 36178, total_loss: 3.9810128211975098
training step: 36179, total_loss: 1.4394330978393555
training step: 36180, total_loss: 4.039095878601074
training step: 36181, total_loss: 3.7499165534973145
training step: 36182, total_loss: 2.134594440460205
training step: 36183, total_loss: 1.367550253868103
training step: 36184, total_loss: 3.6796650886535645
training step: 36185, total_loss: 2.927185535430908
training step: 36186, total_loss: 2.015186071395874
training step: 36187, total_loss: 2.350594997406006
training step: 36188, total_loss: 2.6771578788757324
training step: 36189, total_loss: 3.3122832775115967
training step: 36190, total_loss: 2.2671966552734375
training step: 36191, total_loss: 0.35661226511001587
training step: 36192, total_loss: 2.8947319984436035
training step: 36193, total_loss: 2.59625244140625
training step: 36194, total_loss: 4.266794681549072
training step: 36195, total_loss: 0.4831613004207611
training step: 36196, total_loss: 3.3969521522521973
training step: 36197, total_loss: 0.4776436984539032
training step: 36198, total_loss: 1.1725753545761108
training step: 36199, total_loss: 3.9897847175598145
training step: 36200, total_loss: 2.4396708011627197
training step: 36201, total_loss: 3.0480120182037354
training step: 36202, total_loss: 3.453767776489258
training step: 36203, total_loss: 1.5170555114746094
training step: 36204, total_loss: 3.9571006298065186
training step: 36205, total_loss: 0.5958765149116516
training step: 36206, total_loss: 2.7288265228271484
training step: 36207, total_loss: 0.8445285558700562
training step: 36208, total_loss: 1.4947726726531982
training step: 36209, total_loss: 6.026363372802734
training step: 36210, total_loss: 9.973538398742676
training step: 36211, total_loss: 4.770793437957764
training step: 36212, total_loss: 1.7651000022888184
training step: 36213, total_loss: 2.552433729171753
training step: 36214, total_loss: 2.546497344970703
training step: 36215, total_loss: 1.5299118757247925
training step: 36216, total_loss: 1.937442421913147
training step: 36217, total_loss: 1.8152198791503906
training step: 36218, total_loss: 2.3996498584747314
training step: 36219, total_loss: 2.672609329223633
training step: 36220, total_loss: 3.6293439865112305
training step: 36221, total_loss: 2.3766796588897705
training step: 36222, total_loss: 7.797572135925293
training step: 36223, total_loss: 1.7945141792297363
training step: 36224, total_loss: 2.441826820373535
training step: 36225, total_loss: 1.0897548198699951
training step: 36226, total_loss: 3.6260390281677246
training step: 36227, total_loss: 0.7344040870666504
training step: 36228, total_loss: 3.281525135040283
training step: 36229, total_loss: 3.854585647583008
training step: 36230, total_loss: 1.9367594718933105
training step: 36231, total_loss: 2.6691465377807617
training step: 36232, total_loss: 1.4883390665054321
training step: 36233, total_loss: 2.3634090423583984
training step: 36234, total_loss: 2.2404286861419678
training step: 36235, total_loss: 5.411689281463623
training step: 36236, total_loss: 2.2329537868499756
training step: 36237, total_loss: 4.926730632781982
training step: 36238, total_loss: 1.5165512561798096
training step: 36239, total_loss: 3.775942802429199
training step: 36240, total_loss: 1.8584544658660889
training step: 36241, total_loss: 4.874962329864502
training step: 36242, total_loss: 2.8216185569763184
training step: 36243, total_loss: 3.2919278144836426
training step: 36244, total_loss: 2.6056787967681885
training step: 36245, total_loss: 6.251490592956543
training step: 36246, total_loss: 1.0966596603393555
training step: 36247, total_loss: 4.176746368408203
training step: 36248, total_loss: 1.0558558702468872
training step: 36249, total_loss: 3.9684343338012695
training step: 36250, total_loss: 3.1558918952941895
training step: 36251, total_loss: 1.5297527313232422
training step: 36252, total_loss: 5.933727741241455
training step: 36253, total_loss: 3.299591541290283
training step: 36254, total_loss: 1.9581648111343384
training step: 36255, total_loss: 2.4599201679229736
training step: 36256, total_loss: 1.544366717338562
training step: 36257, total_loss: 0.9059227108955383
training step: 36258, total_loss: 2.280409336090088
training step: 36259, total_loss: 3.3899152278900146
training step: 36260, total_loss: 3.6601479053497314
training step: 36261, total_loss: 4.007673263549805
training step: 36262, total_loss: 3.4163472652435303
training step: 36263, total_loss: 2.068389892578125
training step: 36264, total_loss: 0.3798151910305023
training step: 36265, total_loss: 3.2248222827911377
training step: 36266, total_loss: 1.8448338508605957
training step: 36267, total_loss: 4.4016289710998535
training step: 36268, total_loss: 1.470157265663147
training step: 36269, total_loss: 1.5678510665893555
training step: 36270, total_loss: 2.886657953262329
training step: 36271, total_loss: 3.905700445175171
training step: 36272, total_loss: 2.205617904663086
training step: 36273, total_loss: 1.3474397659301758
training step: 36274, total_loss: 4.590977668762207
training step: 36275, total_loss: 3.0356483459472656
training step: 36276, total_loss: 2.6844754219055176
training step: 36277, total_loss: 4.938282489776611
training step: 36278, total_loss: 2.413702964782715
training step: 36279, total_loss: 2.1265974044799805
training step: 36280, total_loss: 2.788029670715332
training step: 36281, total_loss: 2.8299460411071777
training step: 36282, total_loss: 0.27544671297073364
training step: 36283, total_loss: 3.0313167572021484
training step: 36284, total_loss: 3.466278553009033
training step: 36285, total_loss: 2.781038761138916
training step: 36286, total_loss: 3.9139184951782227
training step: 36287, total_loss: 0.3371891379356384
training step: 36288, total_loss: 2.60935115814209
training step: 36289, total_loss: 2.6783077716827393
training step: 36290, total_loss: 2.4431846141815186
training step: 36291, total_loss: 2.17208194732666
training step: 36292, total_loss: 2.3572840690612793
training step: 36293, total_loss: 6.025487899780273
training step: 36294, total_loss: 2.6305994987487793
training step: 36295, total_loss: 2.5626420974731445
training step: 36296, total_loss: 3.903029441833496
training step: 36297, total_loss: 2.685903549194336
training step: 36298, total_loss: 2.4723188877105713
training step: 36299, total_loss: 2.146332263946533
training step: 36300, total_loss: 2.9147095680236816
training step: 36301, total_loss: 2.3072640895843506
training step: 36302, total_loss: 5.520475387573242
training step: 36303, total_loss: 2.9669418334960938
training step: 36304, total_loss: 2.155669689178467
training step: 36305, total_loss: 1.2987604141235352
training step: 36306, total_loss: 1.936094045639038
training step: 36307, total_loss: 1.6915245056152344
training step: 36308, total_loss: 3.996060848236084
training step: 36309, total_loss: 0.7604479789733887
training step: 36310, total_loss: 2.523178815841675
training step: 36311, total_loss: 3.7940850257873535
training step: 36312, total_loss: 2.3329663276672363
training step: 36313, total_loss: 3.097623348236084
training step: 36314, total_loss: 1.1112475395202637
training step: 36315, total_loss: 3.762324571609497
training step: 36316, total_loss: 1.4541618824005127
training step: 36317, total_loss: 2.8152918815612793
training step: 36318, total_loss: 0.6444704532623291
training step: 36319, total_loss: 1.3117808103561401
training step: 36320, total_loss: 2.9351577758789062
training step: 36321, total_loss: 2.9463891983032227
training step: 36322, total_loss: 3.584667682647705
training step: 36323, total_loss: 0.6490151882171631
training step: 36324, total_loss: 2.884998321533203
training step: 36325, total_loss: 4.25612735748291
training step: 36326, total_loss: 1.9174246788024902
training step: 36327, total_loss: 1.038184642791748
training step: 36328, total_loss: 1.9773526191711426
training step: 36329, total_loss: 5.8281660079956055
training step: 36330, total_loss: 0.8928377628326416
training step: 36331, total_loss: 1.315398097038269
training step: 36332, total_loss: 0.5310349464416504
training step: 36333, total_loss: 2.388138771057129
training step: 36334, total_loss: 1.9021806716918945
training step: 36335, total_loss: 2.0858774185180664
training step: 36336, total_loss: 0.06062735617160797
training step: 36337, total_loss: 5.017247200012207
training step: 36338, total_loss: 1.9165480136871338
training step: 36339, total_loss: 2.7451913356781006
training step: 36340, total_loss: 2.839480400085449
training step: 36341, total_loss: 3.150041103363037
training step: 36342, total_loss: 1.9581820964813232
training step: 36343, total_loss: 2.9590084552764893
training step: 36344, total_loss: 3.0748541355133057
training step: 36345, total_loss: 2.7283787727355957
training step: 36346, total_loss: 1.738026738166809
training step: 36347, total_loss: 3.789912223815918
training step: 36348, total_loss: 3.516756057739258
training step: 36349, total_loss: 1.9398777484893799
training step: 36350, total_loss: 2.4043843746185303
training step: 36351, total_loss: 2.033175468444824
training step: 36352, total_loss: 0.5767052173614502
training step: 36353, total_loss: 3.282728672027588
training step: 36354, total_loss: 1.2081202268600464
training step: 36355, total_loss: 4.800121784210205
training step: 36356, total_loss: 2.2688093185424805
training step: 36357, total_loss: 1.8395789861679077
training step: 36358, total_loss: 0.9084954857826233
training step: 36359, total_loss: 3.004240036010742
training step: 36360, total_loss: 3.4542288780212402
training step: 36361, total_loss: 1.881714105606079
training step: 36362, total_loss: 3.30319881439209
training step: 36363, total_loss: 4.174256324768066
training step: 36364, total_loss: 0.529171347618103
training step: 36365, total_loss: 4.202960968017578
training step: 36366, total_loss: 3.0925371646881104
training step: 36367, total_loss: 1.3100110292434692
training step: 36368, total_loss: 4.06342077255249
training step: 36369, total_loss: 3.1574599742889404
training step: 36370, total_loss: 3.1619253158569336
training step: 36371, total_loss: 3.147559404373169
training step: 36372, total_loss: 2.8759121894836426
training step: 36373, total_loss: 2.0138051509857178
training step: 36374, total_loss: 1.5927119255065918
training step: 36375, total_loss: 4.882525444030762
training step: 36376, total_loss: 2.9504477977752686
training step: 36377, total_loss: 1.7143850326538086
training step: 36378, total_loss: 3.0711917877197266
training step: 36379, total_loss: 2.412950038909912
training step: 36380, total_loss: 1.8380439281463623
training step: 36381, total_loss: 3.4421753883361816
training step: 36382, total_loss: 1.2197531461715698
training step: 36383, total_loss: 3.4489693641662598
training step: 36384, total_loss: 1.132502794265747
training step: 36385, total_loss: 2.8650014400482178
training step: 36386, total_loss: 2.679342031478882
training step: 36387, total_loss: 2.6201181411743164
training step: 36388, total_loss: 4.014899730682373
training step: 36389, total_loss: 1.0345683097839355
training step: 36390, total_loss: 1.5741828680038452
training step: 36391, total_loss: 2.0867748260498047
training step: 36392, total_loss: 0.9974335432052612
training step: 36393, total_loss: 2.073824882507324
training step: 36394, total_loss: 3.7177109718322754
training step: 36395, total_loss: 1.6385397911071777
training step: 36396, total_loss: 3.007939338684082
training step: 36397, total_loss: 1.5653889179229736
training step: 36398, total_loss: 2.2568836212158203
training step: 36399, total_loss: 0.7825295925140381
training step: 36400, total_loss: 1.220444679260254
training step: 36401, total_loss: 2.674346446990967
training step: 36402, total_loss: 4.459539890289307
training step: 36403, total_loss: 2.7746360301971436
training step: 36404, total_loss: 2.200911045074463
training step: 36405, total_loss: 0.8084346055984497
training step: 36406, total_loss: 2.7174506187438965
training step: 36407, total_loss: 2.1296615600585938
training step: 36408, total_loss: 2.3556535243988037
training step: 36409, total_loss: 3.3841800689697266
training step: 36410, total_loss: 2.2730116844177246
training step: 36411, total_loss: 3.0397708415985107
training step: 36412, total_loss: 2.8750855922698975
training step: 36413, total_loss: 1.6152021884918213
training step: 36414, total_loss: 0.8902701139450073
training step: 36415, total_loss: 0.40313974022865295
training step: 36416, total_loss: 2.7442164421081543
training step: 36417, total_loss: 1.9900263547897339
training step: 36418, total_loss: 2.601567506790161
training step: 36419, total_loss: 1.2163496017456055
training step: 36420, total_loss: 1.970337986946106
training step: 36421, total_loss: 3.080780029296875
training step: 36422, total_loss: 2.531327724456787
training step: 36423, total_loss: 3.0710678100585938
training step: 36424, total_loss: 1.858100175857544
training step: 36425, total_loss: 0.8127471208572388
training step: 36426, total_loss: 0.7810825109481812
training step: 36427, total_loss: 1.2336068153381348
training step: 36428, total_loss: 3.7985734939575195
training step: 36429, total_loss: 3.1747183799743652
training step: 36430, total_loss: 1.2880327701568604
training step: 36431, total_loss: 2.0088186264038086
training step: 36432, total_loss: 3.6339402198791504
training step: 36433, total_loss: 1.2013672590255737
training step: 36434, total_loss: 2.774486780166626
training step: 36435, total_loss: 4.436080455780029
training step: 36436, total_loss: 3.1380133628845215
training step: 36437, total_loss: 3.5692343711853027
training step: 36438, total_loss: 3.3810794353485107
training step: 36439, total_loss: 2.2210195064544678
training step: 36440, total_loss: 3.3668618202209473
training step: 36441, total_loss: 1.80118727684021
training step: 36442, total_loss: 2.776127815246582
training step: 36443, total_loss: 1.5034167766571045
training step: 36444, total_loss: 3.2500500679016113
training step: 36445, total_loss: 1.0541315078735352
training step: 36446, total_loss: 1.9565080404281616
training step: 36447, total_loss: 4.832480430603027
training step: 36448, total_loss: 2.9322214126586914
training step: 36449, total_loss: 2.010558843612671
training step: 36450, total_loss: 0.8387229442596436
training step: 36451, total_loss: 0.993139386177063
training step: 36452, total_loss: 2.217115640640259
training step: 36453, total_loss: 3.536146640777588
training step: 36454, total_loss: 4.392906665802002
training step: 36455, total_loss: 0.9442195892333984
training step: 36456, total_loss: 2.8973388671875
training step: 36457, total_loss: 2.5783655643463135
training step: 36458, total_loss: 2.9875681400299072
training step: 36459, total_loss: 2.402017593383789
training step: 36460, total_loss: 2.9038004875183105
training step: 36461, total_loss: 2.6791560649871826
training step: 36462, total_loss: 3.014026165008545
training step: 36463, total_loss: 2.920638084411621
training step: 36464, total_loss: 1.2814183235168457
training step: 36465, total_loss: 2.219189405441284
training step: 36466, total_loss: 2.8162682056427
training step: 36467, total_loss: 2.9589014053344727
training step: 36468, total_loss: 4.214653968811035
training step: 36469, total_loss: 3.0451366901397705
training step: 36470, total_loss: 3.154146671295166
training step: 36471, total_loss: 3.9914450645446777
training step: 36472, total_loss: 3.718611240386963
training step: 36473, total_loss: 1.8705254793167114
training step: 36474, total_loss: 2.32163667678833
training step: 36475, total_loss: 0.7502198815345764
training step: 36476, total_loss: 1.6880643367767334
training step: 36477, total_loss: 2.857764720916748
training step: 36478, total_loss: 1.7838144302368164
training step: 36479, total_loss: 1.5836029052734375
training step: 36480, total_loss: 1.2883074283599854
training step: 36481, total_loss: 2.8966405391693115
training step: 36482, total_loss: 0.9685885906219482
training step: 36483, total_loss: 1.0846858024597168
training step: 36484, total_loss: 1.9135419130325317
training step: 36485, total_loss: 4.173203468322754
training step: 36486, total_loss: 0.13026656210422516
training step: 36487, total_loss: 2.359750270843506
training step: 36488, total_loss: 0.8021314144134521
training step: 36489, total_loss: 6.776227951049805
training step: 36490, total_loss: 1.9556015729904175
training step: 36491, total_loss: 3.760876417160034
training step: 36492, total_loss: 3.904083013534546
training step: 36493, total_loss: 1.4076173305511475
training step: 36494, total_loss: 1.6823103427886963
training step: 36495, total_loss: 0.29714152216911316
training step: 36496, total_loss: 3.87459135055542
training step: 36497, total_loss: 2.464163303375244
training step: 36498, total_loss: 4.608237266540527
training step: 36499, total_loss: 2.7798826694488525
training step: 36500, total_loss: 3.300546169281006
training step: 36501, total_loss: 2.5515995025634766
training step: 36502, total_loss: 2.458747386932373
training step: 36503, total_loss: 1.5241987705230713
training step: 36504, total_loss: 1.9888675212860107
training step: 36505, total_loss: 1.1451079845428467
training step: 36506, total_loss: 2.154900074005127
training step: 36507, total_loss: 4.982421875
training step: 36508, total_loss: 1.5028350353240967
training step: 36509, total_loss: 0.1693795919418335
training step: 36510, total_loss: 3.472135066986084
training step: 36511, total_loss: 2.67649507522583
training step: 36512, total_loss: 4.257465839385986
training step: 36513, total_loss: 5.462500095367432
training step: 36514, total_loss: 2.065680980682373
training step: 36515, total_loss: 3.9498038291931152
training step: 36516, total_loss: 1.923120141029358
training step: 36517, total_loss: 3.816183090209961
training step: 36518, total_loss: 2.259559154510498
training step: 36519, total_loss: 6.567107200622559
training step: 36520, total_loss: 1.5739414691925049
training step: 36521, total_loss: 2.8198390007019043
training step: 36522, total_loss: 2.1991615295410156
training step: 36523, total_loss: 3.6538612842559814
training step: 36524, total_loss: 2.4786245822906494
training step: 36525, total_loss: 2.3383357524871826
training step: 36526, total_loss: 1.1521133184432983
training step: 36527, total_loss: 3.0522642135620117
training step: 36528, total_loss: 3.2476389408111572
training step: 36529, total_loss: 3.1717689037323
training step: 36530, total_loss: 2.5415306091308594
training step: 36531, total_loss: 6.244441032409668
training step: 36532, total_loss: 3.030324935913086
training step: 36533, total_loss: 1.432459831237793
training step: 36534, total_loss: 0.3362269401550293
training step: 36535, total_loss: 2.161611557006836
training step: 36536, total_loss: 3.9447131156921387
training step: 36537, total_loss: 0.9860351085662842
training step: 36538, total_loss: 4.426705360412598
training step: 36539, total_loss: 2.570425033569336
training step: 36540, total_loss: 1.3043129444122314
training step: 36541, total_loss: 3.740398406982422
training step: 36542, total_loss: 2.7519094944000244
training step: 36543, total_loss: 2.145512104034424
training step: 36544, total_loss: 1.4135206937789917
training step: 36545, total_loss: 1.1231398582458496
training step: 36546, total_loss: 5.088501930236816
training step: 36547, total_loss: 2.357131004333496
training step: 36548, total_loss: 2.0484201908111572
training step: 36549, total_loss: 2.575157403945923
training step: 36550, total_loss: 1.4572359323501587
training step: 36551, total_loss: 1.392676591873169
training step: 36552, total_loss: 3.0262327194213867
training step: 36553, total_loss: 3.049091339111328
training step: 36554, total_loss: 3.4351067543029785
training step: 36555, total_loss: 3.892477512359619
training step: 36556, total_loss: 1.467799425125122
training step: 36557, total_loss: 2.140045642852783
training step: 36558, total_loss: 1.210418939590454
training step: 36559, total_loss: 2.9191689491271973
training step: 36560, total_loss: 2.398564338684082
training step: 36561, total_loss: 3.099667549133301
training step: 36562, total_loss: 1.8463506698608398
training step: 36563, total_loss: 2.3068675994873047
training step: 36564, total_loss: 4.346097469329834
training step: 36565, total_loss: 2.9509785175323486
training step: 36566, total_loss: 2.1362955570220947
training step: 36567, total_loss: 1.4403529167175293
training step: 36568, total_loss: 2.0701119899749756
training step: 36569, total_loss: 2.396557092666626
training step: 36570, total_loss: 1.604231357574463
training step: 36571, total_loss: 1.700258731842041
training step: 36572, total_loss: 1.268308162689209
training step: 36573, total_loss: 0.791756272315979
training step: 36574, total_loss: 4.456812381744385
training step: 36575, total_loss: 0.7466886043548584
training step: 36576, total_loss: 1.8245849609375
training step: 36577, total_loss: 3.7948079109191895
training step: 36578, total_loss: 3.802554130554199
training step: 36579, total_loss: 2.4649782180786133
training step: 36580, total_loss: 2.3541016578674316
training step: 36581, total_loss: 1.9856436252593994
training step: 36582, total_loss: 3.1469078063964844
training step: 36583, total_loss: 3.5627708435058594
training step: 36584, total_loss: 2.7046127319335938
training step: 36585, total_loss: 1.7589833736419678
training step: 36586, total_loss: 5.166354179382324
training step: 36587, total_loss: 2.2024049758911133
training step: 36588, total_loss: 1.560914158821106
training step: 36589, total_loss: 2.8802573680877686
training step: 36590, total_loss: 3.2546043395996094
training step: 36591, total_loss: 1.3105717897415161
training step: 36592, total_loss: 2.1539626121520996
training step: 36593, total_loss: 1.5584473609924316
training step: 36594, total_loss: 2.111696243286133
training step: 36595, total_loss: 3.8147811889648438
training step: 36596, total_loss: 2.944085121154785
training step: 36597, total_loss: 1.0282466411590576
training step: 36598, total_loss: 4.152586936950684
training step: 36599, total_loss: 1.8665144443511963
training step: 36600, total_loss: 2.8632164001464844
training step: 36601, total_loss: 2.240100145339966
training step: 36602, total_loss: 3.5843610763549805
training step: 36603, total_loss: 0.8588729500770569
training step: 36604, total_loss: 2.4154698848724365
training step: 36605, total_loss: 1.385493278503418
training step: 36606, total_loss: 2.1198041439056396
training step: 36607, total_loss: 1.7519917488098145
training step: 36608, total_loss: 2.2872366905212402
training step: 36609, total_loss: 1.8302282094955444
training step: 36610, total_loss: 0.5569608211517334
training step: 36611, total_loss: 2.661937713623047
training step: 36612, total_loss: 3.395400285720825
training step: 36613, total_loss: 0.5665021538734436
training step: 36614, total_loss: 1.2793009281158447
training step: 36615, total_loss: 3.799067974090576
training step: 36616, total_loss: 4.265023231506348
training step: 36617, total_loss: 1.4387001991271973
training step: 36618, total_loss: 1.99393630027771
training step: 36619, total_loss: 0.7129131555557251
training step: 36620, total_loss: 2.6602914333343506
training step: 36621, total_loss: 2.8276307582855225
training step: 36622, total_loss: 1.33035147190094
training step: 36623, total_loss: 2.0142569541931152
training step: 36624, total_loss: 3.9454288482666016
training step: 36625, total_loss: 2.336669921875
training step: 36626, total_loss: 1.9575660228729248
training step: 36627, total_loss: 2.4665703773498535
training step: 36628, total_loss: 1.2213624715805054
training step: 36629, total_loss: 0.7566782832145691
training step: 36630, total_loss: 1.378122329711914
training step: 36631, total_loss: 3.1991524696350098
training step: 36632, total_loss: 4.456622123718262
training step: 36633, total_loss: 1.1795730590820312
training step: 36634, total_loss: 3.138676404953003
training step: 36635, total_loss: 5.186614990234375
training step: 36636, total_loss: 2.30173659324646
training step: 36637, total_loss: 2.936858654022217
training step: 36638, total_loss: 2.817887783050537
training step: 36639, total_loss: 2.1393492221832275
training step: 36640, total_loss: 1.5136184692382812
training step: 36641, total_loss: 2.6463825702667236
training step: 36642, total_loss: 4.799035549163818
training step: 36643, total_loss: 2.3017828464508057
training step: 36644, total_loss: 1.929914951324463
training step: 36645, total_loss: 0.9962854981422424
training step: 36646, total_loss: 0.8268857598304749
training step: 36647, total_loss: 2.3277456760406494
training step: 36648, total_loss: 2.364427328109741
training step: 36649, total_loss: 1.3108878135681152
training step: 36650, total_loss: 2.3680508136749268
training step: 36651, total_loss: 4.8440093994140625
training step: 36652, total_loss: 2.2817821502685547
training step: 36653, total_loss: 0.49219512939453125
training step: 36654, total_loss: 4.350448131561279
training step: 36655, total_loss: 0.48702147603034973
training step: 36656, total_loss: 2.336463689804077
training step: 36657, total_loss: 2.696808099746704
training step: 36658, total_loss: 1.8621602058410645
training step: 36659, total_loss: 3.4576730728149414
training step: 36660, total_loss: 1.4629284143447876
training step: 36661, total_loss: 3.5668020248413086
training step: 36662, total_loss: 2.6863577365875244
training step: 36663, total_loss: 1.5609869956970215
training step: 36664, total_loss: 1.9412384033203125
training step: 36665, total_loss: 2.621920347213745
training step: 36666, total_loss: 2.369269371032715
training step: 36667, total_loss: 2.0451183319091797
training step: 36668, total_loss: 1.7657642364501953
training step: 36669, total_loss: 2.0919432640075684
training step: 36670, total_loss: 2.0362155437469482
training step: 36671, total_loss: 2.1879353523254395
training step: 36672, total_loss: 1.8967885971069336
training step: 36673, total_loss: 2.1915833950042725
training step: 36674, total_loss: 6.224662780761719
training step: 36675, total_loss: 1.5261039733886719
training step: 36676, total_loss: 0.9766374826431274
training step: 36677, total_loss: 2.664324998855591
training step: 36678, total_loss: 2.719188928604126
training step: 36679, total_loss: 1.5266056060791016
training step: 36680, total_loss: 6.592177867889404
training step: 36681, total_loss: 0.5945006012916565
training step: 36682, total_loss: 1.6923813819885254
training step: 36683, total_loss: 2.7655019760131836
training step: 36684, total_loss: 1.235884666442871
training step: 36685, total_loss: 3.853304862976074
training step: 36686, total_loss: 1.9315803050994873
training step: 36687, total_loss: 3.056515693664551
training step: 36688, total_loss: 2.3318774700164795
training step: 36689, total_loss: 2.5984339714050293
training step: 36690, total_loss: 2.1885321140289307
training step: 36691, total_loss: 2.276564836502075
training step: 36692, total_loss: 2.1199522018432617
training step: 36693, total_loss: 1.3104898929595947
training step: 36694, total_loss: 3.7324023246765137
training step: 36695, total_loss: 3.885702610015869
training step: 36696, total_loss: 2.6515278816223145
training step: 36697, total_loss: 1.5352061986923218
training step: 36698, total_loss: 2.6214711666107178
training step: 36699, total_loss: 3.2207698822021484
training step: 36700, total_loss: 1.2696806192398071
training step: 36701, total_loss: 4.845566749572754
training step: 36702, total_loss: 3.8874223232269287
training step: 36703, total_loss: 5.551804065704346
training step: 36704, total_loss: 1.9130828380584717
training step: 36705, total_loss: 1.36677885055542
training step: 36706, total_loss: 2.3991358280181885
training step: 36707, total_loss: 2.0018563270568848
training step: 36708, total_loss: 2.840308904647827
training step: 36709, total_loss: 2.5962116718292236
training step: 36710, total_loss: 1.6847813129425049
training step: 36711, total_loss: 3.7333054542541504
training step: 36712, total_loss: 3.29719877243042
training step: 36713, total_loss: 2.9473564624786377
training step: 36714, total_loss: 2.0387961864471436
training step: 36715, total_loss: 3.43318510055542
training step: 36716, total_loss: 2.8268837928771973
training step: 36717, total_loss: 3.276442050933838
training step: 36718, total_loss: 2.4632182121276855
training step: 36719, total_loss: 4.19956636428833
training step: 36720, total_loss: 2.3474907875061035
training step: 36721, total_loss: 3.6934609413146973
training step: 36722, total_loss: 2.559410333633423
training step: 36723, total_loss: 1.1658141613006592
training step: 36724, total_loss: 3.114189624786377
training step: 36725, total_loss: 1.7716848850250244
training step: 36726, total_loss: 4.974742889404297
training step: 36727, total_loss: 2.574244976043701
training step: 36728, total_loss: 0.08086934685707092
training step: 36729, total_loss: 2.379023551940918
training step: 36730, total_loss: 2.765103340148926
training step: 36731, total_loss: 1.4532121419906616
training step: 36732, total_loss: 2.1842212677001953
training step: 36733, total_loss: 2.6352477073669434
training step: 36734, total_loss: 1.60919988155365
training step: 36735, total_loss: 2.0753304958343506
training step: 36736, total_loss: 2.547128438949585
training step: 36737, total_loss: 2.6194076538085938
training step: 36738, total_loss: 2.9366960525512695
training step: 36739, total_loss: 3.013211488723755
training step: 36740, total_loss: 1.9095650911331177
training step: 36741, total_loss: 0.8778015971183777
training step: 36742, total_loss: 2.9967849254608154
training step: 36743, total_loss: 3.7119884490966797
training step: 36744, total_loss: 1.9025022983551025
training step: 36745, total_loss: 1.1646132469177246
training step: 36746, total_loss: 1.6041350364685059
training step: 36747, total_loss: 3.3656861782073975
training step: 36748, total_loss: 2.0715136528015137
training step: 36749, total_loss: 4.035555839538574
training step: 36750, total_loss: 1.7212893962860107
training step: 36751, total_loss: 0.7714626789093018
training step: 36752, total_loss: 1.9678504467010498
training step: 36753, total_loss: 2.4402713775634766
training step: 36754, total_loss: 1.7601757049560547
training step: 36755, total_loss: 0.4543086588382721
training step: 36756, total_loss: 2.2012832164764404
training step: 36757, total_loss: 3.1319937705993652
training step: 36758, total_loss: 4.114964962005615
training step: 36759, total_loss: 1.9901328086853027
training step: 36760, total_loss: 3.177584171295166
training step: 36761, total_loss: 1.7168464660644531
training step: 36762, total_loss: 1.9997785091400146
training step: 36763, total_loss: 2.6785788536071777
training step: 36764, total_loss: 3.7739768028259277
training step: 36765, total_loss: 2.8728764057159424
training step: 36766, total_loss: 0.8273912668228149
training step: 36767, total_loss: 1.1841316223144531
training step: 36768, total_loss: 1.563346028327942
training step: 36769, total_loss: 3.630786418914795
training step: 36770, total_loss: 3.776115655899048
training step: 36771, total_loss: 0.2674137055873871
training step: 36772, total_loss: 0.37253546714782715
training step: 36773, total_loss: 0.1569070816040039
training step: 36774, total_loss: 0.9548802375793457
training step: 36775, total_loss: 3.4139256477355957
training step: 36776, total_loss: 2.885385036468506
training step: 36777, total_loss: 3.035942554473877
training step: 36778, total_loss: 2.87735915184021
training step: 36779, total_loss: 2.1601221561431885
training step: 36780, total_loss: 1.9825605154037476
training step: 36781, total_loss: 5.310826301574707
training step: 36782, total_loss: 2.084214448928833
training step: 36783, total_loss: 1.4455361366271973
training step: 36784, total_loss: 1.7925820350646973
training step: 36785, total_loss: 2.021388292312622
training step: 36786, total_loss: 4.677040100097656
training step: 36787, total_loss: 3.3536670207977295
training step: 36788, total_loss: 1.6569209098815918
training step: 36789, total_loss: 0.9028788805007935
training step: 36790, total_loss: 1.9368962049484253
training step: 36791, total_loss: 6.247550964355469
training step: 36792, total_loss: 1.5598621368408203
training step: 36793, total_loss: 2.2029502391815186
training step: 36794, total_loss: 5.215423583984375
training step: 36795, total_loss: 1.6805758476257324
training step: 36796, total_loss: 3.3570303916931152
training step: 36797, total_loss: 2.1354634761810303
training step: 36798, total_loss: 1.280896782875061
training step: 36799, total_loss: 4.4676361083984375
training step: 36800, total_loss: 1.9300813674926758
training step: 36801, total_loss: 2.582235336303711
training step: 36802, total_loss: 1.565011739730835
training step: 36803, total_loss: 1.7879536151885986
training step: 36804, total_loss: 3.6874301433563232
training step: 36805, total_loss: 3.336440086364746
training step: 36806, total_loss: 0.959128737449646
training step: 36807, total_loss: 1.9269435405731201
training step: 36808, total_loss: 2.386708974838257
training step: 36809, total_loss: 1.6701250076293945
training step: 36810, total_loss: 3.2153310775756836
training step: 36811, total_loss: 1.0436369180679321
training step: 36812, total_loss: 1.8523725271224976
training step: 36813, total_loss: 1.6763324737548828
training step: 36814, total_loss: 0.8234012126922607
training step: 36815, total_loss: 1.7810804843902588
training step: 36816, total_loss: 4.951958656311035
training step: 36817, total_loss: 1.6588032245635986
training step: 36818, total_loss: 2.0272889137268066
training step: 36819, total_loss: 0.18663732707500458
training step: 36820, total_loss: 2.4425277709960938
training step: 36821, total_loss: 3.460686683654785
training step: 36822, total_loss: 2.375694751739502
training step: 36823, total_loss: 2.9395172595977783
training step: 36824, total_loss: 4.853734970092773
training step: 36825, total_loss: 3.3538899421691895
training step: 36826, total_loss: 2.590257167816162
training step: 36827, total_loss: 0.17364536225795746
training step: 36828, total_loss: 1.4331607818603516
training step: 36829, total_loss: 2.837445020675659
training step: 36830, total_loss: 3.5142171382904053
training step: 36831, total_loss: 3.6093087196350098
training step: 36832, total_loss: 2.824261426925659
training step: 36833, total_loss: 0.7577542066574097
training step: 36834, total_loss: 2.9096269607543945
training step: 36835, total_loss: 2.1532702445983887
training step: 36836, total_loss: 1.2962390184402466
training step: 36837, total_loss: 1.3072142601013184
training step: 36838, total_loss: 1.1047112941741943
training step: 36839, total_loss: 2.8081552982330322
training step: 36840, total_loss: 5.254865646362305
training step: 36841, total_loss: 1.9643280506134033
training step: 36842, total_loss: 6.024147033691406
training step: 36843, total_loss: 1.3428382873535156
training step: 36844, total_loss: 1.543365478515625
training step: 36845, total_loss: 1.7292002439498901
training step: 36846, total_loss: 3.014456033706665
training step: 36847, total_loss: 2.8773534297943115
training step: 36848, total_loss: 1.78660249710083
training step: 36849, total_loss: 1.5471197366714478
training step: 36850, total_loss: 3.4360861778259277
training step: 36851, total_loss: 2.2164807319641113
training step: 36852, total_loss: 2.051506280899048
training step: 36853, total_loss: 3.2986578941345215
training step: 36854, total_loss: 1.478285312652588
training step: 36855, total_loss: 3.3459014892578125
training step: 36856, total_loss: 2.488569736480713
training step: 36857, total_loss: 3.9770216941833496
training step: 36858, total_loss: 4.468254089355469
training step: 36859, total_loss: 5.590871334075928
training step: 36860, total_loss: 2.943737506866455
training step: 36861, total_loss: 3.3647239208221436
training step: 36862, total_loss: 2.9989097118377686
training step: 36863, total_loss: 4.044562339782715
training step: 36864, total_loss: 2.6120762825012207
training step: 36865, total_loss: 1.2440955638885498
training step: 36866, total_loss: 2.007068157196045
training step: 36867, total_loss: 2.8578150272369385
training step: 36868, total_loss: 0.06098504737019539
training step: 36869, total_loss: 2.77225399017334
training step: 36870, total_loss: 2.6682167053222656
training step: 36871, total_loss: 1.9939329624176025
training step: 36872, total_loss: 1.8329806327819824
training step: 36873, total_loss: 3.659768581390381
training step: 36874, total_loss: 2.761904716491699
training step: 36875, total_loss: 3.3495934009552
training step: 36876, total_loss: 5.020758628845215
training step: 36877, total_loss: 1.2760831117630005
training step: 36878, total_loss: 6.869778633117676
training step: 36879, total_loss: 2.7769908905029297
training step: 36880, total_loss: 2.311659336090088
training step: 36881, total_loss: 2.2885501384735107
training step: 36882, total_loss: 1.0833611488342285
training step: 36883, total_loss: 0.04978296160697937
training step: 36884, total_loss: 2.970884323120117
training step: 36885, total_loss: 1.1699199676513672
training step: 36886, total_loss: 0.8135111927986145
training step: 36887, total_loss: 1.2887142896652222
training step: 36888, total_loss: 3.149282932281494
training step: 36889, total_loss: 4.275725364685059
training step: 36890, total_loss: 2.4505953788757324
training step: 36891, total_loss: 1.4195564985275269
training step: 36892, total_loss: 2.8008956909179688
training step: 36893, total_loss: 4.887232780456543/home/user/miniconda/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
/home/user/miniconda/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
  return f(*args, **kwds)
INFO:tensorflow:Writing predictions to: test_output/predictions_37000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_37000.json

training step: 36894, total_loss: 2.041210412979126
training step: 36895, total_loss: 1.867128610610962
training step: 36896, total_loss: 1.7103807926177979
training step: 36897, total_loss: 1.6189727783203125
training step: 36898, total_loss: 3.946432113647461
training step: 36899, total_loss: 3.1289255619049072
training step: 36900, total_loss: 2.9449546337127686
training step: 36901, total_loss: 5.420266628265381
training step: 36902, total_loss: 2.2514288425445557
training step: 36903, total_loss: 5.712652683258057
training step: 36904, total_loss: 3.244213342666626
training step: 36905, total_loss: 1.021843433380127
training step: 36906, total_loss: 3.1369524002075195
training step: 36907, total_loss: 3.5647971630096436
training step: 36908, total_loss: 1.697386384010315
training step: 36909, total_loss: 2.556408405303955
training step: 36910, total_loss: 2.2716622352600098
training step: 36911, total_loss: 1.5068647861480713
training step: 36912, total_loss: 1.7283742427825928
training step: 36913, total_loss: 5.265958786010742
training step: 36914, total_loss: 2.581490993499756
training step: 36915, total_loss: 2.9630727767944336
training step: 36916, total_loss: 4.68396520614624
training step: 36917, total_loss: 2.3429312705993652
training step: 36918, total_loss: 1.5670356750488281
training step: 36919, total_loss: 3.0488057136535645
training step: 36920, total_loss: 2.29133939743042
training step: 36921, total_loss: 0.16397276520729065
training step: 36922, total_loss: 0.14946645498275757
training step: 36923, total_loss: 3.165581226348877
training step: 36924, total_loss: 2.6526224613189697
training step: 36925, total_loss: 3.408031463623047
training step: 36926, total_loss: 2.4598388671875
training step: 36927, total_loss: 3.5539731979370117
training step: 36928, total_loss: 1.6045459508895874
training step: 36929, total_loss: 2.34222149848938
training step: 36930, total_loss: 3.0374794006347656
training step: 36931, total_loss: 2.277865409851074
training step: 36932, total_loss: 0.1847923994064331
training step: 36933, total_loss: 4.450503349304199
training step: 36934, total_loss: 1.9766242504119873
training step: 36935, total_loss: 3.115570545196533
training step: 36936, total_loss: 2.224548578262329
training step: 36937, total_loss: 1.4348417520523071
training step: 36938, total_loss: 0.09897108376026154
training step: 36939, total_loss: 1.9783110618591309
training step: 36940, total_loss: 1.2312757968902588
training step: 36941, total_loss: 3.550996780395508
training step: 36942, total_loss: 1.3589317798614502
training step: 36943, total_loss: 3.400325298309326
training step: 36944, total_loss: 2.0240490436553955
training step: 36945, total_loss: 2.7357730865478516
training step: 36946, total_loss: 3.7912492752075195
training step: 36947, total_loss: 3.0586657524108887
training step: 36948, total_loss: 2.1390299797058105
training step: 36949, total_loss: 2.0325605869293213
training step: 36950, total_loss: 1.7092921733856201
training step: 36951, total_loss: 1.8261586427688599
training step: 36952, total_loss: 0.4242953062057495
training step: 36953, total_loss: 2.3602805137634277
training step: 36954, total_loss: 4.8804779052734375
training step: 36955, total_loss: 0.11787474155426025
training step: 36956, total_loss: 4.078401565551758
training step: 36957, total_loss: 2.0561885833740234
training step: 36958, total_loss: 0.7798962593078613
training step: 36959, total_loss: 1.3962482213974
training step: 36960, total_loss: 3.221917152404785
training step: 36961, total_loss: 2.88248610496521
training step: 36962, total_loss: 2.484513282775879
training step: 36963, total_loss: 3.24613094329834
training step: 36964, total_loss: 0.021195193752646446
training step: 36965, total_loss: 1.083814024925232
training step: 36966, total_loss: 3.345942497253418
training step: 36967, total_loss: 1.6500734090805054
training step: 36968, total_loss: 2.306793212890625
training step: 36969, total_loss: 4.064126014709473
training step: 36970, total_loss: 1.325876235961914
training step: 36971, total_loss: 3.2455015182495117
training step: 36972, total_loss: 3.7845685482025146
training step: 36973, total_loss: 2.7119486331939697
training step: 36974, total_loss: 1.2832720279693604
training step: 36975, total_loss: 1.9848310947418213
training step: 36976, total_loss: 0.44860777258872986
training step: 36977, total_loss: 0.5779954195022583
training step: 36978, total_loss: 2.6170992851257324
training step: 36979, total_loss: 2.4120988845825195
training step: 36980, total_loss: 6.194710731506348
training step: 36981, total_loss: 2.2845354080200195
training step: 36982, total_loss: 3.5342812538146973
training step: 36983, total_loss: 4.066391468048096
training step: 36984, total_loss: 1.9683606624603271
training step: 36985, total_loss: 1.5281833410263062
training step: 36986, total_loss: 2.1718032360076904
training step: 36987, total_loss: 4.41117000579834
training step: 36988, total_loss: 4.195279121398926
training step: 36989, total_loss: 1.3136556148529053
training step: 36990, total_loss: 4.400466442108154
training step: 36991, total_loss: 0.4801580309867859
training step: 36992, total_loss: 2.237689256668091
training step: 36993, total_loss: 2.537719249725342
training step: 36994, total_loss: 4.5284905433654785
training step: 36995, total_loss: 0.7669040560722351
training step: 36996, total_loss: 2.708364963531494
training step: 36997, total_loss: 1.2318676710128784
training step: 36998, total_loss: 1.9283136129379272
training step: 36999, total_loss: 2.098447799682617
training step: 37000, total_loss: 3.1798250675201416
epoch finished! shuffle=False
evaluation: 1000, total_loss: 1.53865647315979, f1: 58.2436723117533, followup: 18.378213905370455, yesno: 65.43435265479994, heq: 53.24813631522897, dheq: 3.5

Model saved in path test_output//model_37000.ckpt
training step: 37001, total_loss: 0.525795578956604
training step: 37002, total_loss: 2.0264971256256104
training step: 37003, total_loss: 2.0305490493774414
training step: 37004, total_loss: 2.2275876998901367
training step: 37005, total_loss: 0.9220902919769287
training step: 37006, total_loss: 2.4198975563049316
training step: 37007, total_loss: 1.5890321731567383
training step: 37008, total_loss: 1.375410556793213
training step: 37009, total_loss: 1.907573938369751
training step: 37010, total_loss: 0.34003108739852905
training step: 37011, total_loss: 3.2108469009399414
training step: 37012, total_loss: 3.8189992904663086
training step: 37013, total_loss: 2.0906286239624023
training step: 37014, total_loss: 1.038515329360962
training step: 37015, total_loss: 3.5142345428466797
training step: 37016, total_loss: 2.9708175659179688
training step: 37017, total_loss: 2.558537721633911
training step: 37018, total_loss: 2.1377334594726562
training step: 37019, total_loss: 0.9210512638092041
training step: 37020, total_loss: 0.5821415781974792
training step: 37021, total_loss: 1.1063649654388428
training step: 37022, total_loss: 2.1681742668151855
training step: 37023, total_loss: 5.2695746421813965
training step: 37024, total_loss: 2.8959429264068604
training step: 37025, total_loss: 1.9069483280181885
training step: 37026, total_loss: 2.6318793296813965
training step: 37027, total_loss: 4.095768451690674
training step: 37028, total_loss: 4.070767402648926
training step: 37029, total_loss: 1.8316421508789062
training step: 37030, total_loss: 4.238959789276123
training step: 37031, total_loss: 1.0926545858383179
training step: 37032, total_loss: 1.79789137840271
training step: 37033, total_loss: 2.6840643882751465
training step: 37034, total_loss: 3.751657724380493
training step: 37035, total_loss: 1.8156626224517822
training step: 37036, total_loss: 3.446453332901001
training step: 37037, total_loss: 3.0909948348999023
training step: 37038, total_loss: 0.5753880739212036
training step: 37039, total_loss: 2.11171817779541
training step: 37040, total_loss: 2.4250147342681885
training step: 37041, total_loss: 2.255387783050537
training step: 37042, total_loss: 1.0490504503250122
training step: 37043, total_loss: 1.6688716411590576
training step: 37044, total_loss: 2.8045601844787598
training step: 37045, total_loss: 1.4980006217956543
training step: 37046, total_loss: 2.816767930984497
training step: 37047, total_loss: 1.6528761386871338
training step: 37048, total_loss: 3.8194737434387207
training step: 37049, total_loss: 4.273107528686523
training step: 37050, total_loss: 4.019636154174805
training step: 37051, total_loss: 1.412520408630371
training step: 37052, total_loss: 2.1914589405059814
training step: 37053, total_loss: 1.1721186637878418
training step: 37054, total_loss: 2.646099090576172
training step: 37055, total_loss: 6.214447975158691
training step: 37056, total_loss: 4.574438095092773
training step: 37057, total_loss: 4.266216278076172
training step: 37058, total_loss: 2.714684009552002
training step: 37059, total_loss: 5.207572937011719
training step: 37060, total_loss: 2.0195279121398926
training step: 37061, total_loss: 2.322178840637207
training step: 37062, total_loss: 1.64339280128479
training step: 37063, total_loss: 1.48618745803833
training step: 37064, total_loss: 1.327934980392456
training step: 37065, total_loss: 2.735719919204712
training step: 37066, total_loss: 0.12550482153892517
training step: 37067, total_loss: 2.683575391769409
training step: 37068, total_loss: 1.9318442344665527
training step: 37069, total_loss: 4.249972343444824
training step: 37070, total_loss: 1.1204921007156372
training step: 37071, total_loss: 2.073814868927002
training step: 37072, total_loss: 4.428674697875977
training step: 37073, total_loss: 3.676743745803833
training step: 37074, total_loss: 2.379458427429199
training step: 37075, total_loss: 4.260191917419434
training step: 37076, total_loss: 3.4750990867614746
training step: 37077, total_loss: 1.9048082828521729
training step: 37078, total_loss: 2.7232494354248047
training step: 37079, total_loss: 0.8648923635482788
training step: 37080, total_loss: 0.41893821954727173
training step: 37081, total_loss: 2.6074295043945312
training step: 37082, total_loss: 1.8026344776153564
training step: 37083, total_loss: 3.994967460632324
training step: 37084, total_loss: 3.6014795303344727
training step: 37085, total_loss: 1.6832549571990967
training step: 37086, total_loss: 1.3574261665344238
training step: 37087, total_loss: 2.5593831539154053
training step: 37088, total_loss: 4.094688415527344
training step: 37089, total_loss: 0.0555969662964344
training step: 37090, total_loss: 1.0045934915542603
training step: 37091, total_loss: 1.2604790925979614
training step: 37092, total_loss: 1.4112839698791504
training step: 37093, total_loss: 2.300985336303711
training step: 37094, total_loss: 3.22245192527771
training step: 37095, total_loss: 3.242733955383301
training step: 37096, total_loss: 1.3159699440002441
training step: 37097, total_loss: 1.6925662755966187
training step: 37098, total_loss: 1.3454813957214355
training step: 37099, total_loss: 2.5711145401000977
training step: 37100, total_loss: 1.3779411315917969
training step: 37101, total_loss: 1.981209397315979
training step: 37102, total_loss: 1.2168984413146973
training step: 37103, total_loss: 0.9532436728477478
training step: 37104, total_loss: 3.6419360637664795
training step: 37105, total_loss: 3.0608558654785156
training step: 37106, total_loss: 2.9319241046905518
training step: 37107, total_loss: 0.3716592788696289
training step: 37108, total_loss: 1.139812707901001
training step: 37109, total_loss: 2.5654220581054688
training step: 37110, total_loss: 4.5600433349609375
training step: 37111, total_loss: 3.5780606269836426
training step: 37112, total_loss: 0.7682555913925171
training step: 37113, total_loss: 0.2974321246147156
training step: 37114, total_loss: 1.7128987312316895
training step: 37115, total_loss: 2.9358060359954834
training step: 37116, total_loss: 3.156876802444458
training step: 37117, total_loss: 4.047626495361328
training step: 37118, total_loss: 3.3759336471557617
training step: 37119, total_loss: 2.8000845909118652
training step: 37120, total_loss: 2.7645435333251953
training step: 37121, total_loss: 0.7881903648376465
training step: 37122, total_loss: 1.068740725517273
training step: 37123, total_loss: 4.057216644287109
training step: 37124, total_loss: 2.732943534851074
training step: 37125, total_loss: 2.8903751373291016
training step: 37126, total_loss: 2.5901806354522705
training step: 37127, total_loss: 5.292534828186035
training step: 37128, total_loss: 1.5231965780258179
training step: 37129, total_loss: 3.0702309608459473
training step: 37130, total_loss: 4.292366981506348
training step: 37131, total_loss: 3.767145872116089
training step: 37132, total_loss: 1.4048426151275635
training step: 37133, total_loss: 2.6346333026885986
training step: 37134, total_loss: 1.6752294301986694
training step: 37135, total_loss: 1.0428965091705322
training step: 37136, total_loss: 2.918189525604248
training step: 37137, total_loss: 4.875213623046875
training step: 37138, total_loss: 2.668423891067505
training step: 37139, total_loss: 3.878509521484375
training step: 37140, total_loss: 2.115546226501465
training step: 37141, total_loss: 3.944505214691162
training step: 37142, total_loss: 2.879394054412842
training step: 37143, total_loss: 3.3528871536254883
training step: 37144, total_loss: 1.6069190502166748
training step: 37145, total_loss: 1.4169142246246338
training step: 37146, total_loss: 4.328649520874023
training step: 37147, total_loss: 2.777158498764038
training step: 37148, total_loss: 1.4267852306365967
training step: 37149, total_loss: 1.8054306507110596
training step: 37150, total_loss: 2.6301932334899902
training step: 37151, total_loss: 2.1023032665252686
training step: 37152, total_loss: 2.393183469772339
training step: 37153, total_loss: 1.1121225357055664
training step: 37154, total_loss: 2.9559295177459717
training step: 37155, total_loss: 1.4415215253829956
training step: 37156, total_loss: 2.0351552963256836
training step: 37157, total_loss: 2.7656989097595215
training step: 37158, total_loss: 4.187019348144531
training step: 37159, total_loss: 1.6869137287139893
training step: 37160, total_loss: 0.36475667357444763
training step: 37161, total_loss: 2.225102424621582
training step: 37162, total_loss: 2.092226266860962
training step: 37163, total_loss: 3.2288389205932617
training step: 37164, total_loss: 2.132495880126953
training step: 37165, total_loss: 3.0957088470458984
training step: 37166, total_loss: 1.6403783559799194
training step: 37167, total_loss: 5.71771240234375
training step: 37168, total_loss: 1.7552870512008667
training step: 37169, total_loss: 2.101632833480835
training step: 37170, total_loss: 0.6901445984840393
training step: 37171, total_loss: 1.5564236640930176
training step: 37172, total_loss: 2.5340218544006348
training step: 37173, total_loss: 1.7400754690170288
training step: 37174, total_loss: 2.5768845081329346
training step: 37175, total_loss: 2.07857084274292
training step: 37176, total_loss: 2.3364453315734863
training step: 37177, total_loss: 0.5463720560073853
training step: 37178, total_loss: 3.159792423248291
training step: 37179, total_loss: 1.9001450538635254
training step: 37180, total_loss: 2.0709917545318604
training step: 37181, total_loss: 0.1268496960401535
training step: 37182, total_loss: 3.8384079933166504
training step: 37183, total_loss: 1.9477949142456055
training step: 37184, total_loss: 1.0285719633102417
training step: 37185, total_loss: 0.5032037496566772
training step: 37186, total_loss: 3.347921848297119
training step: 37187, total_loss: 1.3624942302703857
training step: 37188, total_loss: 3.4177193641662598
training step: 37189, total_loss: 3.044628143310547
training step: 37190, total_loss: 0.8418278694152832
training step: 37191, total_loss: 1.3499107360839844
training step: 37192, total_loss: 3.3788657188415527
training step: 37193, total_loss: 1.5200742483139038
training step: 37194, total_loss: 2.453730821609497
training step: 37195, total_loss: 0.8867800235748291
training step: 37196, total_loss: 1.6695241928100586
training step: 37197, total_loss: 1.6683716773986816
training step: 37198, total_loss: 2.2669975757598877
training step: 37199, total_loss: 1.116674780845642
training step: 37200, total_loss: 2.587100028991699
training step: 37201, total_loss: 4.365716457366943
training step: 37202, total_loss: 3.8425936698913574
training step: 37203, total_loss: 3.168795108795166
training step: 37204, total_loss: 5.7425408363342285
training step: 37205, total_loss: 1.5180366039276123
training step: 37206, total_loss: 2.766251802444458
training step: 37207, total_loss: 2.2056593894958496
training step: 37208, total_loss: 0.23357823491096497
training step: 37209, total_loss: 1.5911163091659546
training step: 37210, total_loss: 1.557438850402832
training step: 37211, total_loss: 1.106151819229126
training step: 37212, total_loss: 4.041235446929932
training step: 37213, total_loss: 2.5061044692993164
training step: 37214, total_loss: 0.3181200623512268
training step: 37215, total_loss: 1.0255811214447021
training step: 37216, total_loss: 0.03346550464630127
training step: 37217, total_loss: 1.403951644897461
training step: 37218, total_loss: 3.579526424407959
training step: 37219, total_loss: 2.7623891830444336
training step: 37220, total_loss: 2.9335150718688965
training step: 37221, total_loss: 3.9993255138397217
training step: 37222, total_loss: 3.1804585456848145
training step: 37223, total_loss: 0.177487313747406
training step: 37224, total_loss: 1.6148731708526611
training step: 37225, total_loss: 1.4403655529022217
training step: 37226, total_loss: 1.6680970191955566
training step: 37227, total_loss: 1.3686538934707642
training step: 37228, total_loss: 3.6281356811523438
training step: 37229, total_loss: 0.1427578628063202
training step: 37230, total_loss: 4.793522357940674
training step: 37231, total_loss: 1.4024651050567627
training step: 37232, total_loss: 1.7251957654953003
training step: 37233, total_loss: 0.5542786717414856
training step: 37234, total_loss: 0.7336187362670898
training step: 37235, total_loss: 3.1550545692443848
training step: 37236, total_loss: 1.7869093418121338
training step: 37237, total_loss: 2.691176414489746
training step: 37238, total_loss: 2.5950191020965576
training step: 37239, total_loss: 7.068323612213135
training step: 37240, total_loss: 4.040027141571045
training step: 37241, total_loss: 1.6870089769363403
training step: 37242, total_loss: 3.3900632858276367
training step: 37243, total_loss: 2.0255677700042725
training step: 37244, total_loss: 1.6545870304107666
training step: 37245, total_loss: 1.200994849205017
training step: 37246, total_loss: 1.266750693321228
training step: 37247, total_loss: 1.8717550039291382
training step: 37248, total_loss: 1.7781975269317627
training step: 37249, total_loss: 1.9472811222076416
training step: 37250, total_loss: 1.528391718864441
training step: 37251, total_loss: 3.100121259689331
training step: 37252, total_loss: 3.3144876956939697
training step: 37253, total_loss: 1.8774964809417725
training step: 37254, total_loss: 2.2372968196868896
training step: 37255, total_loss: 2.8232786655426025
training step: 37256, total_loss: 4.0390448570251465
training step: 37257, total_loss: 5.611011028289795
training step: 37258, total_loss: 0.5389226078987122
training step: 37259, total_loss: 1.7547584772109985
training step: 37260, total_loss: 1.8754794597625732
training step: 37261, total_loss: 2.5329036712646484
training step: 37262, total_loss: 2.64456844329834
training step: 37263, total_loss: 1.6027997732162476
training step: 37264, total_loss: 1.9019439220428467
training step: 37265, total_loss: 2.7454440593719482
training step: 37266, total_loss: 2.0055885314941406
training step: 37267, total_loss: 2.8240227699279785
training step: 37268, total_loss: 1.6290946006774902
training step: 37269, total_loss: 0.3932620584964752
training step: 37270, total_loss: 3.4899473190307617
training step: 37271, total_loss: 2.8629119396209717
training step: 37272, total_loss: 2.546283721923828
training step: 37273, total_loss: 2.296124219894409
training step: 37274, total_loss: 1.865041732788086
training step: 37275, total_loss: 0.8494910597801208
training step: 37276, total_loss: 1.8807140588760376
training step: 37277, total_loss: 1.8377554416656494
training step: 37278, total_loss: 2.1421031951904297
training step: 37279, total_loss: 0.9615582227706909
training step: 37280, total_loss: 2.555856704711914
training step: 37281, total_loss: 2.0309371948242188
training step: 37282, total_loss: 1.14931058883667
training step: 37283, total_loss: 3.606689214706421
training step: 37284, total_loss: 1.162247896194458
training step: 37285, total_loss: 0.01824580691754818
training step: 37286, total_loss: 4.100513458251953
training step: 37287, total_loss: 1.6007983684539795
training step: 37288, total_loss: 1.578446388244629
training step: 37289, total_loss: 1.662203073501587
training step: 37290, total_loss: 1.474611759185791
training step: 37291, total_loss: 3.142312526702881
training step: 37292, total_loss: 2.496908664703369
training step: 37293, total_loss: 4.901035308837891
training step: 37294, total_loss: 2.42238187789917
training step: 37295, total_loss: 1.9276766777038574
training step: 37296, total_loss: 2.0676867961883545
training step: 37297, total_loss: 4.072343826293945
training step: 37298, total_loss: 2.14617919921875
training step: 37299, total_loss: 2.2795629501342773
training step: 37300, total_loss: 2.957329034805298
training step: 37301, total_loss: 2.010054111480713
training step: 37302, total_loss: 0.8021905422210693
training step: 37303, total_loss: 0.3224528431892395
training step: 37304, total_loss: 2.5020642280578613
training step: 37305, total_loss: 2.073765754699707
training step: 37306, total_loss: 4.024247169494629
training step: 37307, total_loss: 2.289532423019409
training step: 37308, total_loss: 1.801180124282837
training step: 37309, total_loss: 1.417604923248291
training step: 37310, total_loss: 3.9708216190338135
training step: 37311, total_loss: 2.556434154510498
training step: 37312, total_loss: 2.381434679031372
training step: 37313, total_loss: 2.0255846977233887
training step: 37314, total_loss: 0.7356773018836975
training step: 37315, total_loss: 1.281527042388916
training step: 37316, total_loss: 2.032628297805786
training step: 37317, total_loss: 1.7448601722717285
training step: 37318, total_loss: 2.209796905517578
training step: 37319, total_loss: 4.812551975250244
training step: 37320, total_loss: 5.076956748962402
training step: 37321, total_loss: 3.0094034671783447
training step: 37322, total_loss: 2.8366284370422363
training step: 37323, total_loss: 1.068042278289795
training step: 37324, total_loss: 2.6070780754089355
training step: 37325, total_loss: 2.8691463470458984
training step: 37326, total_loss: 2.0743227005004883
training step: 37327, total_loss: 0.94089674949646
training step: 37328, total_loss: 2.1144165992736816
training step: 37329, total_loss: 3.317406177520752
training step: 37330, total_loss: 1.7283300161361694
training step: 37331, total_loss: 2.6501598358154297
training step: 37332, total_loss: 1.2702820301055908
training step: 37333, total_loss: 0.8182249069213867
training step: 37334, total_loss: 3.7715868949890137
training step: 37335, total_loss: 2.2158660888671875
training step: 37336, total_loss: 0.8993226289749146
training step: 37337, total_loss: 1.6562838554382324
training step: 37338, total_loss: 0.601824164390564
training step: 37339, total_loss: 2.996634006500244
training step: 37340, total_loss: 1.6563408374786377
training step: 37341, total_loss: 1.528861165046692
training step: 37342, total_loss: 3.161104202270508
training step: 37343, total_loss: 3.8153586387634277
training step: 37344, total_loss: 2.5333170890808105
training step: 37345, total_loss: 2.7466533184051514
training step: 37346, total_loss: 3.444638967514038
training step: 37347, total_loss: 1.769144892692566
training step: 37348, total_loss: 1.1977343559265137
training step: 37349, total_loss: 2.99819016456604
training step: 37350, total_loss: 3.241995096206665
training step: 37351, total_loss: 1.9041903018951416
training step: 37352, total_loss: 3.468838930130005
training step: 37353, total_loss: 2.158992290496826
training step: 37354, total_loss: 6.313464164733887
training step: 37355, total_loss: 5.455765247344971
training step: 37356, total_loss: 0.005076086148619652
training step: 37357, total_loss: 0.9272761940956116
training step: 37358, total_loss: 1.6454753875732422
training step: 37359, total_loss: 3.4225614070892334
training step: 37360, total_loss: 2.6719770431518555
training step: 37361, total_loss: 4.914786338806152
training step: 37362, total_loss: 0.865862250328064
training step: 37363, total_loss: 4.1822509765625
training step: 37364, total_loss: 2.321164608001709
training step: 37365, total_loss: 3.8647961616516113
training step: 37366, total_loss: 1.7356717586517334
training step: 37367, total_loss: 3.1325507164001465
training step: 37368, total_loss: 4.13685417175293
training step: 37369, total_loss: 1.5671131610870361
training step: 37370, total_loss: 2.2309935092926025
training step: 37371, total_loss: 2.811007499694824
training step: 37372, total_loss: 3.246819257736206
training step: 37373, total_loss: 3.291487693786621
training step: 37374, total_loss: 1.697001576423645
training step: 37375, total_loss: 2.8432931900024414
training step: 37376, total_loss: 0.6494045853614807
training step: 37377, total_loss: 1.8700382709503174
training step: 37378, total_loss: 2.6482012271881104
training step: 37379, total_loss: 1.5469123125076294
training step: 37380, total_loss: 1.8206418752670288
training step: 37381, total_loss: 2.4145355224609375
training step: 37382, total_loss: 0.26890698075294495
training step: 37383, total_loss: 1.5255274772644043
training step: 37384, total_loss: 4.538191795349121
training step: 37385, total_loss: 1.0776729583740234
training step: 37386, total_loss: 0.9682073593139648
training step: 37387, total_loss: 2.5846104621887207
training step: 37388, total_loss: 1.2604396343231201
training step: 37389, total_loss: 2.416480541229248
training step: 37390, total_loss: 3.610753059387207
training step: 37391, total_loss: 3.6549479961395264
training step: 37392, total_loss: 1.8881006240844727
training step: 37393, total_loss: 2.6082053184509277
training step: 37394, total_loss: 1.1713558435440063
training step: 37395, total_loss: 2.739201545715332
training step: 37396, total_loss: 0.4327455461025238
training step: 37397, total_loss: 4.414124488830566
training step: 37398, total_loss: 1.2419124841690063
training step: 37399, total_loss: 3.2532753944396973
training step: 37400, total_loss: 0.5967687368392944
training step: 37401, total_loss: 3.2087302207946777
training step: 37402, total_loss: 3.9940614700317383
training step: 37403, total_loss: 3.9012069702148438
training step: 37404, total_loss: 4.387206077575684
training step: 37405, total_loss: 2.212557315826416
training step: 37406, total_loss: 2.2294585704803467
training step: 37407, total_loss: 2.850980281829834
training step: 37408, total_loss: 2.863091230392456
training step: 37409, total_loss: 2.546846389770508
training step: 37410, total_loss: 0.7427982091903687
training step: 37411, total_loss: 0.9861319065093994
training step: 37412, total_loss: 1.637544870376587
training step: 37413, total_loss: 3.396469831466675
training step: 37414, total_loss: 1.6528970003128052
training step: 37415, total_loss: 0.9119478464126587
training step: 37416, total_loss: 2.3321609497070312
training step: 37417, total_loss: 2.5876076221466064
training step: 37418, total_loss: 2.2656095027923584
training step: 37419, total_loss: 2.5000839233398438
training step: 37420, total_loss: 3.1447339057922363
training step: 37421, total_loss: 1.5884690284729004
training step: 37422, total_loss: 0.9272298812866211
training step: 37423, total_loss: 1.1205092668533325
training step: 37424, total_loss: 1.8404841423034668
training step: 37425, total_loss: 0.8781578540802002
training step: 37426, total_loss: 1.736373782157898
training step: 37427, total_loss: 3.8126559257507324
training step: 37428, total_loss: 2.839653968811035
training step: 37429, total_loss: 2.674204111099243
training step: 37430, total_loss: 1.998460292816162
training step: 37431, total_loss: 3.153829574584961
training step: 37432, total_loss: 0.7733476758003235
training step: 37433, total_loss: 2.1739134788513184
training step: 37434, total_loss: 3.605027914047241
training step: 37435, total_loss: 0.12296390533447266
training step: 37436, total_loss: 2.368435859680176
training step: 37437, total_loss: 2.911247491836548
training step: 37438, total_loss: 2.5884151458740234
training step: 37439, total_loss: 2.194417953491211
training step: 37440, total_loss: 3.279094696044922
training step: 37441, total_loss: 0.8201395273208618
training step: 37442, total_loss: 1.7278237342834473
training step: 37443, total_loss: 2.0114355087280273
training step: 37444, total_loss: 1.6862952709197998
training step: 37445, total_loss: 1.46519136428833
training step: 37446, total_loss: 3.4987595081329346
training step: 37447, total_loss: 0.8638706207275391
training step: 37448, total_loss: 2.455833911895752
training step: 37449, total_loss: 4.435642719268799
training step: 37450, total_loss: 1.6612229347229004
training step: 37451, total_loss: 2.0252933502197266
training step: 37452, total_loss: 3.3695144653320312
training step: 37453, total_loss: 1.688319444656372
training step: 37454, total_loss: 2.539968252182007
training step: 37455, total_loss: 0.9731935262680054
training step: 37456, total_loss: 3.8070266246795654
training step: 37457, total_loss: 0.6932392120361328
training step: 37458, total_loss: 3.9257538318634033
training step: 37459, total_loss: 2.1383769512176514
training step: 37460, total_loss: 2.0680770874023438
training step: 37461, total_loss: 0.42503833770751953
training step: 37462, total_loss: 3.859226942062378
training step: 37463, total_loss: 2.867262840270996
training step: 37464, total_loss: 2.4525039196014404
training step: 37465, total_loss: 0.7255107760429382
training step: 37466, total_loss: 0.2700679302215576
training step: 37467, total_loss: 1.5824546813964844
training step: 37468, total_loss: 2.3729310035705566
training step: 37469, total_loss: 4.221771240234375
training step: 37470, total_loss: 1.7684327363967896
training step: 37471, total_loss: 2.075784921646118
training step: 37472, total_loss: 0.9741253852844238
training step: 37473, total_loss: 1.7677440643310547
training step: 37474, total_loss: 1.6362621784210205
training step: 37475, total_loss: 1.885082483291626
training step: 37476, total_loss: 2.5004968643188477
training step: 37477, total_loss: 2.0842552185058594
training step: 37478, total_loss: 3.2696797847747803
training step: 37479, total_loss: 2.2887179851531982
training step: 37480, total_loss: 1.6674296855926514
training step: 37481, total_loss: 3.173938751220703
training step: 37482, total_loss: 1.4873093366622925
training step: 37483, total_loss: 4.186125755310059
training step: 37484, total_loss: 5.9730305671691895
training step: 37485, total_loss: 1.996230959892273
training step: 37486, total_loss: 4.533065319061279
training step: 37487, total_loss: 0.33406490087509155
training step: 37488, total_loss: 3.7771148681640625
training step: 37489, total_loss: 3.558060646057129
training step: 37490, total_loss: 3.1721508502960205
training step: 37491, total_loss: 3.5036888122558594
training step: 37492, total_loss: 4.359562873840332
training step: 37493, total_loss: 3.017319917678833
training step: 37494, total_loss: 1.8228483200073242
training step: 37495, total_loss: 1.9022245407104492
training step: 37496, total_loss: 0.7982879877090454
training step: 37497, total_loss: 3.8181328773498535
training step: 37498, total_loss: 0.847606897354126
training step: 37499, total_loss: 0.32922595739364624
training step: 37500, total_loss: 4.152954578399658
training step: 37501, total_loss: 3.198291778564453
training step: 37502, total_loss: 1.3502484560012817
training step: 37503, total_loss: 1.827630639076233
training step: 37504, total_loss: 2.8905129432678223
training step: 37505, total_loss: 0.6963135004043579
training step: 37506, total_loss: 2.287397623062134
training step: 37507, total_loss: 1.9139703512191772
training step: 37508, total_loss: 0.6055203676223755
training step: 37509, total_loss: 1.5973689556121826
training step: 37510, total_loss: 1.3359653949737549
training step: 37511, total_loss: 1.853170394897461
training step: 37512, total_loss: 1.3796558380126953
training step: 37513, total_loss: 4.96915864944458
training step: 37514, total_loss: 2.5292181968688965
training step: 37515, total_loss: 1.909780502319336
training step: 37516, total_loss: 3.53518009185791
training step: 37517, total_loss: 1.02805495262146
training step: 37518, total_loss: 2.903263568878174
training step: 37519, total_loss: 2.4907774925231934
training step: 37520, total_loss: 2.3994789123535156
training step: 37521, total_loss: 2.237344264984131
training step: 37522, total_loss: 2.1815545558929443
training step: 37523, total_loss: 0.20229345560073853
training step: 37524, total_loss: 5.2406511306762695
training step: 37525, total_loss: 0.7192150354385376
training step: 37526, total_loss: 1.8456733226776123
training step: 37527, total_loss: 2.846019744873047
training step: 37528, total_loss: 2.335693836212158
training step: 37529, total_loss: 6.451540946960449
training step: 37530, total_loss: 2.293503522872925
training step: 37531, total_loss: 0.9397708177566528
training step: 37532, total_loss: 0.9697949886322021
training step: 37533, total_loss: 3.263474702835083
training step: 37534, total_loss: 0.14660897850990295
training step: 37535, total_loss: 1.9511914253234863
training step: 37536, total_loss: 1.2781026363372803
training step: 37537, total_loss: 2.254526376724243
training step: 37538, total_loss: 3.4273180961608887
training step: 37539, total_loss: 1.9460080862045288
training step: 37540, total_loss: 2.6481409072875977
training step: 37541, total_loss: 1.6288020610809326
training step: 37542, total_loss: 1.1286512613296509
training step: 37543, total_loss: 3.51298451423645
training step: 37544, total_loss: 3.6272530555725098
training step: 37545, total_loss: 3.286472797393799
training step: 37546, total_loss: 1.9002718925476074
training step: 37547, total_loss: 2.740480899810791
training step: 37548, total_loss: 3.704195499420166
training step: 37549, total_loss: 5.141763687133789
training step: 37550, total_loss: 1.4544193744659424
training step: 37551, total_loss: 2.7474493980407715
training step: 37552, total_loss: 1.2447562217712402
training step: 37553, total_loss: 3.390573740005493
training step: 37554, total_loss: 0.06365463137626648
training step: 37555, total_loss: 2.362481117248535
training step: 37556, total_loss: 1.9572656154632568
training step: 37557, total_loss: 3.4513955116271973
training step: 37558, total_loss: 1.9422857761383057
training step: 37559, total_loss: 1.3466472625732422
training step: 37560, total_loss: 4.347060203552246
training step: 37561, total_loss: 2.4956936836242676
training step: 37562, total_loss: 1.7353017330169678
training step: 37563, total_loss: 0.8972719311714172
training step: 37564, total_loss: 0.9695738554000854
training step: 37565, total_loss: 3.3926138877868652
training step: 37566, total_loss: 2.0790343284606934
training step: 37567, total_loss: 3.868316650390625
training step: 37568, total_loss: 3.5140457153320312
training step: 37569, total_loss: 4.603277206420898
training step: 37570, total_loss: 1.8168377876281738
training step: 37571, total_loss: 0.6805126667022705
training step: 37572, total_loss: 1.95975661277771
training step: 37573, total_loss: 1.583019495010376
training step: 37574, total_loss: 1.5879263877868652
training step: 37575, total_loss: 0.6165555715560913
training step: 37576, total_loss: 3.0933098793029785
training step: 37577, total_loss: 1.5552959442138672
training step: 37578, total_loss: 3.6895647048950195
training step: 37579, total_loss: 1.428093671798706
training step: 37580, total_loss: 0.8093942403793335
training step: 37581, total_loss: 5.981569290161133
training step: 37582, total_loss: 0.4707566797733307
training step: 37583, total_loss: 3.0934386253356934
training step: 37584, total_loss: 3.21909236907959
training step: 37585, total_loss: 1.4380286931991577
training step: 37586, total_loss: 1.898578405380249
training step: 37587, total_loss: 0.9219932556152344
training step: 37588, total_loss: 1.3053820133209229
training step: 37589, total_loss: 0.46738308668136597
training step: 37590, total_loss: 2.817720413208008
training step: 37591, total_loss: 3.739431381225586
training step: 37592, total_loss: 0.006688239052891731
training step: 37593, total_loss: 5.284822463989258
training step: 37594, total_loss: 1.1896976232528687
training step: 37595, total_loss: 1.0293710231781006
training step: 37596, total_loss: 2.1009371280670166
training step: 37597, total_loss: 2.206643581390381
training step: 37598, total_loss: 1.1407755613327026
training step: 37599, total_loss: 1.890897274017334
training step: 37600, total_loss: 0.811124861240387
training step: 37601, total_loss: 2.273646593093872
training step: 37602, total_loss: 2.2754998207092285
training step: 37603, total_loss: 1.4502652883529663
training step: 37604, total_loss: 3.3758668899536133
training step: 37605, total_loss: 5.016683578491211
training step: 37606, total_loss: 3.7499308586120605
training step: 37607, total_loss: 3.5471019744873047
training step: 37608, total_loss: 1.6819649934768677
training step: 37609, total_loss: 0.4892246127128601
training step: 37610, total_loss: 2.3157057762145996
training step: 37611, total_loss: 2.920513868331909
training step: 37612, total_loss: 2.3799972534179688
training step: 37613, total_loss: 0.8400818705558777
training step: 37614, total_loss: 2.6847915649414062
training step: 37615, total_loss: 3.5709753036499023
training step: 37616, total_loss: 2.61854887008667
training step: 37617, total_loss: 2.988058090209961
training step: 37618, total_loss: 6.335808277130127
training step: 37619, total_loss: 1.7816829681396484
training step: 37620, total_loss: 1.6539387702941895
training step: 37621, total_loss: 5.015456199645996
training step: 37622, total_loss: 1.9662131071090698
training step: 37623, total_loss: 3.32733154296875
training step: 37624, total_loss: 3.7179460525512695
training step: 37625, total_loss: 2.3562779426574707
training step: 37626, total_loss: 2.3120627403259277
training step: 37627, total_loss: 2.8241991996765137
training step: 37628, total_loss: 1.3307032585144043
training step: 37629, total_loss: 3.641192674636841
training step: 37630, total_loss: 2.7246923446655273
training step: 37631, total_loss: 2.4703898429870605
training step: 37632, total_loss: 3.1533257961273193
training step: 37633, total_loss: 2.06927752494812
training step: 37634, total_loss: 3.7710859775543213
training step: 37635, total_loss: 1.5488452911376953
training step: 37636, total_loss: 1.3536174297332764
training step: 37637, total_loss: 2.8156676292419434
training step: 37638, total_loss: 2.959713935852051
training step: 37639, total_loss: 1.547304391860962
training step: 37640, total_loss: 2.5952110290527344
training step: 37641, total_loss: 3.242046356201172
training step: 37642, total_loss: 2.274810552597046
training step: 37643, total_loss: 1.2188783884048462
training step: 37644, total_loss: 3.328083038330078
training step: 37645, total_loss: 1.2148181200027466
training step: 37646, total_loss: 1.2538686990737915
training step: 37647, total_loss: 0.4866315722465515
training step: 37648, total_loss: 3.0598304271698
training step: 37649, total_loss: 2.2982544898986816
training step: 37650, total_loss: 1.7983911037445068
training step: 37651, total_loss: 1.745077133178711
training step: 37652, total_loss: 0.4668368399143219
training step: 37653, total_loss: 0.7792726755142212
training step: 37654, total_loss: 2.0045218467712402
training step: 37655, total_loss: 2.8021206855773926
training step: 37656, total_loss: 1.4509556293487549
training step: 37657, total_loss: 0.8456817269325256
training step: 37658, total_loss: 3.2828330993652344
training step: 37659, total_loss: 1.5641710758209229
training step: 37660, total_loss: 2.3200135231018066
training step: 37661, total_loss: 2.689394950866699
training step: 37662, total_loss: 1.637941598892212
training step: 37663, total_loss: 3.5346295833587646
training step: 37664, total_loss: 2.571497678756714
training step: 37665, total_loss: 0.6969159841537476
training step: 37666, total_loss: 4.336057662963867
training step: 37667, total_loss: 4.274012565612793
training step: 37668, total_loss: 1.4926831722259521
training step: 37669, total_loss: 0.9578937292098999
training step: 37670, total_loss: 2.7836742401123047
training step: 37671, total_loss: 0.19461214542388916
training step: 37672, total_loss: 2.906536102294922
training step: 37673, total_loss: 1.338874340057373
training step: 37674, total_loss: 1.848912239074707
training step: 37675, total_loss: 3.0395545959472656
training step: 37676, total_loss: 0.6388773918151855
training step: 37677, total_loss: 1.4405150413513184
training step: 37678, total_loss: 0.8423902988433838
training step: 37679, total_loss: 3.4549612998962402
training step: 37680, total_loss: 3.0530595779418945
training step: 37681, total_loss: 1.0918033123016357
training step: 37682, total_loss: 2.4634556770324707
training step: 37683, total_loss: 0.8290807604789734
training step: 37684, total_loss: 2.426086187362671
training step: 37685, total_loss: 2.51861572265625
training step: 37686, total_loss: 2.2299351692199707
training step: 37687, total_loss: 3.0045409202575684
training step: 37688, total_loss: 3.3849904537200928
training step: 37689, total_loss: 4.211606502532959
training step: 37690, total_loss: 0.549039363861084
training step: 37691, total_loss: 0.585663378238678
training step: 37692, total_loss: 0.9643176794052124
training step: 37693, total_loss: 1.4433672428131104
training step: 37694, total_loss: 1.0689787864685059
training step: 37695, total_loss: 0.6230164170265198
training step: 37696, total_loss: 1.1922246217727661
training step: 37697, total_loss: 1.254823923110962
training step: 37698, total_loss: 4.531523704528809
training step: 37699, total_loss: 1.5968568325042725
training step: 37700, total_loss: 1.6986782550811768
training step: 37701, total_loss: 3.067525863647461
training step: 37702, total_loss: 2.386185646057129
training step: 37703, total_loss: 0.7280608415603638
training step: 37704, total_loss: 3.9218597412109375
training step: 37705, total_loss: 1.9378416538238525
training step: 37706, total_loss: 4.159822463989258
training step: 37707, total_loss: 2.2474727630615234
training step: 37708, total_loss: 3.679989814758301
training step: 37709, total_loss: 1.9326716661453247
training step: 37710, total_loss: 2.5037600994110107
training step: 37711, total_loss: 0.1667613685131073
training step: 37712, total_loss: 0.435444712638855
training step: 37713, total_loss: 2.9483723640441895
training step: 37714, total_loss: 3.216102123260498
training step: 37715, total_loss: 2.4935462474823
training step: 37716, total_loss: 2.014517307281494
training step: 37717, total_loss: 0.8111020922660828
training step: 37718, total_loss: 3.4110257625579834
training step: 37719, total_loss: 1.3274697065353394
training step: 37720, total_loss: 1.6429940462112427
training step: 37721, total_loss: 2.6964666843414307
training step: 37722, total_loss: 3.591409206390381
training step: 37723, total_loss: 2.9289958477020264
training step: 37724, total_loss: 0.39873746037483215
training step: 37725, total_loss: 1.970043420791626
training step: 37726, total_loss: 3.3472042083740234
training step: 37727, total_loss: 1.0784568786621094
training step: 37728, total_loss: 0.49082034826278687
training step: 37729, total_loss: 1.7874476909637451
training step: 37730, total_loss: 1.0134227275848389
training step: 37731, total_loss: 1.2724123001098633
training step: 37732, total_loss: 3.641324281692505
training step: 37733, total_loss: 2.2304975986480713
training step: 37734, total_loss: 3.146728992462158
training step: 37735, total_loss: 1.7552831172943115
training step: 37736, total_loss: 2.2086260318756104
training step: 37737, total_loss: 0.23794929683208466
training step: 37738, total_loss: 1.796720266342163
training step: 37739, total_loss: 2.4629645347595215
training step: 37740, total_loss: 1.6925852298736572
training step: 37741, total_loss: 4.132416248321533
training step: 37742, total_loss: 2.694416046142578
training step: 37743, total_loss: 2.7318716049194336
training step: 37744, total_loss: 3.8227615356445312
training step: 37745, total_loss: 0.8767507672309875
training step: 37746, total_loss: 4.881354331970215
training step: 37747, total_loss: 1.1127384901046753
training step: 37748, total_loss: 1.7835240364074707
training step: 37749, total_loss: 1.515427589416504
training step: 37750, total_loss: 3.593738079071045
training step: 37751, total_loss: 2.560676097869873
training step: 37752, total_loss: 0.0066808369010686874
training step: 37753, total_loss: 4.238664627075195
training step: 37754, total_loss: 3.8253068923950195
training step: 37755, total_loss: 3.175595283508301
training step: 37756, total_loss: 3.13104248046875
training step: 37757, total_loss: 4.429732322692871
training step: 37758, total_loss: 2.001357078552246
training step: 37759, total_loss: 2.116182565689087
training step: 37760, total_loss: 1.9912444353103638
training step: 37761, total_loss: 1.5027852058410645
training step: 37762, total_loss: 2.373512029647827
training step: 37763, total_loss: 1.047078013420105
training step: 37764, total_loss: 5.153718948364258
training step: 37765, total_loss: 1.2756284475326538
training step: 37766, total_loss: 3.3520045280456543
training step: 37767, total_loss: 0.7839769721031189
training step: 37768, total_loss: 2.204845428466797
training step: 37769, total_loss: 2.341947555541992
training step: 37770, total_loss: 2.5498008728027344
training step: 37771, total_loss: 1.1816697120666504
training step: 37772, total_loss: 2.5254931449890137
training step: 37773, total_loss: 1.8777706623077393
training step: 37774, total_loss: 1.0816712379455566
training step: 37775, total_loss: 2.580960750579834
training step: 37776, total_loss: 3.492814779281616
training step: 37777, total_loss: 2.106400966644287
training step: 37778, total_loss: 1.422053575515747
training step: 37779, total_loss: 1.2634811401367188
training step: 37780, total_loss: 3.185309886932373
training step: 37781, total_loss: 1.4053044319152832
training step: 37782, total_loss: 0.7267676591873169
training step: 37783, total_loss: 3.9790749549865723
training step: 37784, total_loss: 3.6471495628356934
training step: 37785, total_loss: 1.2911721467971802
training step: 37786, total_loss: 3.3721437454223633
training step: 37787, total_loss: 0.9459109306335449
training step: 37788, total_loss: 2.663784980773926
training step: 37789, total_loss: 2.4858741760253906
training step: 37790, total_loss: 2.4057822227478027
training step: 37791, total_loss: 1.0032997131347656
training step: 37792, total_loss: 3.1733510494232178
training step: 37793, total_loss: 1.1610913276672363
training step: 37794, total_loss: 1.4352130889892578
training step: 37795, total_loss: 1.7046517133712769
training step: 37796, total_loss: 1.6100986003875732
training step: 37797, total_loss: 2.7418060302734375
training step: 37798, total_loss: 1.6929347515106201
training step: 37799, total_loss: 3.2351953983306885
training step: 37800, total_loss: 0.4565127491950989
training step: 37801, total_loss: 4.8519697189331055
training step: 37802, total_loss: 2.0448246002197266
training step: 37803, total_loss: 3.287954092025757
training step: 37804, total_loss: 0.2572596073150635
training step: 37805, total_loss: 1.8741121292114258
training step: 37806, total_loss: 0.8003417253494263
training step: 37807, total_loss: 3.6030783653259277
training step: 37808, total_loss: 4.047330379486084
training step: 37809, total_loss: 1.782318115234375
training step: 37810, total_loss: 2.976161479949951
training step: 37811, total_loss: 3.717393398284912
training step: 37812, total_loss: 1.591972827911377
training step: 37813, total_loss: 1.9486325979232788
training step: 37814, total_loss: 3.995994806289673
training step: 37815, total_loss: 1.8674873113632202
training step: 37816, total_loss: 1.8620052337646484
training step: 37817, total_loss: 1.2668421268463135
training step: 37818, total_loss: 2.4464621543884277
training step: 37819, total_loss: 2.0216543674468994
training step: 37820, total_loss: 2.6189754009246826
training step: 37821, total_loss: 3.650723934173584
training step: 37822, total_loss: 2.5663340091705322
training step: 37823, total_loss: 3.621148109436035
training step: 37824, total_loss: 1.224302887916565
training step: 37825, total_loss: 2.1005983352661133
training step: 37826, total_loss: 3.5882914066314697
training step: 37827, total_loss: 3.1381030082702637
training step: 37828, total_loss: 3.275707721710205
training step: 37829, total_loss: 3.1527342796325684
training step: 37830, total_loss: 1.3872549533843994
training step: 37831, total_loss: 2.7783944606781006
training step: 37832, total_loss: 0.9808210134506226
training step: 37833, total_loss: 2.7150375843048096
training step: 37834, total_loss: 1.5754419565200806
training step: 37835, total_loss: 2.6542458534240723
training step: 37836, total_loss: 1.1885817050933838
training step: 37837, total_loss: 5.3970627784729
training step: 37838, total_loss: 3.3864924907684326
training step: 37839, total_loss: 3.2073874473571777
training step: 37840, total_loss: 2.599806308746338
training step: 37841, total_loss: 1.8673405647277832
training step: 37842, total_loss: 1.0211093425750732
training step: 37843, total_loss: 2.9386236667633057
training step: 37844, total_loss: 4.003816604614258
training step: 37845, total_loss: 2.0063745975494385
training step: 37846, total_loss: 1.1236125230789185
training step: 37847, total_loss: 0.4597357213497162
training step: 37848, total_loss: 1.5411858558654785
training step: 37849, total_loss: 1.0960440635681152
training step: 37850, total_loss: 2.7582476139068604
training step: 37851, total_loss: 4.396614074707031
training step: 37852, total_loss: 1.4278278350830078
training step: 37853, total_loss: 0.7817164063453674
training step: 37854, total_loss: 1.583883285522461
training step: 37855, total_loss: 0.9364748597145081
training step: 37856, total_loss: 1.1134109497070312
training step: 37857, total_loss: 2.4010181427001953
training step: 37858, total_loss: 1.640714168548584
training step: 37859, total_loss: 1.2063480615615845
training step: 37860, total_loss: 2.369370460510254
training step: 37861, total_loss: 2.4909727573394775
training step: 37862, total_loss: 2.518411636352539
training step: 37863, total_loss: 1.8581397533416748
training step: 37864, total_loss: 0.8401027917861938
training step: 37865, total_loss: 3.3653388023376465
training step: 37866, total_loss: 0.02605951577425003
training step: 37867, total_loss: 1.159371256828308
training step: 37868, total_loss: 2.1043779850006104
training step: 37869, total_loss: 6.0761332511901855
training step: 37870, total_loss: 0.8837176561355591
training step: 37871, total_loss: 2.6796817779541016
training step: 37872, total_loss: 3.195347547531128
training step: 37873, total_loss: 1.8042287826538086
training step: 37874, total_loss: 5.179621696472168
training step: 37875, total_loss: 3.1258339881896973
training step: 37876, total_loss: 1.167843222618103
training step: 37877, total_loss: 2.1297101974487305
training step: 37878, total_loss: 2.462089776992798
training step: 37879, total_loss: 0.012440508231520653
training step: 37880, total_loss: 4.01931095123291
training step: 37881, total_loss: 1.7332730293273926
training step: 37882, total_loss: 3.3577520847320557
training step: 37883, total_loss: 4.540952682495117
training step: 37884, total_loss: 0.21776100993156433
training step: 37885, total_loss: 5.366257667541504
training step: 37886, total_loss: 1.211970329284668
training step: 37887, total_loss: 1.6247729063034058
training step: 37888, total_loss: 1.1151429414749146
training step: 37889, total_loss: 5.42492151260376
training step: 37890, total_loss: 2.2702062129974365
training step: 37891, total_loss: 0.2845034897327423
training step: 37892, total_loss: 1.9426621198654175
training step: 37893, total_loss: 0.13754260540008545
training step: 37894, total_loss: 3.288177967071533
training step: 37895, total_loss: 3.444779634475708
training step: 37896, total_loss: 2.0765843391418457
training step: 37897, total_loss: 0.3498063385486603
training step: 37898, total_loss: 0.5019150376319885
training step: 37899, total_loss: 4.46100378036499
training step: 37900, total_loss: 4.540748119354248
training step: 37901, total_loss: 1.0595462322235107
training step: 37902, total_loss: 1.4987685680389404
training step: 37903, total_loss: 1.2968392372131348
training step: 37904, total_loss: 2.7427423000335693
training step: 37905, total_loss: 1.304250955581665
training step: 37906, total_loss: 1.683752179145813
training step: 37907, total_loss: 3.906712532043457
training step: 37908, total_loss: 3.712688446044922
training step: 37909, total_loss: 4.353771209716797
training step: 37910, total_loss: 0.8680177330970764
training step: 37911, total_loss: 4.52936315536499
training step: 37912, total_loss: 1.2997753620147705
training step: 37913, total_loss: 0.3540535569190979
training step: 37914, total_loss: 1.997646450996399
training step: 37915, total_loss: 3.389118194580078
training step: 37916, total_loss: 4.429107666015625
training step: 37917, total_loss: 6.097868919372559
training step: 37918, total_loss: 1.8259861469268799
training step: 37919, total_loss: 1.554051399230957
training step: 37920, total_loss: 2.5436887741088867
training step: 37921, total_loss: 2.533053159713745
training step: 37922, total_loss: 1.7304017543792725
training step: 37923, total_loss: 0.6190304160118103
training step: 37924, total_loss: 4.456578254699707
training step: 37925, total_loss: 3.9773006439208984
training step: 37926, total_loss: 0.26915261149406433
training step: 37927, total_loss: 0.07906730473041534
training step: 37928, total_loss: 1.1451776027679443
training step: 37929, total_loss: 1.645984411239624
training step: 37930, total_loss: 1.8613648414611816
training step: 37931, total_loss: 1.669226884841919
training step: 37932, total_loss: 1.5390681028366089
training step: 37933, total_loss: 2.4173765182495117
training step: 37934, total_loss: 2.4440078735351562
training step: 37935, total_loss: 1.0427062511444092
training step: 37936, total_loss: 3.6263890266418457
training step: 37937, total_loss: 1.785211205482483
training step: 37938, total_loss: 1.787649154663086
training step: 37939, total_loss: 1.7694475650787354
training step: 37940, total_loss: 1.7064728736877441
training step: 37941, total_loss: 2.137678384780884
training step: 37942, total_loss: 4.437337875366211
training step: 37943, total_loss: 0.8635531663894653
training step: 37944, total_loss: 3.9891090393066406
training step: 37945, total_loss: 3.655108690261841
training step: 37946, total_loss: 1.3693114519119263
training step: 37947, total_loss: 1.1675070524215698
training step: 37948, total_loss: 1.0862321853637695
training step: 37949, total_loss: 4.571265697479248
training step: 37950, total_loss: 1.3616045713424683
training step: 37951, total_loss: 2.1566171646118164
training step: 37952, total_loss: 2.1360890865325928
training step: 37953, total_loss: 1.297340750694275
training step: 37954, total_loss: 0.7805710434913635
training step: 37955, total_loss: 2.809685468673706
training step: 37956, total_loss: 2.608895778656006
training step: 37957, total_loss: 0.5496643781661987
training step: 37958, total_loss: 0.8622525930404663
training step: 37959, total_loss: 4.581618309020996
training step: 37960, total_loss: 0.736125111579895
training step: 37961, total_loss: 2.0940070152282715
training step: 37962, total_loss: 2.203451156616211
training step: 37963, total_loss: 3.4864089488983154
training step: 37964, total_loss: 2.151651382446289
training step: 37965, total_loss: 0.38675639033317566
training step: 37966, total_loss: 2.968184471130371
training step: 37967, total_loss: 0.7113316655158997
training step: 37968, total_loss: 1.3530383110046387
training step: 37969, total_loss: 1.46647310256958
training step: 37970, total_loss: 2.2349393367767334
training step: 37971, total_loss: 1.9463152885437012
training step: 37972, total_loss: 1.3060729503631592
training step: 37973, total_loss: 2.465998888015747
training step: 37974, total_loss: 0.5914782881736755
training step: 37975, total_loss: 0.046947017312049866
training step: 37976, total_loss: 5.73792839050293
training step: 37977, total_loss: 2.986560583114624
training step: 37978, total_loss: 1.8541864156723022
training step: 37979, total_loss: 0.6671035885810852
training step: 37980, total_loss: 1.7355611324310303
training step: 37981, total_loss: 3.7074103355407715INFO:tensorflow:Writing predictions to: test_output/predictions_38000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_38000.json

training step: 37982, total_loss: 2.8970258235931396
training step: 37983, total_loss: 0.47965025901794434
training step: 37984, total_loss: 0.8256552219390869
training step: 37985, total_loss: 2.426175594329834
training step: 37986, total_loss: 3.4019052982330322
training step: 37987, total_loss: 0.9127145409584045
training step: 37988, total_loss: 0.6292641162872314
training step: 37989, total_loss: 1.2621979713439941
training step: 37990, total_loss: 1.0728760957717896
training step: 37991, total_loss: 2.8173985481262207
training step: 37992, total_loss: 2.8864755630493164
training step: 37993, total_loss: 1.4771862030029297
training step: 37994, total_loss: 2.1115517616271973
training step: 37995, total_loss: 3.321096897125244
training step: 37996, total_loss: 2.424882650375366
training step: 37997, total_loss: 0.22527818381786346
training step: 37998, total_loss: 0.3574718236923218
training step: 37999, total_loss: 4.173379898071289
training step: 38000, total_loss: 0.4053197503089905
epoch finished! shuffle=False
evaluation: 2000, total_loss: 1.7454746961593628, f1: 55.00410244533282, followup: 23.900806328921345, yesno: 57.76662102540697, heq: 49.53598052639586, dheq: 3.1

Model saved in path test_output//model_38000.ckpt
training step: 38001, total_loss: 0.2323082983493805
training step: 38002, total_loss: 0.15617704391479492
training step: 38003, total_loss: 0.748794674873352
training step: 38004, total_loss: 5.8831787109375
training step: 38005, total_loss: 2.8420042991638184
training step: 38006, total_loss: 0.584720253944397
training step: 38007, total_loss: 1.7378898859024048
training step: 38008, total_loss: 4.091850757598877
training step: 38009, total_loss: 4.294152736663818
training step: 38010, total_loss: 1.5864171981811523
training step: 38011, total_loss: 6.250127792358398
training step: 38012, total_loss: 1.087198257446289
training step: 38013, total_loss: 1.9823118448257446
training step: 38014, total_loss: 3.6667234897613525
training step: 38015, total_loss: 2.71114444732666
training step: 38016, total_loss: 0.5890506505966187
training step: 38017, total_loss: 3.796877861022949
training step: 38018, total_loss: 2.7156810760498047
training step: 38019, total_loss: 3.5837912559509277
training step: 38020, total_loss: 4.032092094421387
training step: 38021, total_loss: 2.7285876274108887
training step: 38022, total_loss: 2.451948404312134
training step: 38023, total_loss: 0.8473820686340332
training step: 38024, total_loss: 1.3490854501724243
training step: 38025, total_loss: 0.5983232259750366
training step: 38026, total_loss: 1.9809269905090332
training step: 38027, total_loss: 2.7610976696014404
training step: 38028, total_loss: 2.006540536880493
training step: 38029, total_loss: 1.5103925466537476
training step: 38030, total_loss: 2.615429162979126
training step: 38031, total_loss: 1.4282405376434326
training step: 38032, total_loss: 5.117554664611816
training step: 38033, total_loss: 2.592015027999878
training step: 38034, total_loss: 0.9690120220184326
training step: 38035, total_loss: 3.1582415103912354
training step: 38036, total_loss: 3.193016290664673
training step: 38037, total_loss: 2.6305198669433594
training step: 38038, total_loss: 0.596108078956604
training step: 38039, total_loss: 5.574760913848877
training step: 38040, total_loss: 1.9821802377700806
training step: 38041, total_loss: 4.4629669189453125
training step: 38042, total_loss: 1.9374058246612549
training step: 38043, total_loss: 1.418695330619812
training step: 38044, total_loss: 3.028268337249756
training step: 38045, total_loss: 2.0493533611297607
training step: 38046, total_loss: 1.4595838785171509
training step: 38047, total_loss: 1.9627442359924316
training step: 38048, total_loss: 5.233266353607178
training step: 38049, total_loss: 1.826804518699646
training step: 38050, total_loss: 2.030102252960205
training step: 38051, total_loss: 2.1727004051208496
training step: 38052, total_loss: 1.355546474456787
training step: 38053, total_loss: 2.1069259643554688
training step: 38054, total_loss: 1.0397412776947021
training step: 38055, total_loss: 1.245824933052063
training step: 38056, total_loss: 1.1954010725021362
training step: 38057, total_loss: 1.7612086534500122
training step: 38058, total_loss: 2.4712576866149902
training step: 38059, total_loss: 1.799462080001831
training step: 38060, total_loss: 0.10724274814128876
training step: 38061, total_loss: 2.067068338394165
training step: 38062, total_loss: 7.141761302947998
training step: 38063, total_loss: 3.365408420562744
training step: 38064, total_loss: 1.5284593105316162
training step: 38065, total_loss: 2.104621648788452
training step: 38066, total_loss: 0.8906906247138977
training step: 38067, total_loss: 2.1087684631347656
training step: 38068, total_loss: 2.7820284366607666
training step: 38069, total_loss: 3.690718173980713
training step: 38070, total_loss: 1.8525543212890625
training step: 38071, total_loss: 4.784035682678223
training step: 38072, total_loss: 0.16444069147109985
training step: 38073, total_loss: 3.4471969604492188
training step: 38074, total_loss: 5.767995834350586
training step: 38075, total_loss: 1.5756826400756836
training step: 38076, total_loss: 2.8521595001220703
training step: 38077, total_loss: 3.6841745376586914
training step: 38078, total_loss: 1.3993990421295166
training step: 38079, total_loss: 0.8464333415031433
training step: 38080, total_loss: 2.2298049926757812
training step: 38081, total_loss: 0.1810058206319809
training step: 38082, total_loss: 1.7684357166290283
training step: 38083, total_loss: 1.6419111490249634
training step: 38084, total_loss: 3.8348312377929688
training step: 38085, total_loss: 3.883324146270752
training step: 38086, total_loss: 2.188507556915283
training step: 38087, total_loss: 0.12918223440647125
training step: 38088, total_loss: 1.6613328456878662
training step: 38089, total_loss: 2.6093428134918213
training step: 38090, total_loss: 1.9796345233917236
training step: 38091, total_loss: 1.3965672254562378
training step: 38092, total_loss: 2.759215831756592
training step: 38093, total_loss: 2.1433327198028564
training step: 38094, total_loss: 3.487058162689209
training step: 38095, total_loss: 0.29514995217323303
training step: 38096, total_loss: 2.244335174560547
training step: 38097, total_loss: 3.19637131690979
training step: 38098, total_loss: 0.7050530910491943
training step: 38099, total_loss: 3.6617250442504883
training step: 38100, total_loss: 1.453404188156128
training step: 38101, total_loss: 1.15777587890625
training step: 38102, total_loss: 0.08627866953611374
training step: 38103, total_loss: 1.880159854888916
training step: 38104, total_loss: 1.0071239471435547
training step: 38105, total_loss: 2.288388252258301
training step: 38106, total_loss: 0.05771830305457115
training step: 38107, total_loss: 0.20306384563446045
training step: 38108, total_loss: 1.816990852355957
training step: 38109, total_loss: 2.3379592895507812
training step: 38110, total_loss: 4.354559898376465
training step: 38111, total_loss: 2.302544116973877
training step: 38112, total_loss: 6.338864803314209
training step: 38113, total_loss: 4.602938652038574
training step: 38114, total_loss: 2.127467155456543
training step: 38115, total_loss: 0.5287808179855347
training step: 38116, total_loss: 1.5863999128341675
training step: 38117, total_loss: 4.645918846130371
training step: 38118, total_loss: 2.3714401721954346
training step: 38119, total_loss: 2.6223621368408203
training step: 38120, total_loss: 1.7480661869049072
training step: 38121, total_loss: 3.240799903869629
training step: 38122, total_loss: 0.6122907400131226
training step: 38123, total_loss: 1.4348453283309937
training step: 38124, total_loss: 1.8472682237625122
training step: 38125, total_loss: 2.9219589233398438
training step: 38126, total_loss: 1.9302716255187988
training step: 38127, total_loss: 6.42571496963501
training step: 38128, total_loss: 0.6144148707389832
training step: 38129, total_loss: 3.9129202365875244
training step: 38130, total_loss: 1.171033501625061
training step: 38131, total_loss: 4.955416679382324
training step: 38132, total_loss: 1.8606956005096436
training step: 38133, total_loss: 2.174497604370117
training step: 38134, total_loss: 3.3479015827178955
training step: 38135, total_loss: 2.6214966773986816
training step: 38136, total_loss: 1.8749048709869385
training step: 38137, total_loss: 2.9559478759765625
training step: 38138, total_loss: 0.6548778414726257
training step: 38139, total_loss: 1.532151222229004
training step: 38140, total_loss: 1.5821468830108643
training step: 38141, total_loss: 1.9232642650604248
training step: 38142, total_loss: 1.5423049926757812
training step: 38143, total_loss: 2.864903211593628
training step: 38144, total_loss: 3.0623817443847656
training step: 38145, total_loss: 2.547056198120117
training step: 38146, total_loss: 3.250206708908081
training step: 38147, total_loss: 0.9255896806716919
training step: 38148, total_loss: 1.6756718158721924
training step: 38149, total_loss: 2.4538373947143555
training step: 38150, total_loss: 1.609555721282959
training step: 38151, total_loss: 0.9475351572036743
training step: 38152, total_loss: 2.4016036987304688
training step: 38153, total_loss: 2.3443233966827393
training step: 38154, total_loss: 1.1542587280273438
training step: 38155, total_loss: 1.5781519412994385
training step: 38156, total_loss: 1.4671096801757812
training step: 38157, total_loss: 1.8702678680419922
training step: 38158, total_loss: 2.1807708740234375
training step: 38159, total_loss: 2.553953170776367
training step: 38160, total_loss: 2.2170777320861816
training step: 38161, total_loss: 0.40405532717704773
training step: 38162, total_loss: 1.3043361902236938
training step: 38163, total_loss: 3.088850259780884
training step: 38164, total_loss: 3.2786083221435547
training step: 38165, total_loss: 1.86806321144104
training step: 38166, total_loss: 2.6486942768096924
training step: 38167, total_loss: 4.745907306671143
training step: 38168, total_loss: 2.4590353965759277
training step: 38169, total_loss: 2.3034143447875977
training step: 38170, total_loss: 3.2236146926879883
training step: 38171, total_loss: 2.7467422485351562
training step: 38172, total_loss: 0.9779314994812012
training step: 38173, total_loss: 3.8776917457580566
training step: 38174, total_loss: 2.920686721801758
training step: 38175, total_loss: 1.8872060775756836
training step: 38176, total_loss: 2.2451891899108887
training step: 38177, total_loss: 1.664433479309082
training step: 38178, total_loss: 1.0039355754852295
training step: 38179, total_loss: 0.9253948330879211
training step: 38180, total_loss: 2.879171133041382
training step: 38181, total_loss: 2.508275032043457
training step: 38182, total_loss: 1.759178876876831
training step: 38183, total_loss: 3.476256847381592
training step: 38184, total_loss: 3.944364547729492
training step: 38185, total_loss: 4.334179878234863
training step: 38186, total_loss: 3.9030942916870117
training step: 38187, total_loss: 0.8075424432754517
training step: 38188, total_loss: 2.076319456100464
training step: 38189, total_loss: 3.365422248840332
training step: 38190, total_loss: 3.1808595657348633
training step: 38191, total_loss: 0.5890864133834839
training step: 38192, total_loss: 2.1657357215881348
training step: 38193, total_loss: 2.868147373199463
training step: 38194, total_loss: 2.8518199920654297
training step: 38195, total_loss: 1.420458436012268
training step: 38196, total_loss: 3.085740089416504
training step: 38197, total_loss: 4.0518364906311035
training step: 38198, total_loss: 3.6822237968444824
training step: 38199, total_loss: 1.6710882186889648
training step: 38200, total_loss: 2.2063519954681396
training step: 38201, total_loss: 2.877443552017212
training step: 38202, total_loss: 0.3300546705722809
training step: 38203, total_loss: 1.9918873310089111
training step: 38204, total_loss: 2.573615074157715
training step: 38205, total_loss: 2.2541494369506836
training step: 38206, total_loss: 1.509242057800293
training step: 38207, total_loss: 1.6570250988006592
training step: 38208, total_loss: 1.117088794708252
training step: 38209, total_loss: 4.347653388977051
training step: 38210, total_loss: 1.7317358255386353
training step: 38211, total_loss: 3.3519513607025146
training step: 38212, total_loss: 2.489995241165161
training step: 38213, total_loss: 2.1081106662750244
training step: 38214, total_loss: 0.832582950592041
training step: 38215, total_loss: 1.0895733833312988
training step: 38216, total_loss: 0.0096169114112854
training step: 38217, total_loss: 1.845271110534668
training step: 38218, total_loss: 1.9213435649871826
training step: 38219, total_loss: 0.24646911025047302
training step: 38220, total_loss: 2.3284764289855957
training step: 38221, total_loss: 1.268192172050476
training step: 38222, total_loss: 1.4817323684692383
training step: 38223, total_loss: 1.277083396911621
training step: 38224, total_loss: 1.0031814575195312
training step: 38225, total_loss: 4.415679931640625
training step: 38226, total_loss: 4.687594413757324
training step: 38227, total_loss: 4.730229377746582
training step: 38228, total_loss: 3.7606749534606934
training step: 38229, total_loss: 3.5587997436523438
training step: 38230, total_loss: 0.8848576545715332
training step: 38231, total_loss: 2.774862766265869
training step: 38232, total_loss: 7.360323905944824
training step: 38233, total_loss: 3.088555097579956
training step: 38234, total_loss: 2.439995527267456
training step: 38235, total_loss: 2.2217044830322266
training step: 38236, total_loss: 1.4824512004852295
training step: 38237, total_loss: 2.024277687072754
training step: 38238, total_loss: 1.5357029438018799
training step: 38239, total_loss: 2.035869598388672
training step: 38240, total_loss: 0.7968435287475586
training step: 38241, total_loss: 2.232152223587036
training step: 38242, total_loss: 2.447878122329712
training step: 38243, total_loss: 0.8378549814224243
training step: 38244, total_loss: 2.691439390182495
training step: 38245, total_loss: 1.207696557044983
training step: 38246, total_loss: 1.282968521118164
training step: 38247, total_loss: 2.0506253242492676
training step: 38248, total_loss: 3.7692432403564453
training step: 38249, total_loss: 2.3239188194274902
training step: 38250, total_loss: 0.45552584528923035
training step: 38251, total_loss: 0.6189162135124207
training step: 38252, total_loss: 1.8503992557525635
training step: 38253, total_loss: 0.9760386347770691
training step: 38254, total_loss: 4.244452953338623
training step: 38255, total_loss: 1.8863856792449951
training step: 38256, total_loss: 0.9831278920173645
training step: 38257, total_loss: 0.5775383710861206
training step: 38258, total_loss: 1.8990423679351807
training step: 38259, total_loss: 1.2544463872909546
training step: 38260, total_loss: 2.940329074859619
training step: 38261, total_loss: 1.4396861791610718
training step: 38262, total_loss: 3.088437080383301
training step: 38263, total_loss: 2.04524827003479
training step: 38264, total_loss: 2.38814640045166
training step: 38265, total_loss: 2.2937934398651123
training step: 38266, total_loss: 4.499473571777344
training step: 38267, total_loss: 0.25059592723846436
training step: 38268, total_loss: 2.1916415691375732
training step: 38269, total_loss: 1.101494550704956
training step: 38270, total_loss: 2.2066729068756104
training step: 38271, total_loss: 8.68008041381836
training step: 38272, total_loss: 3.726872682571411
training step: 38273, total_loss: 1.026503562927246
training step: 38274, total_loss: 3.5002083778381348
training step: 38275, total_loss: 2.97884202003479
training step: 38276, total_loss: 3.140578269958496
training step: 38277, total_loss: 0.016334736719727516
training step: 38278, total_loss: 4.329732418060303
training step: 38279, total_loss: 1.7520725727081299
training step: 38280, total_loss: 3.6173653602600098
training step: 38281, total_loss: 2.8686418533325195
training step: 38282, total_loss: 1.8720927238464355
training step: 38283, total_loss: 2.4640698432922363
training step: 38284, total_loss: 2.6155295372009277
training step: 38285, total_loss: 2.010402202606201
training step: 38286, total_loss: 3.5571889877319336
training step: 38287, total_loss: 0.963439404964447
training step: 38288, total_loss: 2.8222885131835938
training step: 38289, total_loss: 0.5413610339164734
training step: 38290, total_loss: 0.11352570354938507
training step: 38291, total_loss: 2.275590658187866
training step: 38292, total_loss: 1.3996913433074951
training step: 38293, total_loss: 2.9284207820892334
training step: 38294, total_loss: 2.2766404151916504
training step: 38295, total_loss: 2.0104124546051025
training step: 38296, total_loss: 0.9384108781814575
training step: 38297, total_loss: 1.8547160625457764
training step: 38298, total_loss: 3.982180595397949
training step: 38299, total_loss: 0.8841984272003174
training step: 38300, total_loss: 2.652451515197754
training step: 38301, total_loss: 1.7794702053070068
training step: 38302, total_loss: 2.2788774967193604
training step: 38303, total_loss: 0.6571053266525269
training step: 38304, total_loss: 1.8311939239501953
training step: 38305, total_loss: 1.025320053100586
training step: 38306, total_loss: 1.8393083810806274
training step: 38307, total_loss: 4.75475549697876
training step: 38308, total_loss: 3.2282588481903076
training step: 38309, total_loss: 2.9978036880493164
training step: 38310, total_loss: 0.5267336368560791
training step: 38311, total_loss: 1.297303318977356
training step: 38312, total_loss: 1.1802033185958862
training step: 38313, total_loss: 0.6332044005393982
training step: 38314, total_loss: 2.6756844520568848
training step: 38315, total_loss: 2.8038763999938965
training step: 38316, total_loss: 5.525643348693848
training step: 38317, total_loss: 2.915402889251709
training step: 38318, total_loss: 0.0804048702120781
training step: 38319, total_loss: 0.9655942320823669
training step: 38320, total_loss: 2.571599006652832
training step: 38321, total_loss: 3.6898951530456543
training step: 38322, total_loss: 1.8320894241333008
training step: 38323, total_loss: 1.995443344116211
training step: 38324, total_loss: 1.2944607734680176
training step: 38325, total_loss: 2.920962333679199
training step: 38326, total_loss: 2.588160276412964
training step: 38327, total_loss: 2.6511635780334473
training step: 38328, total_loss: 2.657839298248291
training step: 38329, total_loss: 1.484597086906433
training step: 38330, total_loss: 1.4264700412750244
training step: 38331, total_loss: 0.5522618889808655
training step: 38332, total_loss: 3.5256857872009277
training step: 38333, total_loss: 1.3991713523864746
training step: 38334, total_loss: 5.2764153480529785
training step: 38335, total_loss: 0.8686249852180481
training step: 38336, total_loss: 2.23606014251709
training step: 38337, total_loss: 1.5215859413146973
training step: 38338, total_loss: 2.0392494201660156
training step: 38339, total_loss: 5.609066009521484
training step: 38340, total_loss: 3.6253409385681152
training step: 38341, total_loss: 0.7063688635826111
training step: 38342, total_loss: 0.555766224861145
training step: 38343, total_loss: 4.164856910705566
training step: 38344, total_loss: 0.5864943265914917
training step: 38345, total_loss: 1.6587567329406738
training step: 38346, total_loss: 1.342781662940979
training step: 38347, total_loss: 4.137110710144043
training step: 38348, total_loss: 1.479241132736206
training step: 38349, total_loss: 2.2133359909057617
training step: 38350, total_loss: 2.176431655883789
training step: 38351, total_loss: 0.01766575127840042
training step: 38352, total_loss: 0.8244962692260742
training step: 38353, total_loss: 1.2826554775238037
training step: 38354, total_loss: 3.7605504989624023
training step: 38355, total_loss: 2.7411282062530518
training step: 38356, total_loss: 1.61903715133667
training step: 38357, total_loss: 2.1301231384277344
training step: 38358, total_loss: 3.056494951248169
training step: 38359, total_loss: 2.136535406112671
training step: 38360, total_loss: 2.1471681594848633
training step: 38361, total_loss: 1.487271785736084
training step: 38362, total_loss: 3.9794390201568604
training step: 38363, total_loss: 3.230861186981201
training step: 38364, total_loss: 1.5719165802001953
training step: 38365, total_loss: 3.4027867317199707
training step: 38366, total_loss: 2.5208473205566406
training step: 38367, total_loss: 1.3676915168762207
training step: 38368, total_loss: 3.1478652954101562
training step: 38369, total_loss: 0.537700355052948
training step: 38370, total_loss: 2.4078710079193115
training step: 38371, total_loss: 1.6609631776809692
training step: 38372, total_loss: 1.4681271314620972
training step: 38373, total_loss: 2.7569150924682617
training step: 38374, total_loss: 1.564447045326233
training step: 38375, total_loss: 2.2468714714050293
training step: 38376, total_loss: 1.8410296440124512
training step: 38377, total_loss: 2.0518572330474854
training step: 38378, total_loss: 0.9840507507324219
training step: 38379, total_loss: 3.275315284729004
training step: 38380, total_loss: 4.13608980178833
training step: 38381, total_loss: 3.1070919036865234
training step: 38382, total_loss: 0.08703406155109406
training step: 38383, total_loss: 2.6609809398651123
training step: 38384, total_loss: 2.1175873279571533
training step: 38385, total_loss: 1.7806987762451172
training step: 38386, total_loss: 2.3032021522521973
training step: 38387, total_loss: 3.8469948768615723
training step: 38388, total_loss: 2.960465908050537
training step: 38389, total_loss: 1.7063159942626953
training step: 38390, total_loss: 0.6042129397392273
training step: 38391, total_loss: 3.928255081176758
training step: 38392, total_loss: 4.641867637634277
training step: 38393, total_loss: 1.088133454322815
training step: 38394, total_loss: 1.5532625913619995
training step: 38395, total_loss: 3.970090627670288
training step: 38396, total_loss: 2.043154716491699
training step: 38397, total_loss: 0.6440922617912292
training step: 38398, total_loss: 1.1442840099334717
training step: 38399, total_loss: 8.139823913574219
training step: 38400, total_loss: 1.0936620235443115
training step: 38401, total_loss: 1.5967261791229248
training step: 38402, total_loss: 1.0318788290023804
training step: 38403, total_loss: 1.5539380311965942
training step: 38404, total_loss: 1.0891505479812622
training step: 38405, total_loss: 2.5873281955718994
training step: 38406, total_loss: 4.2905144691467285
training step: 38407, total_loss: 4.265917778015137
training step: 38408, total_loss: 2.8230819702148438
training step: 38409, total_loss: 0.8600977659225464
training step: 38410, total_loss: 2.384248733520508
training step: 38411, total_loss: 3.1513328552246094
training step: 38412, total_loss: 2.021977424621582
training step: 38413, total_loss: 1.709752082824707
training step: 38414, total_loss: 3.59574031829834
training step: 38415, total_loss: 1.6848171949386597
training step: 38416, total_loss: 2.491762638092041
training step: 38417, total_loss: 1.336066722869873
training step: 38418, total_loss: 3.3405771255493164
training step: 38419, total_loss: 2.044170379638672
training step: 38420, total_loss: 0.9045050740242004
training step: 38421, total_loss: 3.7974655628204346
training step: 38422, total_loss: 0.9140051603317261
training step: 38423, total_loss: 1.6929569244384766
training step: 38424, total_loss: 2.1056766510009766
training step: 38425, total_loss: 0.5894218683242798
training step: 38426, total_loss: 2.794581413269043
training step: 38427, total_loss: 0.3778645396232605
training step: 38428, total_loss: 2.9231247901916504
training step: 38429, total_loss: 2.945754051208496
training step: 38430, total_loss: 1.7777454853057861
training step: 38431, total_loss: 3.439152240753174
training step: 38432, total_loss: 0.9740188121795654
training step: 38433, total_loss: 0.7351669073104858
training step: 38434, total_loss: 1.6158061027526855
training step: 38435, total_loss: 2.2508177757263184
training step: 38436, total_loss: 3.1077523231506348
training step: 38437, total_loss: 2.4532670974731445
training step: 38438, total_loss: 2.0710463523864746
training step: 38439, total_loss: 3.9674155712127686
training step: 38440, total_loss: 2.005143165588379
training step: 38441, total_loss: 3.7414817810058594
training step: 38442, total_loss: 3.0571417808532715
training step: 38443, total_loss: 2.6788783073425293
training step: 38444, total_loss: 1.454830288887024
training step: 38445, total_loss: 2.5455896854400635
training step: 38446, total_loss: 1.408020257949829
training step: 38447, total_loss: 3.5374507904052734
training step: 38448, total_loss: 2.585658073425293
training step: 38449, total_loss: 3.872819662094116
training step: 38450, total_loss: 2.745150566101074
training step: 38451, total_loss: 0.9295920133590698
training step: 38452, total_loss: 2.598606586456299
training step: 38453, total_loss: 1.536741852760315
training step: 38454, total_loss: 4.70108699798584
training step: 38455, total_loss: 3.31466007232666
training step: 38456, total_loss: 2.6868247985839844
training step: 38457, total_loss: 2.2876853942871094
training step: 38458, total_loss: 0.553213357925415
training step: 38459, total_loss: 1.2372057437896729
training step: 38460, total_loss: 1.1232669353485107
training step: 38461, total_loss: 2.878596782684326
training step: 38462, total_loss: 3.251862049102783
training step: 38463, total_loss: 2.218672752380371
training step: 38464, total_loss: 2.4867115020751953
training step: 38465, total_loss: 2.2502214908599854
training step: 38466, total_loss: 2.2777674198150635
training step: 38467, total_loss: 1.4768822193145752
training step: 38468, total_loss: 1.2786515951156616
training step: 38469, total_loss: 2.5092859268188477
training step: 38470, total_loss: 2.928976535797119
training step: 38471, total_loss: 1.2380881309509277
training step: 38472, total_loss: 2.657823324203491
training step: 38473, total_loss: 2.60831356048584
training step: 38474, total_loss: 1.4347003698349
training step: 38475, total_loss: 0.31754082441329956
training step: 38476, total_loss: 1.7813806533813477
training step: 38477, total_loss: 1.152311086654663
training step: 38478, total_loss: 1.9728574752807617
training step: 38479, total_loss: 1.9624059200286865
training step: 38480, total_loss: 0.4760398268699646
training step: 38481, total_loss: 2.22192120552063
training step: 38482, total_loss: 0.18713855743408203
training step: 38483, total_loss: 1.4852772951126099
training step: 38484, total_loss: 5.344221591949463
training step: 38485, total_loss: 2.118499279022217
training step: 38486, total_loss: 2.7727489471435547
training step: 38487, total_loss: 2.184692859649658
training step: 38488, total_loss: 1.475938320159912
training step: 38489, total_loss: 2.2227208614349365
training step: 38490, total_loss: 2.7116360664367676
training step: 38491, total_loss: 2.049635887145996
training step: 38492, total_loss: 4.861339569091797
training step: 38493, total_loss: 1.4140199422836304
training step: 38494, total_loss: 2.0473670959472656
training step: 38495, total_loss: 2.986565589904785
training step: 38496, total_loss: 2.2932333946228027
training step: 38497, total_loss: 2.204497814178467
training step: 38498, total_loss: 2.338365077972412
training step: 38499, total_loss: 1.8783068656921387
training step: 38500, total_loss: 2.5273642539978027
training step: 38501, total_loss: 1.2290401458740234
training step: 38502, total_loss: 3.060218334197998
training step: 38503, total_loss: 3.1024653911590576
training step: 38504, total_loss: 1.8766162395477295
training step: 38505, total_loss: 4.595513343811035
training step: 38506, total_loss: 1.4329755306243896
training step: 38507, total_loss: 1.53148353099823
training step: 38508, total_loss: 2.4279725551605225
training step: 38509, total_loss: 0.10049480199813843
training step: 38510, total_loss: 1.8714916706085205
training step: 38511, total_loss: 3.473567485809326
training step: 38512, total_loss: 2.4594662189483643
training step: 38513, total_loss: 1.462282419204712
training step: 38514, total_loss: 0.5977630615234375
training step: 38515, total_loss: 3.142796516418457
training step: 38516, total_loss: 2.1848349571228027
training step: 38517, total_loss: 1.0934311151504517
training step: 38518, total_loss: 2.6559901237487793
training step: 38519, total_loss: 2.615936517715454
training step: 38520, total_loss: 0.3949168920516968
training step: 38521, total_loss: 0.8087469339370728
training step: 38522, total_loss: 1.3905234336853027
training step: 38523, total_loss: 2.6636428833007812
training step: 38524, total_loss: 1.747588872909546
training step: 38525, total_loss: 0.5737878680229187
training step: 38526, total_loss: 4.164880275726318
training step: 38527, total_loss: 2.2485475540161133
training step: 38528, total_loss: 0.578046441078186
training step: 38529, total_loss: 1.5749672651290894
training step: 38530, total_loss: 0.5371364951133728
training step: 38531, total_loss: 2.518679618835449
training step: 38532, total_loss: 4.379514694213867
training step: 38533, total_loss: 2.073375701904297
training step: 38534, total_loss: 0.8126913905143738
training step: 38535, total_loss: 1.2301169633865356
training step: 38536, total_loss: 0.14726462960243225
training step: 38537, total_loss: 0.8827494978904724
training step: 38538, total_loss: 0.8182971477508545
training step: 38539, total_loss: 1.1558754444122314
training step: 38540, total_loss: 1.3420751094818115
training step: 38541, total_loss: 0.7310163974761963
training step: 38542, total_loss: 1.4577226638793945
training step: 38543, total_loss: 1.268860101699829
training step: 38544, total_loss: 2.763395309448242
training step: 38545, total_loss: 3.368164539337158
training step: 38546, total_loss: 2.6083836555480957
training step: 38547, total_loss: 1.2343770265579224
training step: 38548, total_loss: 2.502465009689331
training step: 38549, total_loss: 2.5035557746887207
training step: 38550, total_loss: 3.545851945877075
training step: 38551, total_loss: 2.6849331855773926
training step: 38552, total_loss: 2.1846561431884766
training step: 38553, total_loss: 3.9159657955169678
training step: 38554, total_loss: 1.7009437084197998
training step: 38555, total_loss: 3.3977644443511963
training step: 38556, total_loss: 1.9436619281768799
training step: 38557, total_loss: 2.6663901805877686
training step: 38558, total_loss: 0.1558372974395752
training step: 38559, total_loss: 0.6383097171783447
training step: 38560, total_loss: 1.6911687850952148
training step: 38561, total_loss: 0.6135503053665161
training step: 38562, total_loss: 0.16782796382904053
training step: 38563, total_loss: 0.47355061769485474
training step: 38564, total_loss: 1.4309242963790894
training step: 38565, total_loss: 2.8538918495178223
training step: 38566, total_loss: 1.9659242630004883
training step: 38567, total_loss: 0.026598844677209854
training step: 38568, total_loss: 4.318373680114746
training step: 38569, total_loss: 4.277503967285156
training step: 38570, total_loss: 3.726778030395508
training step: 38571, total_loss: 1.4967434406280518
training step: 38572, total_loss: 2.397853374481201
training step: 38573, total_loss: 1.4153823852539062
training step: 38574, total_loss: 2.9550364017486572
training step: 38575, total_loss: 0.20157600939273834
training step: 38576, total_loss: 1.5108336210250854
training step: 38577, total_loss: 2.8409423828125
training step: 38578, total_loss: 1.5708281993865967
training step: 38579, total_loss: 0.9970700740814209
training step: 38580, total_loss: 4.074878215789795
training step: 38581, total_loss: 1.5707125663757324
training step: 38582, total_loss: 2.536452293395996
training step: 38583, total_loss: 5.9782490730285645
training step: 38584, total_loss: 2.1805508136749268
training step: 38585, total_loss: 1.5129952430725098
training step: 38586, total_loss: 5.173900604248047
training step: 38587, total_loss: 0.8173913955688477
training step: 38588, total_loss: 2.1934003829956055
training step: 38589, total_loss: 3.910064220428467
training step: 38590, total_loss: 2.6102347373962402
training step: 38591, total_loss: 4.721371650695801
training step: 38592, total_loss: 3.7387146949768066
training step: 38593, total_loss: 2.9132308959960938
training step: 38594, total_loss: 0.5145660638809204
training step: 38595, total_loss: 1.0469202995300293
training step: 38596, total_loss: 1.2822352647781372
training step: 38597, total_loss: 1.438559651374817
training step: 38598, total_loss: 2.9575564861297607
training step: 38599, total_loss: 2.4418139457702637
training step: 38600, total_loss: 3.2401108741760254
training step: 38601, total_loss: 1.3265094757080078
training step: 38602, total_loss: 0.8759225606918335
training step: 38603, total_loss: 0.4759557247161865
training step: 38604, total_loss: 2.186405658721924
training step: 38605, total_loss: 1.895247220993042
training step: 38606, total_loss: 1.6102769374847412
training step: 38607, total_loss: 1.0951778888702393
training step: 38608, total_loss: 7.122807502746582
training step: 38609, total_loss: 1.651523232460022
training step: 38610, total_loss: 0.7041255235671997
training step: 38611, total_loss: 1.3049750328063965
training step: 38612, total_loss: 3.8216824531555176
training step: 38613, total_loss: 2.359969139099121
training step: 38614, total_loss: 2.2925658226013184
training step: 38615, total_loss: 1.3558933734893799
training step: 38616, total_loss: 2.157052516937256
training step: 38617, total_loss: 0.1400606334209442
training step: 38618, total_loss: 3.633638858795166
training step: 38619, total_loss: 1.582769513130188
training step: 38620, total_loss: 0.46770086884498596
training step: 38621, total_loss: 2.5033516883850098
training step: 38622, total_loss: 2.1112451553344727
training step: 38623, total_loss: 1.8925684690475464
training step: 38624, total_loss: 3.7176356315612793
training step: 38625, total_loss: 3.313645839691162
training step: 38626, total_loss: 0.6419920921325684
training step: 38627, total_loss: 2.02285099029541
training step: 38628, total_loss: 0.618483304977417
training step: 38629, total_loss: 1.614288330078125
training step: 38630, total_loss: 1.6914901733398438
training step: 38631, total_loss: 2.458320140838623
training step: 38632, total_loss: 1.7439794540405273
training step: 38633, total_loss: 2.1477675437927246
training step: 38634, total_loss: 6.232411861419678
training step: 38635, total_loss: 2.7351303100585938
training step: 38636, total_loss: 0.23746684193611145
training step: 38637, total_loss: 0.35866057872772217
training step: 38638, total_loss: 2.5905096530914307
training step: 38639, total_loss: 1.809617042541504
training step: 38640, total_loss: 2.4561376571655273
training step: 38641, total_loss: 3.0464892387390137
training step: 38642, total_loss: 2.280317783355713
training step: 38643, total_loss: 1.388250708580017
training step: 38644, total_loss: 2.4999611377716064
training step: 38645, total_loss: 2.8231711387634277
training step: 38646, total_loss: 1.1629104614257812
training step: 38647, total_loss: 2.9385390281677246
training step: 38648, total_loss: 3.3867592811584473
training step: 38649, total_loss: 1.4777820110321045
training step: 38650, total_loss: 0.6071370840072632
training step: 38651, total_loss: 2.785928249359131
training step: 38652, total_loss: 0.903368353843689
training step: 38653, total_loss: 3.2992334365844727
training step: 38654, total_loss: 3.266489028930664
training step: 38655, total_loss: 0.9376490712165833
training step: 38656, total_loss: 4.449279308319092
training step: 38657, total_loss: 1.110990285873413
training step: 38658, total_loss: 3.073680877685547
training step: 38659, total_loss: 2.7639031410217285
training step: 38660, total_loss: 1.9893444776535034
training step: 38661, total_loss: 2.4548723697662354
training step: 38662, total_loss: 1.9620506763458252
training step: 38663, total_loss: 0.5008978247642517
training step: 38664, total_loss: 3.382826805114746
training step: 38665, total_loss: 1.2495546340942383
training step: 38666, total_loss: 2.4598569869995117
training step: 38667, total_loss: 3.6546640396118164
training step: 38668, total_loss: 1.419832468032837
training step: 38669, total_loss: 0.002677176147699356
training step: 38670, total_loss: 4.126779556274414
training step: 38671, total_loss: 2.263522148132324
training step: 38672, total_loss: 1.306990146636963
training step: 38673, total_loss: 2.6805577278137207
training step: 38674, total_loss: 1.0363152027130127
training step: 38675, total_loss: 1.4131901264190674
training step: 38676, total_loss: 1.7717188596725464
training step: 38677, total_loss: 3.479860305786133
training step: 38678, total_loss: 0.16885045170783997
training step: 38679, total_loss: 2.0978899002075195
training step: 38680, total_loss: 2.023698568344116
training step: 38681, total_loss: 3.232783079147339
training step: 38682, total_loss: 1.3378753662109375
training step: 38683, total_loss: 2.485487222671509
training step: 38684, total_loss: 2.911076307296753
training step: 38685, total_loss: 1.70632803440094
training step: 38686, total_loss: 0.9239187240600586
training step: 38687, total_loss: 0.2371388077735901
training step: 38688, total_loss: 1.0507516860961914
training step: 38689, total_loss: 2.7865819931030273
training step: 38690, total_loss: 2.012380361557007
training step: 38691, total_loss: 1.0594258308410645
training step: 38692, total_loss: 0.07790666818618774
training step: 38693, total_loss: 1.984967589378357
training step: 38694, total_loss: 3.2483019828796387
training step: 38695, total_loss: 1.296283483505249
training step: 38696, total_loss: 3.723811626434326
training step: 38697, total_loss: 1.0934151411056519
training step: 38698, total_loss: 1.9315521717071533
training step: 38699, total_loss: 1.3076900243759155
training step: 38700, total_loss: 0.04728196561336517
training step: 38701, total_loss: 1.4473049640655518
training step: 38702, total_loss: 0.9348791837692261
training step: 38703, total_loss: 6.4970221519470215
training step: 38704, total_loss: 0.9170352220535278
training step: 38705, total_loss: 1.9982578754425049
training step: 38706, total_loss: 3.1878082752227783
training step: 38707, total_loss: 0.703108549118042
training step: 38708, total_loss: 0.5087608695030212
training step: 38709, total_loss: 2.965510368347168
training step: 38710, total_loss: 1.2611689567565918
training step: 38711, total_loss: 0.8173273205757141
training step: 38712, total_loss: 2.236478328704834
training step: 38713, total_loss: 2.03517484664917
training step: 38714, total_loss: 3.9844751358032227
training step: 38715, total_loss: 6.595469951629639
training step: 38716, total_loss: 3.823488712310791
training step: 38717, total_loss: 2.2234320640563965
training step: 38718, total_loss: 0.3882911801338196
training step: 38719, total_loss: 2.6226391792297363
training step: 38720, total_loss: 3.416435718536377
training step: 38721, total_loss: 2.5570967197418213
training step: 38722, total_loss: 0.501788854598999
training step: 38723, total_loss: 0.7498396039009094
training step: 38724, total_loss: 0.45177924633026123
training step: 38725, total_loss: 3.733229398727417
training step: 38726, total_loss: 1.9746372699737549
training step: 38727, total_loss: 2.6612534523010254
training step: 38728, total_loss: 0.47479957342147827
training step: 38729, total_loss: 1.5161528587341309
training step: 38730, total_loss: 2.573435068130493
training step: 38731, total_loss: 3.6679463386535645
training step: 38732, total_loss: 2.2836852073669434
training step: 38733, total_loss: 0.7514413595199585
training step: 38734, total_loss: 0.18795044720172882
training step: 38735, total_loss: 2.7466330528259277
training step: 38736, total_loss: 1.8965582847595215
training step: 38737, total_loss: 2.04868221282959
training step: 38738, total_loss: 1.1206674575805664
training step: 38739, total_loss: 1.4765639305114746
training step: 38740, total_loss: 2.343784809112549
training step: 38741, total_loss: 2.405031681060791
training step: 38742, total_loss: 1.5779571533203125
training step: 38743, total_loss: 2.3148062229156494
training step: 38744, total_loss: 0.3324219882488251
training step: 38745, total_loss: 2.3787553310394287
training step: 38746, total_loss: 1.0963393449783325
training step: 38747, total_loss: 0.7760176062583923
training step: 38748, total_loss: 7.935773849487305
training step: 38749, total_loss: 2.0182340145111084
training step: 38750, total_loss: 2.6785268783569336
training step: 38751, total_loss: 1.2421188354492188
training step: 38752, total_loss: 2.515289545059204
training step: 38753, total_loss: 2.3258450031280518
training step: 38754, total_loss: 0.8317857980728149
training step: 38755, total_loss: 1.5091708898544312
training step: 38756, total_loss: 3.197650909423828
training step: 38757, total_loss: 2.0724518299102783
training step: 38758, total_loss: 1.5185418128967285
training step: 38759, total_loss: 1.7442235946655273
training step: 38760, total_loss: 0.6451891660690308
training step: 38761, total_loss: 0.9143134355545044
training step: 38762, total_loss: 1.5038108825683594
training step: 38763, total_loss: 2.8382368087768555
training step: 38764, total_loss: 1.4683356285095215
training step: 38765, total_loss: 2.810248613357544
training step: 38766, total_loss: 2.56768798828125
training step: 38767, total_loss: 1.1025004386901855
training step: 38768, total_loss: 2.8753490447998047
training step: 38769, total_loss: 2.354830741882324
training step: 38770, total_loss: 2.1986818313598633
training step: 38771, total_loss: 1.0152587890625
training step: 38772, total_loss: 3.8533012866973877
training step: 38773, total_loss: 0.355033278465271
training step: 38774, total_loss: 1.975359320640564
training step: 38775, total_loss: 4.005099296569824
training step: 38776, total_loss: 1.5648727416992188
training step: 38777, total_loss: 2.761838436126709
training step: 38778, total_loss: 1.2614145278930664
training step: 38779, total_loss: 1.9456719160079956
training step: 38780, total_loss: 2.2214090824127197
training step: 38781, total_loss: 1.1954431533813477
training step: 38782, total_loss: 1.404013752937317
training step: 38783, total_loss: 0.026832133531570435
training step: 38784, total_loss: 2.065535545349121
training step: 38785, total_loss: 2.1654486656188965
training step: 38786, total_loss: 1.9016766548156738
training step: 38787, total_loss: 1.592362403869629
training step: 38788, total_loss: 2.7638497352600098
training step: 38789, total_loss: 0.6805442571640015
training step: 38790, total_loss: 1.0061733722686768
training step: 38791, total_loss: 2.945406913757324
training step: 38792, total_loss: 1.706175684928894
training step: 38793, total_loss: 0.1595277637243271
training step: 38794, total_loss: 0.28101110458374023
training step: 38795, total_loss: 1.7314033508300781
training step: 38796, total_loss: 2.8420569896698
training step: 38797, total_loss: 1.4835325479507446
training step: 38798, total_loss: 0.914970874786377
training step: 38799, total_loss: 0.9022098183631897
training step: 38800, total_loss: 0.6556475162506104
training step: 38801, total_loss: 0.4310644865036011
training step: 38802, total_loss: 3.263624668121338
training step: 38803, total_loss: 0.03588327765464783
training step: 38804, total_loss: 2.491206645965576
training step: 38805, total_loss: 0.9550272822380066
training step: 38806, total_loss: 1.0503334999084473
training step: 38807, total_loss: 1.209040880203247
training step: 38808, total_loss: 1.7104917764663696
training step: 38809, total_loss: 1.789158821105957
training step: 38810, total_loss: 0.6769024729728699
training step: 38811, total_loss: 0.047501448541879654
training step: 38812, total_loss: 1.9657244682312012
training step: 38813, total_loss: 0.17913076281547546
training step: 38814, total_loss: 2.8574934005737305
training step: 38815, total_loss: 0.0823332890868187
training step: 38816, total_loss: 0.27117711305618286
training step: 38817, total_loss: 3.6578614711761475
training step: 38818, total_loss: 1.0072795152664185
training step: 38819, total_loss: 0.5361829400062561
training step: 38820, total_loss: 0.042601242661476135
training step: 38821, total_loss: 0.3953035771846771
training step: 38822, total_loss: 2.6565370559692383
training step: 38823, total_loss: 1.9732418060302734
training step: 38824, total_loss: 0.8399021625518799
training step: 38825, total_loss: 3.2230448722839355
training step: 38826, total_loss: 4.179907321929932
training step: 38827, total_loss: 0.5725926756858826
training step: 38828, total_loss: 1.5605396032333374
training step: 38829, total_loss: 3.4908244609832764
training step: 38830, total_loss: 0.02414821833372116
training step: 38831, total_loss: 1.7765686511993408
training step: 38832, total_loss: 2.076502799987793
training step: 38833, total_loss: 5.097694396972656
training step: 38834, total_loss: 1.7547088861465454
training step: 38835, total_loss: 2.000441312789917
training step: 38836, total_loss: 0.6838194131851196
training step: 38837, total_loss: 4.107004165649414
training step: 38838, total_loss: 2.2565412521362305
training step: 38839, total_loss: 1.4915430545806885
training step: 38840, total_loss: 0.382481187582016
training step: 38841, total_loss: 0.849120020866394
training step: 38842, total_loss: 1.2135586738586426
training step: 38843, total_loss: 0.8030599355697632
training step: 38844, total_loss: 3.2921934127807617
training step: 38845, total_loss: 0.0016130844596773386
training step: 38846, total_loss: 0.10544364899396896
training step: 38847, total_loss: 0.9195557236671448
training step: 38848, total_loss: 0.49403804540634155
training step: 38849, total_loss: 2.3256497383117676
training step: 38850, total_loss: 1.9753590822219849
training step: 38851, total_loss: 1.6442766189575195
training step: 38852, total_loss: 1.2932307720184326
training step: 38853, total_loss: 0.3239968717098236
training step: 38854, total_loss: 2.0721888542175293
training step: 38855, total_loss: 1.1605656147003174
training step: 38856, total_loss: 1.4095056056976318
training step: 38857, total_loss: 0.0714663416147232
training step: 38858, total_loss: 1.0418535470962524
training step: 38859, total_loss: 5.018843650817871
training step: 38860, total_loss: 1.0386626720428467
training step: 38861, total_loss: 0.3387770652770996
training step: 38862, total_loss: 1.0470612049102783
training step: 38863, total_loss: 3.070080041885376
training step: 38864, total_loss: 1.676682472229004
training step: 38865, total_loss: 4.812463760375977
training step: 38866, total_loss: 0.7480345964431763
training step: 38867, total_loss: 0.7565428018569946
training step: 38868, total_loss: 0.6330961585044861
training step: 38869, total_loss: 0.0059097567573189735
training step: 38870, total_loss: 0.9151737689971924
training step: 38871, total_loss: 4.650638580322266
training step: 38872, total_loss: 0.4716549515724182
training step: 38873, total_loss: 1.950323462486267
training step: 38874, total_loss: 0.3789936304092407
training step: 38875, total_loss: 1.074183702468872
training step: 38876, total_loss: 3.3712105751037598
training step: 38877, total_loss: 2.774393320083618
training step: 38878, total_loss: 1.0678259134292603
training step: 38879, total_loss: 0.5405075550079346
training step: 38880, total_loss: 1.8962905406951904
training step: 38881, total_loss: 0.32342907786369324
training step: 38882, total_loss: 0.26413607597351074
training step: 38883, total_loss: 0.16279825568199158
training step: 38884, total_loss: 0.22050395607948303
training step: 38885, total_loss: 0.7485324740409851
training step: 38886, total_loss: 0.19185210764408112
training step: 38887, total_loss: 1.1643425226211548
training step: 38888, total_loss: 0.05748120695352554
training step: 38889, total_loss: 0.9230675101280212
training step: 38890, total_loss: 2.3867273330688477
training step: 38891, total_loss: 1.891923427581787
training step: 38892, total_loss: 2.9518380165100098
training step: 38893, total_loss: 0.7460899353027344
training step: 38894, total_loss: 0.21191911399364471
training step: 38895, total_loss: 1.0487443208694458
training step: 38896, total_loss: 0.6741737723350525
training step: 38897, total_loss: 1.0291783809661865
training step: 38898, total_loss: 0.6437633633613586
training step: 38899, total_loss: 0.34279757738113403
training step: 38900, total_loss: 1.181024193763733
training step: 38901, total_loss: 5.748661518096924
training step: 38902, total_loss: 1.292590618133545
training step: 38903, total_loss: 1.6858866214752197
training step: 38904, total_loss: 5.146780490875244
training step: 38905, total_loss: 2.464679718017578
training step: 38906, total_loss: 1.7100708484649658
training step: 38907, total_loss: 4.986775875091553
training step: 38908, total_loss: 1.534679889678955
training step: 38909, total_loss: 1.3566112518310547
training step: 38910, total_loss: 3.0021839141845703
training step: 38911, total_loss: 0.32283636927604675INFO:tensorflow:Writing predictions to: test_output/predictions_39000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_39000.json

training step: 38912, total_loss: 4.2364115715026855
training step: 38913, total_loss: 2.5480189323425293
training step: 38914, total_loss: 1.3524928092956543
training step: 38915, total_loss: 0.7959403991699219
training step: 38916, total_loss: 1.8926122188568115
training step: 38917, total_loss: 1.5796542167663574
training step: 38918, total_loss: 1.7151787281036377
training step: 38919, total_loss: 2.9652154445648193
training step: 38920, total_loss: 1.5364110469818115
training step: 38921, total_loss: 3.5587263107299805
training step: 38922, total_loss: 3.5710294246673584
training step: 38923, total_loss: 1.3629566431045532
training step: 38924, total_loss: 1.8611879348754883
training step: 38925, total_loss: 5.014223098754883
training step: 38926, total_loss: 2.0554516315460205
training step: 38927, total_loss: 1.504495620727539
training step: 38928, total_loss: 1.210533618927002
training step: 38929, total_loss: 0.24482473731040955
training step: 38930, total_loss: 3.1442201137542725
training step: 38931, total_loss: 1.0296748876571655
training step: 38932, total_loss: 2.0182902812957764
training step: 38933, total_loss: 1.3704301118850708
training step: 38934, total_loss: 1.0866039991378784
training step: 38935, total_loss: 0.5640511512756348
training step: 38936, total_loss: 5.270631313323975
training step: 38937, total_loss: 0.1845102608203888
training step: 38938, total_loss: 1.959412932395935
training step: 38939, total_loss: 1.9748430252075195
training step: 38940, total_loss: 3.348906993865967
training step: 38941, total_loss: 1.8185317516326904
training step: 38942, total_loss: 1.183186411857605
training step: 38943, total_loss: 2.201568126678467
training step: 38944, total_loss: 2.517713785171509
training step: 38945, total_loss: 0.8500157594680786
training step: 38946, total_loss: 1.7020138502120972
training step: 38947, total_loss: 1.3023133277893066
training step: 38948, total_loss: 1.8439414501190186
training step: 38949, total_loss: 3.0596020221710205
training step: 38950, total_loss: 2.1793863773345947
training step: 38951, total_loss: 2.189117670059204
training step: 38952, total_loss: 1.2516075372695923
training step: 38953, total_loss: 0.401958703994751
training step: 38954, total_loss: 3.412541389465332
training step: 38955, total_loss: 1.6160426139831543
training step: 38956, total_loss: 0.8362129926681519
training step: 38957, total_loss: 1.5640537738800049
training step: 38958, total_loss: 0.9523401260375977
training step: 38959, total_loss: 0.718420147895813
training step: 38960, total_loss: 1.27565598487854
training step: 38961, total_loss: 2.410104274749756
training step: 38962, total_loss: 1.0170824527740479
training step: 38963, total_loss: 3.109562397003174
training step: 38964, total_loss: 0.7051136493682861
training step: 38965, total_loss: 0.8520717620849609
training step: 38966, total_loss: 1.5414013862609863
training step: 38967, total_loss: 3.1825690269470215
training step: 38968, total_loss: 2.7034966945648193
training step: 38969, total_loss: 0.0015162610216066241
training step: 38970, total_loss: 3.2724695205688477
training step: 38971, total_loss: 1.9986802339553833
training step: 38972, total_loss: 8.273772239685059
training step: 38973, total_loss: 2.521388292312622
training step: 38974, total_loss: 3.7034056186676025
training step: 38975, total_loss: 1.4883537292480469
training step: 38976, total_loss: 1.924295425415039
training step: 38977, total_loss: 2.002493381500244
training step: 38978, total_loss: 1.9412868022918701
training step: 38979, total_loss: 2.6809818744659424
training step: 38980, total_loss: 1.578992486000061
training step: 38981, total_loss: 1.8555834293365479
training step: 38982, total_loss: 1.14176607131958
training step: 38983, total_loss: 0.9798045754432678
training step: 38984, total_loss: 0.994807779788971
training step: 38985, total_loss: 0.5265225172042847
training step: 38986, total_loss: 0.5000443458557129
training step: 38987, total_loss: 0.05878783017396927
training step: 38988, total_loss: 0.4362616539001465
training step: 38989, total_loss: 0.07977188378572464
training step: 38990, total_loss: 2.1575727462768555
training step: 38991, total_loss: 3.195425510406494
training step: 38992, total_loss: 0.15760166943073273
training step: 38993, total_loss: 0.05538734421133995
training step: 38994, total_loss: 4.179990291595459
training step: 38995, total_loss: 1.7739670276641846
training step: 38996, total_loss: 3.7940521240234375
training step: 38997, total_loss: 1.6307830810546875
training step: 38998, total_loss: 3.501671314239502
training step: 38999, total_loss: 0.3469047546386719
training step: 39000, total_loss: 1.3706251382827759
epoch finished! shuffle=False
evaluation: 3000, total_loss: 2.012948989868164, f1: 53.82307887614991, followup: 44.1655256351742, yesno: 74.59303210101932, heq: 48.075460216035296, dheq: 2.8

Model saved in path test_output//model_39000.ckpt
training step: 39001, total_loss: 3.6441659927368164
training step: 39002, total_loss: 1.472943663597107
training step: 39003, total_loss: 1.417968511581421
training step: 39004, total_loss: 4.244409561157227
training step: 39005, total_loss: 0.6427794098854065
training step: 39006, total_loss: 0.00905525777488947
training step: 39007, total_loss: 2.720125198364258
training step: 39008, total_loss: 3.790924549102783
training step: 39009, total_loss: 3.781331777572632
training step: 39010, total_loss: 1.1832023859024048
training step: 39011, total_loss: 1.1055165529251099
training step: 39012, total_loss: 1.2138643264770508
training step: 39013, total_loss: 0.5805559754371643
training step: 39014, total_loss: 2.9818778038024902
training step: 39015, total_loss: 0.32341498136520386
training step: 39016, total_loss: 3.1583690643310547
training step: 39017, total_loss: 2.4073667526245117
training step: 39018, total_loss: 0.7565328478813171
training step: 39019, total_loss: 2.4594526290893555
training step: 39020, total_loss: 2.791700839996338
training step: 39021, total_loss: 4.844588279724121
training step: 39022, total_loss: 3.99002742767334
training step: 39023, total_loss: 0.3137168288230896
training step: 39024, total_loss: 0.8142854571342468
training step: 39025, total_loss: 0.12838439643383026
training step: 39026, total_loss: 2.626697540283203
training step: 39027, total_loss: 2.697035312652588
training step: 39028, total_loss: 1.8187782764434814
training step: 39029, total_loss: 3.5179524421691895
training step: 39030, total_loss: 1.1495333909988403
training step: 39031, total_loss: 4.581543922424316
training step: 39032, total_loss: 0.8866935968399048
training step: 39033, total_loss: 1.084371566772461
training step: 39034, total_loss: 1.7121402025222778
training step: 39035, total_loss: 1.7452051639556885
training step: 39036, total_loss: 1.3047175407409668
training step: 39037, total_loss: 0.17259149253368378
training step: 39038, total_loss: 2.1070942878723145
training step: 39039, total_loss: 2.226290225982666
training step: 39040, total_loss: 1.3163719177246094
training step: 39041, total_loss: 2.332103729248047
training step: 39042, total_loss: 2.815826654434204
training step: 39043, total_loss: 1.8435900211334229
training step: 39044, total_loss: 0.8620620369911194
training step: 39045, total_loss: 2.1848297119140625
training step: 39046, total_loss: 1.6314629316329956
training step: 39047, total_loss: 2.0871224403381348
training step: 39048, total_loss: 1.8809707164764404
training step: 39049, total_loss: 2.5908327102661133
training step: 39050, total_loss: 3.286653518676758
training step: 39051, total_loss: 0.755296528339386
training step: 39052, total_loss: 0.44711780548095703
training step: 39053, total_loss: 2.9369776248931885
training step: 39054, total_loss: 0.27782100439071655
training step: 39055, total_loss: 2.5526037216186523
training step: 39056, total_loss: 1.433335781097412
training step: 39057, total_loss: 1.6475300788879395
training step: 39058, total_loss: 2.095592498779297
training step: 39059, total_loss: 2.21162486076355
training step: 39060, total_loss: 1.348742961883545
training step: 39061, total_loss: 2.2017159461975098
training step: 39062, total_loss: 2.92751407623291
training step: 39063, total_loss: 3.6954140663146973
training step: 39064, total_loss: 0.6970906257629395
training step: 39065, total_loss: 1.6506904363632202
training step: 39066, total_loss: 2.510133743286133
training step: 39067, total_loss: 0.701391339302063
training step: 39068, total_loss: 0.47042202949523926
training step: 39069, total_loss: 0.7484338283538818
training step: 39070, total_loss: 3.257560968399048
training step: 39071, total_loss: 1.8904922008514404
training step: 39072, total_loss: 1.6283857822418213
training step: 39073, total_loss: 2.5195631980895996
training step: 39074, total_loss: 0.09331414103507996
training step: 39075, total_loss: 1.9703168869018555
training step: 39076, total_loss: 1.0558369159698486
training step: 39077, total_loss: 3.3419604301452637
training step: 39078, total_loss: 3.046597957611084
training step: 39079, total_loss: 0.05770842358469963
training step: 39080, total_loss: 2.4021189212799072
training step: 39081, total_loss: 1.9794270992279053
training step: 39082, total_loss: 0.024887649342417717
training step: 39083, total_loss: 0.0035589057952165604
training step: 39084, total_loss: 1.9403306245803833
training step: 39085, total_loss: 0.14667366445064545
training step: 39086, total_loss: 0.5640887022018433
training step: 39087, total_loss: 1.1420410871505737
training step: 39088, total_loss: 3.5731687545776367
training step: 39089, total_loss: 2.40622615814209
training step: 39090, total_loss: 2.487093687057495
training step: 39091, total_loss: 2.9126110076904297
training step: 39092, total_loss: 1.0335006713867188
training step: 39093, total_loss: 2.2623655796051025
training step: 39094, total_loss: 0.39275509119033813
training step: 39095, total_loss: 2.939591646194458
training step: 39096, total_loss: 0.6499375700950623
training step: 39097, total_loss: 4.212856292724609
training step: 39098, total_loss: 0.01913318783044815
training step: 39099, total_loss: 0.15953750908374786
training step: 39100, total_loss: 0.4570501446723938
training step: 39101, total_loss: 1.7991933822631836
training step: 39102, total_loss: 0.9317169785499573
training step: 39103, total_loss: 1.353523850440979
training step: 39104, total_loss: 1.246927261352539
training step: 39105, total_loss: 1.8676276206970215
training step: 39106, total_loss: 1.2931160926818848
training step: 39107, total_loss: 1.3068548440933228
training step: 39108, total_loss: 0.16580411791801453
training step: 39109, total_loss: 2.0116589069366455
training step: 39110, total_loss: 4.073772430419922
training step: 39111, total_loss: 0.7198556661605835
training step: 39112, total_loss: 1.4598939418792725
training step: 39113, total_loss: 1.5774896144866943
training step: 39114, total_loss: 1.2617729902267456
training step: 39115, total_loss: 0.2666589021682739
training step: 39116, total_loss: 1.3997236490249634
training step: 39117, total_loss: 2.42012357711792
training step: 39118, total_loss: 2.3696770668029785
training step: 39119, total_loss: 2.3452930450439453
training step: 39120, total_loss: 2.173776149749756
training step: 39121, total_loss: 2.2597274780273438
training step: 39122, total_loss: 3.093151807785034
training step: 39123, total_loss: 0.5059446692466736
training step: 39124, total_loss: 1.255789041519165
training step: 39125, total_loss: 1.2690565586090088
training step: 39126, total_loss: 0.6883541345596313
training step: 39127, total_loss: 1.240086317062378
training step: 39128, total_loss: 2.7132787704467773
training step: 39129, total_loss: 3.0965933799743652
training step: 39130, total_loss: 0.018583128228783607
training step: 39131, total_loss: 1.8261107206344604
training step: 39132, total_loss: 2.4442460536956787
training step: 39133, total_loss: 0.09553422033786774
training step: 39134, total_loss: 5.050468444824219
training step: 39135, total_loss: 1.2777388095855713
training step: 39136, total_loss: 2.366778612136841
training step: 39137, total_loss: 3.3194336891174316
training step: 39138, total_loss: 3.067509889602661
training step: 39139, total_loss: 2.321932077407837
training step: 39140, total_loss: 2.498398780822754
training step: 39141, total_loss: 1.0075206756591797
training step: 39142, total_loss: 0.7001744508743286
training step: 39143, total_loss: 0.6884893774986267
training step: 39144, total_loss: 1.445591688156128
training step: 39145, total_loss: 2.888434648513794
training step: 39146, total_loss: 1.7742949724197388
training step: 39147, total_loss: 3.1841206550598145
training step: 39148, total_loss: 3.1905572414398193
training step: 39149, total_loss: 1.2486636638641357
training step: 39150, total_loss: 2.410245418548584
training step: 39151, total_loss: 1.9530107975006104
training step: 39152, total_loss: 3.8329319953918457
training step: 39153, total_loss: 2.8251543045043945
training step: 39154, total_loss: 2.368727684020996
training step: 39155, total_loss: 1.2086268663406372
training step: 39156, total_loss: 5.380521297454834
training step: 39157, total_loss: 1.3915834426879883
training step: 39158, total_loss: 3.6223034858703613
training step: 39159, total_loss: 0.00418675784021616
training step: 39160, total_loss: 1.5116151571273804
training step: 39161, total_loss: 0.3247593641281128
training step: 39162, total_loss: 0.07182695716619492
training step: 39163, total_loss: 4.794351577758789
training step: 39164, total_loss: 0.0030674845911562443
training step: 39165, total_loss: 0.7734361886978149
training step: 39166, total_loss: 2.949462413787842
training step: 39167, total_loss: 0.2919072210788727
training step: 39168, total_loss: 1.432717204093933
training step: 39169, total_loss: 2.485452651977539
training step: 39170, total_loss: 0.7739184498786926
training step: 39171, total_loss: 1.1309778690338135
training step: 39172, total_loss: 0.7439043521881104
training step: 39173, total_loss: 3.285572052001953
training step: 39174, total_loss: 3.3641157150268555
training step: 39175, total_loss: 1.9722737073898315
training step: 39176, total_loss: 1.6553013324737549
training step: 39177, total_loss: 2.618924140930176
training step: 39178, total_loss: 3.1386942863464355
training step: 39179, total_loss: 0.8544915914535522
training step: 39180, total_loss: 1.3934752941131592
training step: 39181, total_loss: 1.2081834077835083
training step: 39182, total_loss: 0.09060565382242203
training step: 39183, total_loss: 2.5442280769348145
training step: 39184, total_loss: 2.473971128463745
training step: 39185, total_loss: 0.14067383110523224
training step: 39186, total_loss: 0.48774001002311707
training step: 39187, total_loss: 1.538114309310913
training step: 39188, total_loss: 1.1246039867401123
training step: 39189, total_loss: 1.237392783164978
training step: 39190, total_loss: 0.9169082045555115
training step: 39191, total_loss: 1.940850853919983
training step: 39192, total_loss: 0.17565664649009705
training step: 39193, total_loss: 3.6454577445983887
training step: 39194, total_loss: 2.107473850250244
training step: 39195, total_loss: 0.3493572771549225
training step: 39196, total_loss: 4.2665114402771
training step: 39197, total_loss: 0.5784793496131897
training step: 39198, total_loss: 3.3348147869110107
training step: 39199, total_loss: 0.5597566366195679
training step: 39200, total_loss: 0.7533949017524719
training step: 39201, total_loss: 2.491339921951294
training step: 39202, total_loss: 2.586637496948242
training step: 39203, total_loss: 0.7611250877380371
training step: 39204, total_loss: 1.3807798624038696
training step: 39205, total_loss: 2.3138670921325684
training step: 39206, total_loss: 2.4231858253479004
training step: 39207, total_loss: 2.991219997406006
training step: 39208, total_loss: 2.8079781532287598
training step: 39209, total_loss: 3.2686567306518555
training step: 39210, total_loss: 1.9893639087677002
training step: 39211, total_loss: 2.8350460529327393
training step: 39212, total_loss: 0.32378849387168884
training step: 39213, total_loss: 1.1501661539077759
training step: 39214, total_loss: 0.80866539478302
training step: 39215, total_loss: 2.597720146179199
training step: 39216, total_loss: 0.34345701336860657
training step: 39217, total_loss: 3.4197802543640137
training step: 39218, total_loss: 1.74998140335083
training step: 39219, total_loss: 0.4219130277633667
training step: 39220, total_loss: 0.3676941394805908
training step: 39221, total_loss: 2.163926601409912
training step: 39222, total_loss: 4.9700422286987305
training step: 39223, total_loss: 0.263182669878006
training step: 39224, total_loss: 1.8205935955047607
training step: 39225, total_loss: 1.490234136581421
training step: 39226, total_loss: 0.2638007700443268
training step: 39227, total_loss: 0.025729693472385406
training step: 39228, total_loss: 0.021639350801706314
training step: 39229, total_loss: 1.3583670854568481
training step: 39230, total_loss: 0.7353435158729553
training step: 39231, total_loss: 0.18710139393806458
training step: 39232, total_loss: 0.500628650188446
training step: 39233, total_loss: 3.3038768768310547
training step: 39234, total_loss: 1.3612083196640015
training step: 39235, total_loss: 1.9297966957092285
training step: 39236, total_loss: 0.6927082538604736
training step: 39237, total_loss: 1.0307862758636475
training step: 39238, total_loss: 1.5632928609848022
training step: 39239, total_loss: 3.1634936332702637
training step: 39240, total_loss: 1.247664213180542
training step: 39241, total_loss: 2.5946807861328125
training step: 39242, total_loss: 0.4184297025203705
training step: 39243, total_loss: 1.922580361366272
training step: 39244, total_loss: 1.0843696594238281
training step: 39245, total_loss: 1.9141080379486084
training step: 39246, total_loss: 2.1195907592773438
training step: 39247, total_loss: 1.1297800540924072
training step: 39248, total_loss: 3.8191022872924805
training step: 39249, total_loss: 1.0722581148147583
training step: 39250, total_loss: 1.4332257509231567
training step: 39251, total_loss: 0.7092224955558777
training step: 39252, total_loss: 1.3664942979812622
training step: 39253, total_loss: 0.01277158223092556
training step: 39254, total_loss: 3.7679381370544434
training step: 39255, total_loss: 0.044855497777462006
training step: 39256, total_loss: 3.7058563232421875
training step: 39257, total_loss: 0.3527446687221527
training step: 39258, total_loss: 2.931328296661377
training step: 39259, total_loss: 3.956325054168701
training step: 39260, total_loss: 5.0455474853515625
training step: 39261, total_loss: 0.6960265636444092
training step: 39262, total_loss: 2.8104915618896484
training step: 39263, total_loss: 2.532761573791504
training step: 39264, total_loss: 4.364891052246094
training step: 39265, total_loss: 2.2806735038757324
training step: 39266, total_loss: 3.8972599506378174
training step: 39267, total_loss: 2.9543440341949463
training step: 39268, total_loss: 0.8658018708229065
training step: 39269, total_loss: 0.4938231110572815
training step: 39270, total_loss: 1.8083499670028687
training step: 39271, total_loss: 2.768310070037842
training step: 39272, total_loss: 2.080871105194092
training step: 39273, total_loss: 2.243584632873535
training step: 39274, total_loss: 0.8797599673271179
training step: 39275, total_loss: 1.9401507377624512
training step: 39276, total_loss: 2.4257161617279053
training step: 39277, total_loss: 0.28428271412849426
training step: 39278, total_loss: 1.5061240196228027
training step: 39279, total_loss: 3.528592586517334
training step: 39280, total_loss: 2.8112246990203857
training step: 39281, total_loss: 3.5386552810668945
training step: 39282, total_loss: 1.4531861543655396
training step: 39283, total_loss: 3.394078254699707
training step: 39284, total_loss: 1.0491160154342651
training step: 39285, total_loss: 2.8880391120910645
training step: 39286, total_loss: 1.7610626220703125
training step: 39287, total_loss: 3.757087230682373
training step: 39288, total_loss: 0.003781881183385849
training step: 39289, total_loss: 2.0217764377593994
training step: 39290, total_loss: 0.7082763910293579
training step: 39291, total_loss: 3.4325108528137207
training step: 39292, total_loss: 0.6113094687461853
training step: 39293, total_loss: 1.909625768661499
training step: 39294, total_loss: 1.0270304679870605
training step: 39295, total_loss: 1.1084856986999512
training step: 39296, total_loss: 2.4444046020507812
training step: 39297, total_loss: 1.7913246154785156
training step: 39298, total_loss: 1.4996349811553955
training step: 39299, total_loss: 0.7278389930725098
training step: 39300, total_loss: 2.1312272548675537
training step: 39301, total_loss: 0.10930316150188446
training step: 39302, total_loss: 0.000796894368249923
training step: 39303, total_loss: 2.460954189300537
training step: 39304, total_loss: 1.0968446731567383
training step: 39305, total_loss: 0.06191592290997505
training step: 39306, total_loss: 2.0275321006774902
training step: 39307, total_loss: 1.0149174928665161
training step: 39308, total_loss: 1.5407636165618896
training step: 39309, total_loss: 0.8591476082801819
training step: 39310, total_loss: 3.7161788940429688
training step: 39311, total_loss: 0.9058146476745605
training step: 39312, total_loss: 0.6593245267868042
training step: 39313, total_loss: 1.4149131774902344
training step: 39314, total_loss: 0.039802469313144684
training step: 39315, total_loss: 1.6909596920013428
training step: 39316, total_loss: 0.6685628890991211
training step: 39317, total_loss: 3.7359495162963867
training step: 39318, total_loss: 1.3715271949768066
training step: 39319, total_loss: 1.18410062789917
training step: 39320, total_loss: 1.8310461044311523
training step: 39321, total_loss: 1.8418776988983154
training step: 39322, total_loss: 2.498098611831665
training step: 39323, total_loss: 2.483027219772339
training step: 39324, total_loss: 2.3680477142333984
training step: 39325, total_loss: 0.31276559829711914
training step: 39326, total_loss: 1.9980676174163818
training step: 39327, total_loss: 1.5338952541351318
training step: 39328, total_loss: 0.5615699887275696
training step: 39329, total_loss: 1.506651759147644
training step: 39330, total_loss: 2.52194881439209
training step: 39331, total_loss: 2.117798328399658
training step: 39332, total_loss: 2.359788179397583
training step: 39333, total_loss: 0.8453295230865479
training step: 39334, total_loss: 1.5553210973739624
training step: 39335, total_loss: 0.24229107797145844
training step: 39336, total_loss: 1.2356681823730469
training step: 39337, total_loss: 0.36706870794296265
training step: 39338, total_loss: 3.366100788116455
training step: 39339, total_loss: 3.40665340423584
training step: 39340, total_loss: 2.202620029449463
training step: 39341, total_loss: 1.809512734413147
training step: 39342, total_loss: 3.324979543685913
training step: 39343, total_loss: 2.2916531562805176
training step: 39344, total_loss: 1.7820649147033691
training step: 39345, total_loss: 0.3819635212421417
training step: 39346, total_loss: 0.14120535552501678
training step: 39347, total_loss: 0.1610632836818695
training step: 39348, total_loss: 1.1114778518676758
training step: 39349, total_loss: 2.4684107303619385
training step: 39350, total_loss: 2.010380506515503
training step: 39351, total_loss: 2.3090193271636963
training step: 39352, total_loss: 1.0047473907470703
training step: 39353, total_loss: 0.004472868517041206
training step: 39354, total_loss: 0.9809313416481018
training step: 39355, total_loss: 0.9139854907989502
training step: 39356, total_loss: 0.05018724128603935
training step: 39357, total_loss: 5.151486396789551
training step: 39358, total_loss: 2.8903260231018066
training step: 39359, total_loss: 0.6805012822151184
training step: 39360, total_loss: 1.580682635307312
training step: 39361, total_loss: 1.961061716079712
training step: 39362, total_loss: 3.5350632667541504
training step: 39363, total_loss: 1.6577616930007935
training step: 39364, total_loss: 2.7235219478607178
training step: 39365, total_loss: 3.0979042053222656
training step: 39366, total_loss: 1.6349701881408691
training step: 39367, total_loss: 0.9732793569564819
training step: 39368, total_loss: 3.5655736923217773
training step: 39369, total_loss: 3.2900631427764893
training step: 39370, total_loss: 1.7420768737792969
training step: 39371, total_loss: 2.9791107177734375
training step: 39372, total_loss: 4.019855499267578
training step: 39373, total_loss: 0.889877438545227
training step: 39374, total_loss: 0.72530198097229
training step: 39375, total_loss: 0.30135902762413025
training step: 39376, total_loss: 0.6358847618103027
training step: 39377, total_loss: 2.076674222946167
training step: 39378, total_loss: 0.793851375579834
training step: 39379, total_loss: 0.4171895980834961
training step: 39380, total_loss: 0.962004542350769
training step: 39381, total_loss: 1.532916784286499
training step: 39382, total_loss: 0.5521215200424194
training step: 39383, total_loss: 2.1848597526550293
training step: 39384, total_loss: 0.010380303487181664
training step: 39385, total_loss: 4.323781967163086
training step: 39386, total_loss: 3.1550564765930176
training step: 39387, total_loss: 0.9333314895629883
training step: 39388, total_loss: 0.02352672442793846
training step: 39389, total_loss: 0.00638021994382143
training step: 39390, total_loss: 1.2815825939178467
training step: 39391, total_loss: 4.7386064529418945
training step: 39392, total_loss: 0.005018538795411587
training step: 39393, total_loss: 0.8689244389533997
training step: 39394, total_loss: 1.6727690696716309
training step: 39395, total_loss: 0.047885604202747345
training step: 39396, total_loss: 0.8201773166656494
training step: 39397, total_loss: 2.9026336669921875
training step: 39398, total_loss: 0.7948063015937805
training step: 39399, total_loss: 2.394174814224243
training step: 39400, total_loss: 2.2873146533966064
training step: 39401, total_loss: 2.2625234127044678
training step: 39402, total_loss: 3.226253032684326
training step: 39403, total_loss: 0.002793555613607168
training step: 39404, total_loss: 0.8337295651435852
training step: 39405, total_loss: 0.00454516988247633
training step: 39406, total_loss: 1.960202932357788
training step: 39407, total_loss: 3.142312526702881
training step: 39408, total_loss: 1.0074162483215332
training step: 39409, total_loss: 0.18028266727924347
training step: 39410, total_loss: 0.5387866497039795
training step: 39411, total_loss: 1.4150340557098389
training step: 39412, total_loss: 4.330552101135254
training step: 39413, total_loss: 3.520825147628784
training step: 39414, total_loss: 1.9456098079681396
training step: 39415, total_loss: 1.9996519088745117
training step: 39416, total_loss: 5.305171012878418
training step: 39417, total_loss: 3.156374931335449
training step: 39418, total_loss: 1.4436330795288086
training step: 39419, total_loss: 0.45204901695251465
training step: 39420, total_loss: 1.014136791229248
training step: 39421, total_loss: 3.1713967323303223
training step: 39422, total_loss: 4.344483375549316
training step: 39423, total_loss: 0.31025430560112
training step: 39424, total_loss: 3.4733519554138184
training step: 39425, total_loss: 2.025134563446045
training step: 39426, total_loss: 1.6403539180755615
training step: 39427, total_loss: 1.0936075448989868
training step: 39428, total_loss: 1.9091908931732178
training step: 39429, total_loss: 1.131591558456421
training step: 39430, total_loss: 0.31691503524780273
training step: 39431, total_loss: 3.7482950687408447
training step: 39432, total_loss: 1.315073013305664
training step: 39433, total_loss: 0.5271016955375671
training step: 39434, total_loss: 2.045658588409424
training step: 39435, total_loss: 1.9159579277038574
training step: 39436, total_loss: 2.6913390159606934
training step: 39437, total_loss: 0.7770791053771973
training step: 39438, total_loss: 3.420562744140625
training step: 39439, total_loss: 1.2195192575454712
training step: 39440, total_loss: 1.013939380645752
training step: 39441, total_loss: 1.1341614723205566
training step: 39442, total_loss: 0.830895721912384
training step: 39443, total_loss: 2.111074209213257
training step: 39444, total_loss: 0.20849642157554626
training step: 39445, total_loss: 1.6247069835662842
training step: 39446, total_loss: 0.3370093107223511
training step: 39447, total_loss: 3.365950107574463
training step: 39448, total_loss: 1.5305685997009277
training step: 39449, total_loss: 6.881757736206055
training step: 39450, total_loss: 1.2954298257827759
training step: 39451, total_loss: 0.07348529994487762
training step: 39452, total_loss: 1.1560392379760742
training step: 39453, total_loss: 0.3795146942138672
training step: 39454, total_loss: 1.1259160041809082
training step: 39455, total_loss: 0.9724483489990234
training step: 39456, total_loss: 0.3949708938598633
training step: 39457, total_loss: 0.42279309034347534
training step: 39458, total_loss: 2.2774877548217773
training step: 39459, total_loss: 0.3712869882583618
training step: 39460, total_loss: 2.2122535705566406
training step: 39461, total_loss: 1.9372961521148682
training step: 39462, total_loss: 1.9464985132217407
training step: 39463, total_loss: 4.585203170776367
training step: 39464, total_loss: 2.259377956390381
training step: 39465, total_loss: 4.338569641113281
training step: 39466, total_loss: 1.607688069343567
training step: 39467, total_loss: 1.0749237537384033
training step: 39468, total_loss: 0.5225809812545776
training step: 39469, total_loss: 1.5775210857391357
training step: 39470, total_loss: 2.163099765777588
training step: 39471, total_loss: 2.920196056365967
training step: 39472, total_loss: 2.12150239944458
training step: 39473, total_loss: 1.067305326461792
training step: 39474, total_loss: 1.8175251483917236
training step: 39475, total_loss: 1.2353811264038086
training step: 39476, total_loss: 0.14072221517562866
training step: 39477, total_loss: 4.3922529220581055
training step: 39478, total_loss: 1.8935153484344482
training step: 39479, total_loss: 0.9931984543800354
training step: 39480, total_loss: 3.5986528396606445
training step: 39481, total_loss: 3.592651128768921
training step: 39482, total_loss: 1.6048840284347534
training step: 39483, total_loss: 0.8328771591186523
training step: 39484, total_loss: 4.4097394943237305
training step: 39485, total_loss: 1.3135275840759277
training step: 39486, total_loss: 1.0150563716888428
training step: 39487, total_loss: 2.497215509414673
training step: 39488, total_loss: 2.0633649826049805
training step: 39489, total_loss: 2.0871057510375977
training step: 39490, total_loss: 0.5383929014205933
training step: 39491, total_loss: 0.6750681400299072
training step: 39492, total_loss: 0.9283835887908936
training step: 39493, total_loss: 3.479907512664795
training step: 39494, total_loss: 2.321964740753174
training step: 39495, total_loss: 0.729111909866333
training step: 39496, total_loss: 1.4046915769577026
training step: 39497, total_loss: 1.4979149103164673
training step: 39498, total_loss: 2.486027717590332
training step: 39499, total_loss: 2.257145881652832
training step: 39500, total_loss: 1.2415883541107178
training step: 39501, total_loss: 2.631155014038086
training step: 39502, total_loss: 0.2039881944656372
training step: 39503, total_loss: 3.33113956451416
training step: 39504, total_loss: 0.06879999488592148
training step: 39505, total_loss: 1.2820158004760742
training step: 39506, total_loss: 2.596470594406128
training step: 39507, total_loss: 3.144320487976074
training step: 39508, total_loss: 2.705836057662964
training step: 39509, total_loss: 1.0731903314590454
training step: 39510, total_loss: 0.33315348625183105
training step: 39511, total_loss: 3.3864974975585938
training step: 39512, total_loss: 3.1811299324035645
training step: 39513, total_loss: 1.1869425773620605
training step: 39514, total_loss: 0.17958039045333862
training step: 39515, total_loss: 0.48701703548431396
training step: 39516, total_loss: 1.3303205966949463
training step: 39517, total_loss: 0.7959014177322388
training step: 39518, total_loss: 0.8168042898178101
training step: 39519, total_loss: 1.3891783952713013
training step: 39520, total_loss: 0.45465517044067383
training step: 39521, total_loss: 2.197728157043457
training step: 39522, total_loss: 0.02460256777703762
training step: 39523, total_loss: 2.706071376800537
training step: 39524, total_loss: 1.1546343564987183
training step: 39525, total_loss: 4.406519412994385
training step: 39526, total_loss: 6.183558464050293
training step: 39527, total_loss: 2.3776795864105225
training step: 39528, total_loss: 0.657676100730896
training step: 39529, total_loss: 0.06411439180374146
training step: 39530, total_loss: 2.901334285736084
training step: 39531, total_loss: 0.884463906288147
training step: 39532, total_loss: 3.097104072570801
training step: 39533, total_loss: 2.767796039581299
training step: 39534, total_loss: 5.760608673095703
training step: 39535, total_loss: 1.784966230392456
training step: 39536, total_loss: 2.6610636711120605
training step: 39537, total_loss: 1.9227375984191895
training step: 39538, total_loss: 5.368508815765381
training step: 39539, total_loss: 4.017643928527832
training step: 39540, total_loss: 1.6830198764801025
training step: 39541, total_loss: 3.087075710296631
training step: 39542, total_loss: 0.32471132278442383
training step: 39543, total_loss: 0.46188920736312866
training step: 39544, total_loss: 0.3676617741584778
training step: 39545, total_loss: 0.035818759351968765
training step: 39546, total_loss: 1.0969558954238892
training step: 39547, total_loss: 1.9301514625549316
training step: 39548, total_loss: 2.996262550354004
training step: 39549, total_loss: 0.03768082708120346
training step: 39550, total_loss: 3.0637691020965576
training step: 39551, total_loss: 0.3480175733566284
training step: 39552, total_loss: 1.896937370300293
training step: 39553, total_loss: 1.7132229804992676
training step: 39554, total_loss: 1.558144450187683
training step: 39555, total_loss: 0.007102093193680048
training step: 39556, total_loss: 0.5840455889701843
training step: 39557, total_loss: 0.9232574701309204
training step: 39558, total_loss: 0.0024226680397987366
training step: 39559, total_loss: 0.15851345658302307
training step: 39560, total_loss: 1.1499501466751099
training step: 39561, total_loss: 0.19631943106651306
training step: 39562, total_loss: 1.1813180446624756
training step: 39563, total_loss: 0.6495919227600098
training step: 39564, total_loss: 2.100757122039795
training step: 39565, total_loss: 2.358205556869507
training step: 39566, total_loss: 2.459625720977783
training step: 39567, total_loss: 0.07620136439800262
training step: 39568, total_loss: 1.6216495037078857
training step: 39569, total_loss: 4.1045122146606445
training step: 39570, total_loss: 0.05370490998029709
training step: 39571, total_loss: 6.626838684082031
training step: 39572, total_loss: 0.26481693983078003
training step: 39573, total_loss: 1.9497809410095215
training step: 39574, total_loss: 1.3053473234176636
training step: 39575, total_loss: 2.288114547729492
training step: 39576, total_loss: 1.6611242294311523
training step: 39577, total_loss: 5.2536940574646
training step: 39578, total_loss: 2.204031467437744
training step: 39579, total_loss: 2.710118055343628
training step: 39580, total_loss: 1.614209771156311
training step: 39581, total_loss: 2.774714708328247
training step: 39582, total_loss: 1.5454859733581543
training step: 39583, total_loss: 2.491168737411499
training step: 39584, total_loss: 3.470914363861084
training step: 39585, total_loss: 2.5970206260681152
training step: 39586, total_loss: 0.15363547205924988
training step: 39587, total_loss: 2.8713274002075195
training step: 39588, total_loss: 1.856576919555664
training step: 39589, total_loss: 0.36936861276626587
training step: 39590, total_loss: 0.9665840268135071
training step: 39591, total_loss: 0.06697705388069153
training step: 39592, total_loss: 3.080270767211914
training step: 39593, total_loss: 1.3384464979171753
training step: 39594, total_loss: 2.553342819213867
training step: 39595, total_loss: 3.3673512935638428
training step: 39596, total_loss: 1.7500520944595337
training step: 39597, total_loss: 1.3608497381210327
training step: 39598, total_loss: 2.5173521041870117
training step: 39599, total_loss: 1.4333981275558472
training step: 39600, total_loss: 1.2059277296066284
training step: 39601, total_loss: 0.8295209407806396
training step: 39602, total_loss: 0.5786511301994324
training step: 39603, total_loss: 2.8495681285858154
training step: 39604, total_loss: 2.1875319480895996
training step: 39605, total_loss: 1.930382251739502
training step: 39606, total_loss: 1.5763146877288818
training step: 39607, total_loss: 0.2213241457939148
training step: 39608, total_loss: 1.589085340499878
training step: 39609, total_loss: 1.8832730054855347
training step: 39610, total_loss: 3.8726067543029785
training step: 39611, total_loss: 0.1579129546880722
training step: 39612, total_loss: 1.3636255264282227
training step: 39613, total_loss: 1.227097988128662
training step: 39614, total_loss: 2.050995111465454
training step: 39615, total_loss: 0.6377065777778625
training step: 39616, total_loss: 2.3071374893188477
training step: 39617, total_loss: 1.1796272993087769
training step: 39618, total_loss: 1.803765058517456
training step: 39619, total_loss: 2.7668707370758057
training step: 39620, total_loss: 2.4206619262695312
training step: 39621, total_loss: 0.007919862866401672
training step: 39622, total_loss: 1.5318307876586914
training step: 39623, total_loss: 0.05539395660161972
training step: 39624, total_loss: 0.5271396636962891
training step: 39625, total_loss: 2.0414175987243652
training step: 39626, total_loss: 0.41955968737602234
training step: 39627, total_loss: 1.0222718715667725
training step: 39628, total_loss: 1.4142963886260986
training step: 39629, total_loss: 1.640638828277588
training step: 39630, total_loss: 1.2361780405044556
training step: 39631, total_loss: 2.402050495147705
training step: 39632, total_loss: 0.17528881132602692
training step: 39633, total_loss: 4.137024879455566
training step: 39634, total_loss: 0.5545434951782227
training step: 39635, total_loss: 2.5082740783691406
training step: 39636, total_loss: 1.7232708930969238
training step: 39637, total_loss: 2.5436768531799316
training step: 39638, total_loss: 2.3865966796875
training step: 39639, total_loss: 4.480710029602051
training step: 39640, total_loss: 2.9189772605895996
training step: 39641, total_loss: 0.9800914525985718
training step: 39642, total_loss: 2.472513437271118
training step: 39643, total_loss: 0.8401529788970947
training step: 39644, total_loss: 1.188111662864685
training step: 39645, total_loss: 0.003339203307405114
training step: 39646, total_loss: 0.9560444951057434
training step: 39647, total_loss: 1.4610536098480225
training step: 39648, total_loss: 2.144078254699707
training step: 39649, total_loss: 0.8797293305397034
training step: 39650, total_loss: 1.0082151889801025
training step: 39651, total_loss: 3.767228603363037
training step: 39652, total_loss: 1.5781595706939697
training step: 39653, total_loss: 0.09646595269441605
training step: 39654, total_loss: 1.3601572513580322
training step: 39655, total_loss: 0.9171067476272583
training step: 39656, total_loss: 0.45878568291664124
training step: 39657, total_loss: 0.9335036277770996
training step: 39658, total_loss: 0.4508255422115326
training step: 39659, total_loss: 0.9339003562927246
training step: 39660, total_loss: 2.0486364364624023
training step: 39661, total_loss: 0.8027046918869019
training step: 39662, total_loss: 0.0009697660570964217
training step: 39663, total_loss: 1.7733087539672852
training step: 39664, total_loss: 0.9408634901046753
training step: 39665, total_loss: 0.11162470281124115
training step: 39666, total_loss: 1.7786245346069336
training step: 39667, total_loss: 0.013965444639325142
training step: 39668, total_loss: 0.4092271625995636
training step: 39669, total_loss: 2.7769289016723633
training step: 39670, total_loss: 2.894069194793701
training step: 39671, total_loss: 1.0902812480926514
training step: 39672, total_loss: 1.1685513257980347
training step: 39673, total_loss: 1.832489252090454
training step: 39674, total_loss: 5.075018882751465
training step: 39675, total_loss: 2.7187180519104004
training step: 39676, total_loss: 0.008679280988872051
training step: 39677, total_loss: 1.6574114561080933
training step: 39678, total_loss: 2.3988046646118164
training step: 39679, total_loss: 0.07577471435070038
training step: 39680, total_loss: 1.653525471687317
training step: 39681, total_loss: 1.9046748876571655
training step: 39682, total_loss: 0.0006707520224153996
training step: 39683, total_loss: 0.6813191175460815
training step: 39684, total_loss: 3.038975715637207
training step: 39685, total_loss: 3.600691795349121
training step: 39686, total_loss: 1.7032861709594727
training step: 39687, total_loss: 1.46767258644104
training step: 39688, total_loss: 0.03551279753446579
training step: 39689, total_loss: 1.1079959869384766
training step: 39690, total_loss: 2.377127170562744
training step: 39691, total_loss: 1.4596850872039795
training step: 39692, total_loss: 0.7340521216392517
training step: 39693, total_loss: 0.5249990820884705
training step: 39694, total_loss: 1.7789976596832275
training step: 39695, total_loss: 3.2934036254882812
training step: 39696, total_loss: 9.545982360839844
training step: 39697, total_loss: 0.07445189356803894
training step: 39698, total_loss: 1.8451130390167236
training step: 39699, total_loss: 2.9856410026550293
training step: 39700, total_loss: 1.7552344799041748
training step: 39701, total_loss: 1.421566128730774
training step: 39702, total_loss: 2.3240702152252197
training step: 39703, total_loss: 0.37524425983428955
training step: 39704, total_loss: 1.2209641933441162
training step: 39705, total_loss: 1.951680064201355
training step: 39706, total_loss: 2.5536341667175293
training step: 39707, total_loss: 0.8676036596298218
training step: 39708, total_loss: 0.3393718898296356
training step: 39709, total_loss: 1.1450166702270508
training step: 39710, total_loss: 0.38446080684661865
training step: 39711, total_loss: 1.7414754629135132
training step: 39712, total_loss: 0.7724709510803223
training step: 39713, total_loss: 0.7144755125045776
training step: 39714, total_loss: 2.551393508911133
training step: 39715, total_loss: 0.7183712720870972
training step: 39716, total_loss: 1.4000911712646484
training step: 39717, total_loss: 2.747063636779785
training step: 39718, total_loss: 1.0386370420455933
training step: 39719, total_loss: 4.105029106140137
training step: 39720, total_loss: 2.400668144226074
training step: 39721, total_loss: 2.945528030395508
training step: 39722, total_loss: 3.4486353397369385
training step: 39723, total_loss: 0.4961909055709839
training step: 39724, total_loss: 0.5545784831047058
training step: 39725, total_loss: 0.005682175979018211
training step: 39726, total_loss: 0.05186997354030609
training step: 39727, total_loss: 2.1485748291015625
training step: 39728, total_loss: 0.6205469965934753
training step: 39729, total_loss: 2.895887851715088
training step: 39730, total_loss: 2.3904881477355957
training step: 39731, total_loss: 0.24336817860603333
training step: 39732, total_loss: 1.5573662519454956
training step: 39733, total_loss: 0.9667706489562988
training step: 39734, total_loss: 0.2932985723018646
training step: 39735, total_loss: 3.27130126953125
training step: 39736, total_loss: 0.06254000216722488
training step: 39737, total_loss: 0.9284728765487671
training step: 39738, total_loss: 3.7434403896331787
training step: 39739, total_loss: 1.0284719467163086
training step: 39740, total_loss: 1.6391849517822266
training step: 39741, total_loss: 1.9183361530303955
training step: 39742, total_loss: 1.8058216571807861
training step: 39743, total_loss: 0.15552595257759094
training step: 39744, total_loss: 0.25567805767059326
training step: 39745, total_loss: 1.5449461936950684
training step: 39746, total_loss: 1.3687143325805664
training step: 39747, total_loss: 0.8201988935470581
training step: 39748, total_loss: 0.5386366844177246
training step: 39749, total_loss: 0.6209442019462585
training step: 39750, total_loss: 0.9701264500617981
training step: 39751, total_loss: 0.06435991078615189
training step: 39752, total_loss: 0.01276901364326477
training step: 39753, total_loss: 5.025934219360352
training step: 39754, total_loss: 3.6392507553100586
training step: 39755, total_loss: 3.205042839050293
training step: 39756, total_loss: 0.649185836315155
training step: 39757, total_loss: 1.8763231039047241
training step: 39758, total_loss: 2.5872793197631836
training step: 39759, total_loss: 0.7059003710746765
training step: 39760, total_loss: 1.6696562767028809
training step: 39761, total_loss: 2.905606269836426
training step: 39762, total_loss: 1.6194549798965454
training step: 39763, total_loss: 1.8358268737792969
training step: 39764, total_loss: 0.9803758263587952
training step: 39765, total_loss: 2.8915233612060547
training step: 39766, total_loss: 2.1322789192199707
training step: 39767, total_loss: 0.7240308523178101
training step: 39768, total_loss: 0.9400705695152283
training step: 39769, total_loss: 2.7007782459259033
training step: 39770, total_loss: 0.7230488657951355
training step: 39771, total_loss: 0.513841450214386
training step: 39772, total_loss: 0.032257329672575
training step: 39773, total_loss: 2.045196294784546
training step: 39774, total_loss: 0.3872075080871582
training step: 39775, total_loss: 0.08117160201072693
training step: 39776, total_loss: 0.16247665882110596
training step: 39777, total_loss: 2.7405054569244385
training step: 39778, total_loss: 2.169679641723633
training step: 39779, total_loss: 0.43860507011413574
training step: 39780, total_loss: 2.0607080459594727
training step: 39781, total_loss: 2.4199068546295166
training step: 39782, total_loss: 1.2623724937438965
training step: 39783, total_loss: 2.45151948928833
training step: 39784, total_loss: 1.7790982723236084
training step: 39785, total_loss: 1.5199902057647705
training step: 39786, total_loss: 0.802893340587616
training step: 39787, total_loss: 0.9178369641304016
training step: 39788, total_loss: 2.481673240661621
training step: 39789, total_loss: 2.2828755378723145
training step: 39790, total_loss: 1.6306967735290527
training step: 39791, total_loss: 0.31763187050819397
training step: 39792, total_loss: 0.690234899520874
training step: 39793, total_loss: 2.8881988525390625
training step: 39794, total_loss: 0.018463118001818657
training step: 39795, total_loss: 0.8268613815307617
training step: 39796, total_loss: 0.05502580478787422
training step: 39797, total_loss: 0.9484850168228149
training step: 39798, total_loss: 1.246837854385376
training step: 39799, total_loss: 4.4023003578186035
training step: 39800, total_loss: 0.035962365567684174
training step: 39801, total_loss: 5.14663028717041
training step: 39802, total_loss: 0.6568278670310974
training step: 39803, total_loss: 1.228358268737793
training step: 39804, total_loss: 3.372528553009033
training step: 39805, total_loss: 2.302639961242676
training step: 39806, total_loss: 2.18330717086792
training step: 39807, total_loss: 1.5039277076721191
training step: 39808, total_loss: 2.095231533050537
training step: 39809, total_loss: 0.14747801423072815
training step: 39810, total_loss: 1.174778938293457
training step: 39811, total_loss: 2.235182762145996
training step: 39812, total_loss: 2.043872833251953
training step: 39813, total_loss: 1.973336935043335
training step: 39814, total_loss: 0.808003842830658
training step: 39815, total_loss: 0.17230607569217682
training step: 39816, total_loss: 0.9908691048622131
training step: 39817, total_loss: 0.8587020635604858
training step: 39818, total_loss: 3.5993967056274414
training step: 39819, total_loss: 0.08260904997587204
training step: 39820, total_loss: 1.8176285028457642
training step: 39821, total_loss: 6.287163257598877
training step: 39822, total_loss: 2.281951427459717
training step: 39823, total_loss: 0.7547037601470947
training step: 39824, total_loss: 1.7463217973709106
training step: 39825, total_loss: 5.559563159942627
training step: 39826, total_loss: 0.7481467127799988
training step: 39827, total_loss: 0.04368878901004791
training step: 39828, total_loss: 0.15589262545108795
training step: 39829, total_loss: 1.8381125926971436
training step: 39830, total_loss: 0.0032457076013088226
training step: 39831, total_loss: 1.817880630493164
training step: 39832, total_loss: 1.2870304584503174
training step: 39833, total_loss: 1.7154314517974854
training step: 39834, total_loss: 2.0759968757629395
training step: 39835, total_loss: 0.31225937604904175
training step: 39836, total_loss: 1.8007786273956299
training step: 39837, total_loss: 1.8176170587539673
training step: 39838, total_loss: 0.39844128489494324
training step: 39839, total_loss: 0.0717756599187851
training step: 39840, total_loss: 1.1426681280136108
training step: 39841, total_loss: 0.4487984776496887
training step: 39842, total_loss: 1.8821691274642944
training step: 39843, total_loss: 0.0593685656785965
training step: 39844, total_loss: 3.3462679386138916
training step: 39845, total_loss: 1.405998706817627
training step: 39846, total_loss: 0.4201597571372986
training step: 39847, total_loss: 0.04620108753442764
training step: 39848, total_loss: 1.1412394046783447
training step: 39849, total_loss: 1.970388412475586
training step: 39850, total_loss: 2.971407413482666
training step: 39851, total_loss: 1.353707194328308
training step: 39852, total_loss: 0.03457554429769516
training step: 39853, total_loss: 2.8080356121063232
training step: 39854, total_loss: 0.9239595532417297
training step: 39855, total_loss: 0.06434836983680725
training step: 39856, total_loss: 2.0691721439361572
training step: 39857, total_loss: 0.10119719803333282
training step: 39858, total_loss: 0.2094724476337433
training step: 39859, total_loss: 1.6784262657165527
training step: 39860, total_loss: 0.2361888736486435
training step: 39861, total_loss: 0.8047351837158203
training step: 39862, total_loss: 0.37389862537384033
training step: 39863, total_loss: 2.1943106651306152
training step: 39864, total_loss: 1.003080129623413
training step: 39865, total_loss: 0.9846817255020142
training step: 39866, total_loss: 0.2559588551521301
training step: 39867, total_loss: 0.9602296948432922
training step: 39868, total_loss: 0.00045003078412264585
training step: 39869, total_loss: 0.1180691346526146
training step: 39870, total_loss: 0.0551551878452301
training step: 39871, total_loss: 0.6849460005760193
training step: 39872, total_loss: 3.660040855407715
training step: 39873, total_loss: 2.022536516189575
training step: 39874, total_loss: 3.8870882987976074
training step: 39875, total_loss: 2.3078176975250244
training step: 39876, total_loss: 0.07356346398591995
training step: 39877, total_loss: 1.397322177886963
training step: 39878, total_loss: 3.3704683780670166
training step: 39879, total_loss: 1.2284371852874756
training step: 39880, total_loss: 1.0377167463302612
training step: 39881, total_loss: 1.6632401943206787
training step: 39882, total_loss: 1.7550859451293945
training step: 39883, total_loss: 0.16493506729602814
training step: 39884, total_loss: 0.004194246139377356
training step: 39885, total_loss: 2.2065625190734863
training step: 39886, total_loss: 4.597526550292969
training step: 39887, total_loss: 2.039902687072754
training step: 39888, total_loss: 1.2599232196807861
training step: 39889, total_loss: 1.3997406959533691
training step: 39890, total_loss: 0.20104888081550598
training step: 39891, total_loss: 1.584787368774414
training step: 39892, total_loss: 0.9654533863067627
training step: 39893, total_loss: 1.1329725980758667
training step: 39894, total_loss: 2.1087090969085693
training step: 39895, total_loss: 1.4297583103179932
training step: 39896, total_loss: 0.01731669157743454
training step: 39897, total_loss: 0.05613032728433609
training step: 39898, total_loss: 0.48118311166763306
training step: 39899, total_loss: 1.9494789838790894
training step: 39900, total_loss: 3.9866394996643066
training step: 39901, total_loss: 0.7233192920684814
training step: 39902, total_loss: 0.9557428956031799
training step: 39903, total_loss: 1.4918322563171387
training step: 39904, total_loss: 0.10786114633083344
training step: 39905, total_loss: 1.8851341009140015
training step: 39906, total_loss: 5.583948612213135
training step: 39907, total_loss: 1.4313418865203857
training step: 39908, total_loss: 1.825122356414795
training step: 39909, total_loss: 0.01309034414589405
training step: 39910, total_loss: 1.8438292741775513
training step: 39911, total_loss: 0.06662864983081818
training step: 39912, total_loss: 2.19966459274292
training step: 39913, total_loss: 0.4463377594947815
training step: 39914, total_loss: 0.029894374310970306
training step: 39915, total_loss: 1.1834385395050049
training step: 39916, total_loss: 2.1618332862854004
training step: 39917, total_loss: 1.5343303680419922
training step: 39918, total_loss: 0.0957086831331253
training step: 39919, total_loss: 0.3955954909324646
training step: 39920, total_loss: 2.344599723815918
training step: 39921, total_loss: 1.8579052686691284
training step: 39922, total_loss: 3.992436408996582
training step: 39923, total_loss: 1.3723787069320679
training step: 39924, total_loss: 2.3942160606384277
training step: 39925, total_loss: 0.2090386599302292
training step: 39926, total_loss: 0.9166741371154785
training step: 39927, total_loss: 1.0960906744003296
training step: 39928, total_loss: 2.261021852493286
training step: 39929, total_loss: 2.745296001434326
training step: 39930, total_loss: 0.24948541820049286
training step: 39931, total_loss: 1.9604947566986084
training step: 39932, total_loss: 1.1381324529647827
training step: 39933, total_loss: 2.3219285011291504
training step: 39934, total_loss: 1.9479520320892334
training step: 39935, total_loss: 1.5411419868469238
training step: 39936, total_loss: 0.5472782850265503
training step: 39937, total_loss: 0.5442442893981934
training step: 39938, total_loss: 1.466130256652832
training step: 39939, total_loss: 0.39265114068984985
training step: 39940, total_loss: 0.6332486271858215
training step: 39941, total_loss: 4.908161163330078
training step: 39942, total_loss: 3.8178648948669434
training step: 39943, total_loss: 0.43115735054016113
training step: 39944, total_loss: 0.07516025006771088
training step: 39945, total_loss: 1.2661799192428589
training step: 39946, total_loss: 1.4004733562469482
training step: 39947, total_loss: 0.058380670845508575
training step: 39948, total_loss: 0.07850774377584457
training step: 39949, total_loss: 2.761867046356201
training step: 39950, total_loss: 2.9781370162963867
training step: 39951, total_loss: 0.2380422055721283
training step: 39952, total_loss: 0.06541614234447479
training step: 39953, total_loss: 0.3654516041278839
training step: 39954, total_loss: 0.03165547549724579
training step: 39955, total_loss: 1.5840160846710205
training step: 39956, total_loss: 1.9156339168548584
training step: 39957, total_loss: 0.04625881463289261
training step: 39958, total_loss: 0.006463557481765747
training step: 39959, total_loss: 1.858931303024292
training step: 39960, total_loss: 1.154988169670105
training step: 39961, total_loss: 0.901934802532196
training step: 39962, total_loss: 2.1815993785858154
training step: 39963, total_loss: 3.416003942489624
training step: 39964, total_loss: 0.9246950745582581
training step: 39965, total_loss: 0.04997088387608528
training step: 39966, total_loss: 1.8381264209747314
training step: 39967, total_loss: 0.2515624761581421
training step: 39968, total_loss: 0.359131395816803
training step: 39969, total_loss: 1.430584192276001
training step: 39970, total_loss: 2.7726235389709473
training step: 39971, total_loss: 3.5760793685913086
training step: 39972, total_loss: 2.0291223526000977
training step: 39973, total_loss: 2.006450891494751
training step: 39974, total_loss: 3.1668379306793213
training step: 39975, total_loss: 1.0871695280075073
training step: 39976, total_loss: 1.6401755809783936
training step: 39977, total_loss: 1.0412853956222534
training step: 39978, total_loss: 1.6509560346603394
training step: 39979, total_loss: 0.03486676141619682
training step: 39980, total_loss: 0.00032669928623363376
training step: 39981, total_loss: 0.6674463748931885
training step: 39982, total_loss: 1.072553038597107
training step: 39983, total_loss: 1.7526538372039795
training step: 39984, total_loss: 2.5500292778015137
training step: 39985, total_loss: 2.442250967025757
training step: 39986, total_loss: 0.33743929862976074
training step: 39987, total_loss: 0.5171223878860474
training step: 39988, total_loss: 2.6535370349884033
training step: 39989, total_loss: 1.6297106742858887
training step: 39990, total_loss: 0.8613648414611816
training step: 39991, total_loss: 0.42930901050567627
training step: 39992, total_loss: 1.7806776762008667
training step: 39993, total_loss: 1.1988908052444458INFO:tensorflow:Writing predictions to: test_output/predictions_40000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_40000.json

training step: 39994, total_loss: 0.6236034631729126
training step: 39995, total_loss: 1.8599872589111328
training step: 39996, total_loss: 1.524788498878479
training step: 39997, total_loss: 1.634324312210083
training step: 39998, total_loss: 3.912994146347046
training step: 39999, total_loss: 1.0912775993347168
training step: 40000, total_loss: 2.8475308418273926
epoch finished! shuffle=False
evaluation: 4000, total_loss: 2.2391226291656494, f1: 48.39729276645901, followup: 24.859272782595465, yesno: 78.10740909782443, heq: 43.20705918150008, dheq: 1.5

Model saved in path test_output//model_40000.ckpt
training step: 40001, total_loss: 2.3651697635650635
training step: 40002, total_loss: 0.22578294575214386
training step: 40003, total_loss: 0.7454987168312073
training step: 40004, total_loss: 3.3964877128601074
training step: 40005, total_loss: 0.5492936372756958
training step: 40006, total_loss: 2.4951932430267334
training step: 40007, total_loss: 0.13215427100658417
training step: 40008, total_loss: 0.5764514207839966
training step: 40009, total_loss: 2.609978199005127
training step: 40010, total_loss: 3.139932155609131
training step: 40011, total_loss: 0.2550731301307678
training step: 40012, total_loss: 1.7400870323181152
training step: 40013, total_loss: 0.9887508153915405
training step: 40014, total_loss: 3.634603500366211
training step: 40015, total_loss: 2.7490320205688477
training step: 40016, total_loss: 1.6656761169433594
training step: 40017, total_loss: 0.981486976146698
training step: 40018, total_loss: 0.847831666469574
training step: 40019, total_loss: 2.387303352355957
training step: 40020, total_loss: 3.6349425315856934
training step: 40021, total_loss: 0.12984268367290497
training step: 40022, total_loss: 2.0674219131469727
training step: 40023, total_loss: 1.8922340869903564
training step: 40024, total_loss: 1.5666875839233398
training step: 40025, total_loss: 0.7342635989189148
training step: 40026, total_loss: 0.6196151971817017
training step: 40027, total_loss: 3.3259458541870117
training step: 40028, total_loss: 1.1338412761688232
training step: 40029, total_loss: 4.523041725158691
training step: 40030, total_loss: 1.2275357246398926
training step: 40031, total_loss: 1.0161808729171753
training step: 40032, total_loss: 1.0271207094192505
training step: 40033, total_loss: 0.5683186650276184
training step: 40034, total_loss: 0.9813734889030457
training step: 40035, total_loss: 1.8728742599487305
training step: 40036, total_loss: 2.4152708053588867
training step: 40037, total_loss: 1.5364701747894287
training step: 40038, total_loss: 1.4439176321029663
training step: 40039, total_loss: 4.863061904907227
training step: 40040, total_loss: 2.4326417446136475
training step: 40041, total_loss: 2.303169012069702
training step: 40042, total_loss: 1.0965094566345215
training step: 40043, total_loss: 2.131765604019165
training step: 40044, total_loss: 3.3269569873809814
training step: 40045, total_loss: 2.7889065742492676
training step: 40046, total_loss: 2.2135426998138428
training step: 40047, total_loss: 0.7605926990509033
training step: 40048, total_loss: 2.112823486328125
training step: 40049, total_loss: 0.18755316734313965
training step: 40050, total_loss: 1.288352370262146
training step: 40051, total_loss: 0.2572087049484253
training step: 40052, total_loss: 4.7060699462890625
training step: 40053, total_loss: 2.1679811477661133
training step: 40054, total_loss: 0.2628577649593353
training step: 40055, total_loss: 2.336459159851074
training step: 40056, total_loss: 2.079573631286621
training step: 40057, total_loss: 2.2789413928985596
training step: 40058, total_loss: 2.3959858417510986
training step: 40059, total_loss: 0.907014787197113
training step: 40060, total_loss: 1.4727349281311035
training step: 40061, total_loss: 0.4341179430484772
training step: 40062, total_loss: 3.3214876651763916
training step: 40063, total_loss: 2.0064380168914795
training step: 40064, total_loss: 0.09277433156967163
training step: 40065, total_loss: 0.7128134965896606
training step: 40066, total_loss: 2.460047721862793
training step: 40067, total_loss: 0.44998347759246826
training step: 40068, total_loss: 2.2965686321258545
training step: 40069, total_loss: 3.828134059906006
training step: 40070, total_loss: 0.7454652190208435
training step: 40071, total_loss: 0.5393219590187073
training step: 40072, total_loss: 0.7443414330482483
training step: 40073, total_loss: 0.1504620611667633
training step: 40074, total_loss: 1.4491987228393555
training step: 40075, total_loss: 0.7359914183616638
training step: 40076, total_loss: 0.8335245847702026
training step: 40077, total_loss: 0.7223606705665588
training step: 40078, total_loss: 0.515292227268219
training step: 40079, total_loss: 0.779873788356781
training step: 40080, total_loss: 0.8033238649368286
training step: 40081, total_loss: 0.5821653604507446
training step: 40082, total_loss: 1.7139617204666138
training step: 40083, total_loss: 2.982635021209717
training step: 40084, total_loss: 2.8008129596710205
training step: 40085, total_loss: 3.0148658752441406
training step: 40086, total_loss: 0.34485965967178345
training step: 40087, total_loss: 1.0420434474945068
training step: 40088, total_loss: 4.563583850860596
training step: 40089, total_loss: 1.7817823886871338
training step: 40090, total_loss: 3.682065486907959
training step: 40091, total_loss: 1.0679360628128052
training step: 40092, total_loss: 0.0436396487057209
training step: 40093, total_loss: 3.651829957962036
training step: 40094, total_loss: 3.1914563179016113
training step: 40095, total_loss: 1.964375376701355
training step: 40096, total_loss: 0.39548617601394653
training step: 40097, total_loss: 1.6715087890625
training step: 40098, total_loss: 0.8605887293815613
training step: 40099, total_loss: 0.3784022331237793
training step: 40100, total_loss: 0.9579222202301025
training step: 40101, total_loss: 1.4904096126556396
training step: 40102, total_loss: 1.0303711891174316
training step: 40103, total_loss: 1.0932139158248901
training step: 40104, total_loss: 2.8489222526550293
training step: 40105, total_loss: 2.3252639770507812
training step: 40106, total_loss: 3.6449458599090576
training step: 40107, total_loss: 2.5398449897766113
training step: 40108, total_loss: 2.9431681632995605
training step: 40109, total_loss: 0.0013465008232742548
training step: 40110, total_loss: 3.420140504837036
training step: 40111, total_loss: 0.10319219529628754
training step: 40112, total_loss: 2.688646078109741
training step: 40113, total_loss: 0.6573175191879272
training step: 40114, total_loss: 1.9994003772735596
training step: 40115, total_loss: 2.1669840812683105
training step: 40116, total_loss: 1.0677788257598877
training step: 40117, total_loss: 0.5027011036872864
training step: 40118, total_loss: 3.0938878059387207
training step: 40119, total_loss: 1.061535358428955
training step: 40120, total_loss: 1.093580722808838
training step: 40121, total_loss: 0.5874699950218201
training step: 40122, total_loss: 1.9868450164794922
training step: 40123, total_loss: 0.9066029787063599
training step: 40124, total_loss: 2.2144014835357666
training step: 40125, total_loss: 4.90686559677124
training step: 40126, total_loss: 0.0443267822265625
training step: 40127, total_loss: 1.682959794998169
training step: 40128, total_loss: 2.0111148357391357
training step: 40129, total_loss: 4.836442947387695
training step: 40130, total_loss: 3.5420773029327393
training step: 40131, total_loss: 2.471712589263916
training step: 40132, total_loss: 1.0669971704483032
training step: 40133, total_loss: 4.009855270385742
training step: 40134, total_loss: 1.4529085159301758
training step: 40135, total_loss: 1.5121541023254395
training step: 40136, total_loss: 4.213263988494873
training step: 40137, total_loss: 0.38546305894851685
training step: 40138, total_loss: 0.3745751976966858
training step: 40139, total_loss: 0.45199254155158997
training step: 40140, total_loss: 1.4735749959945679
training step: 40141, total_loss: 1.004541277885437
training step: 40142, total_loss: 6.752318382263184
training step: 40143, total_loss: 6.916107177734375
training step: 40144, total_loss: 0.28655171394348145
training step: 40145, total_loss: 1.6650267839431763
training step: 40146, total_loss: 2.7292113304138184
training step: 40147, total_loss: 1.2454159259796143
training step: 40148, total_loss: 0.5068797469139099
training step: 40149, total_loss: 0.49227994680404663
training step: 40150, total_loss: 5.769817352294922
training step: 40151, total_loss: 1.6017045974731445
training step: 40152, total_loss: 0.8949326276779175
training step: 40153, total_loss: 1.9459657669067383
training step: 40154, total_loss: 1.3467516899108887
training step: 40155, total_loss: 0.11091490089893341
training step: 40156, total_loss: 0.6584597826004028
training step: 40157, total_loss: 1.056404709815979
training step: 40158, total_loss: 0.6553031206130981
training step: 40159, total_loss: 1.093530535697937
training step: 40160, total_loss: 1.0080006122589111
training step: 40161, total_loss: 3.5057241916656494
training step: 40162, total_loss: 0.6897866129875183
training step: 40163, total_loss: 4.053447246551514
training step: 40164, total_loss: 2.223963975906372
training step: 40165, total_loss: 0.2877080738544464
training step: 40166, total_loss: 0.024094857275485992
training step: 40167, total_loss: 4.555361270904541
training step: 40168, total_loss: 0.8128398060798645
training step: 40169, total_loss: 3.721665382385254
training step: 40170, total_loss: 2.1217992305755615
training step: 40171, total_loss: 0.6751332879066467
training step: 40172, total_loss: 1.8896124362945557
training step: 40173, total_loss: 3.393587112426758
training step: 40174, total_loss: 2.8562870025634766
training step: 40175, total_loss: 1.3485304117202759
training step: 40176, total_loss: 3.2660012245178223
training step: 40177, total_loss: 0.5279006361961365
training step: 40178, total_loss: 3.020411968231201
training step: 40179, total_loss: 0.44826480746269226
training step: 40180, total_loss: 0.6534439921379089
training step: 40181, total_loss: 5.010307788848877
training step: 40182, total_loss: 2.1909117698669434
training step: 40183, total_loss: 0.4318022131919861
training step: 40184, total_loss: 1.088954210281372
training step: 40185, total_loss: 0.8010474443435669
training step: 40186, total_loss: 1.6279046535491943
training step: 40187, total_loss: 3.4228200912475586
training step: 40188, total_loss: 2.3036274909973145
training step: 40189, total_loss: 4.333046913146973
training step: 40190, total_loss: 0.15331634879112244
training step: 40191, total_loss: 0.03290461003780365
training step: 40192, total_loss: 3.6398367881774902
training step: 40193, total_loss: 3.2865946292877197
training step: 40194, total_loss: 1.0882205963134766
training step: 40195, total_loss: 1.9294536113739014
training step: 40196, total_loss: 0.6392776966094971
training step: 40197, total_loss: 1.7056941986083984
training step: 40198, total_loss: 0.5509340167045593
training step: 40199, total_loss: 1.571153163909912
training step: 40200, total_loss: 1.3259713649749756
training step: 40201, total_loss: 4.312909126281738
training step: 40202, total_loss: 0.007340226322412491
training step: 40203, total_loss: 1.3474819660186768
training step: 40204, total_loss: 1.7208532094955444
training step: 40205, total_loss: 3.0929155349731445
training step: 40206, total_loss: 0.535146951675415
training step: 40207, total_loss: 4.0301594734191895
training step: 40208, total_loss: 1.276945948600769
training step: 40209, total_loss: 0.753695011138916
training step: 40210, total_loss: 1.1247795820236206
training step: 40211, total_loss: 2.4029996395111084
training step: 40212, total_loss: 0.8343486785888672
training step: 40213, total_loss: 2.7917962074279785
training step: 40214, total_loss: 0.7815824747085571
training step: 40215, total_loss: 0.3214110732078552
training step: 40216, total_loss: 2.1468565464019775
training step: 40217, total_loss: 0.02189657650887966
training step: 40218, total_loss: 1.4558515548706055
training step: 40219, total_loss: 1.0630409717559814
training step: 40220, total_loss: 5.93375301361084
training step: 40221, total_loss: 0.05557083338499069
training step: 40222, total_loss: 1.1154390573501587
training step: 40223, total_loss: 1.6803361177444458
training step: 40224, total_loss: 2.741542339324951
training step: 40225, total_loss: 0.5383586287498474
training step: 40226, total_loss: 1.2275686264038086
training step: 40227, total_loss: 1.6109107732772827
training step: 40228, total_loss: 0.6145097613334656
training step: 40229, total_loss: 1.6600698232650757
training step: 40230, total_loss: 0.44774046540260315
training step: 40231, total_loss: 1.849299669265747
training step: 40232, total_loss: 1.4144814014434814
training step: 40233, total_loss: 1.7818412780761719
training step: 40234, total_loss: 2.531182289123535
training step: 40235, total_loss: 2.1773862838745117
training step: 40236, total_loss: 2.9529619216918945
training step: 40237, total_loss: 1.0429284572601318
training step: 40238, total_loss: 0.7652093768119812
training step: 40239, total_loss: 1.1648393869400024
training step: 40240, total_loss: 2.4391894340515137
training step: 40241, total_loss: 2.266237735748291
training step: 40242, total_loss: 0.13681021332740784
training step: 40243, total_loss: 1.940937876701355
training step: 40244, total_loss: 0.88134765625
training step: 40245, total_loss: 1.1226061582565308
training step: 40246, total_loss: 1.2839555740356445
training step: 40247, total_loss: 0.6440225839614868
training step: 40248, total_loss: 2.957273483276367
training step: 40249, total_loss: 0.7307966947555542
training step: 40250, total_loss: 3.106341600418091
training step: 40251, total_loss: 0.45064085721969604
training step: 40252, total_loss: 3.120762348175049
training step: 40253, total_loss: 0.8795842528343201
training step: 40254, total_loss: 1.1322526931762695
training step: 40255, total_loss: 0.06256508827209473
training step: 40256, total_loss: 2.872596263885498
training step: 40257, total_loss: 1.8371695280075073
training step: 40258, total_loss: 1.8418174982070923
training step: 40259, total_loss: 1.9090871810913086
training step: 40260, total_loss: 2.1956348419189453
training step: 40261, total_loss: 3.666637659072876
training step: 40262, total_loss: 2.101412534713745
training step: 40263, total_loss: 0.6910883188247681
training step: 40264, total_loss: 1.3549724817276
training step: 40265, total_loss: 0.10609589517116547
training step: 40266, total_loss: 3.815978765487671
training step: 40267, total_loss: 2.0308656692504883
training step: 40268, total_loss: 1.0187134742736816
training step: 40269, total_loss: 0.7069113850593567
training step: 40270, total_loss: 0.6090972423553467
training step: 40271, total_loss: 3.7462270259857178
training step: 40272, total_loss: 0.9500271677970886
training step: 40273, total_loss: 2.696560859680176
training step: 40274, total_loss: 1.1841905117034912
training step: 40275, total_loss: 2.5754811763763428
training step: 40276, total_loss: 0.6614409685134888
training step: 40277, total_loss: 2.25006365776062
training step: 40278, total_loss: 0.6328264474868774
training step: 40279, total_loss: 0.49500030279159546
training step: 40280, total_loss: 1.9956648349761963
training step: 40281, total_loss: 2.7332897186279297
training step: 40282, total_loss: 2.2245469093322754
training step: 40283, total_loss: 1.5225036144256592
training step: 40284, total_loss: 2.181032657623291
training step: 40285, total_loss: 2.0412347316741943
training step: 40286, total_loss: 5.441415786743164
training step: 40287, total_loss: 2.4742894172668457
training step: 40288, total_loss: 0.8322539329528809
training step: 40289, total_loss: 4.683372497558594
training step: 40290, total_loss: 1.0976295471191406
training step: 40291, total_loss: 2.0728654861450195
training step: 40292, total_loss: 2.4953956604003906
training step: 40293, total_loss: 0.3865879476070404
training step: 40294, total_loss: 5.002405643463135
training step: 40295, total_loss: 3.218205451965332
training step: 40296, total_loss: 1.1978375911712646
training step: 40297, total_loss: 1.671466588973999
training step: 40298, total_loss: 2.7503652572631836
training step: 40299, total_loss: 1.4105172157287598
training step: 40300, total_loss: 3.630829334259033
training step: 40301, total_loss: 1.6831183433532715
training step: 40302, total_loss: 1.5218151807785034
training step: 40303, total_loss: 0.0052517978474497795
training step: 40304, total_loss: 1.4288618564605713
training step: 40305, total_loss: 3.2717514038085938
training step: 40306, total_loss: 3.4107003211975098
training step: 40307, total_loss: 1.5892481803894043
training step: 40308, total_loss: 3.110464096069336
training step: 40309, total_loss: 1.6573553085327148
training step: 40310, total_loss: 1.6854159832000732
training step: 40311, total_loss: 0.5920268297195435
training step: 40312, total_loss: 1.689034342765808
training step: 40313, total_loss: 3.186816453933716
training step: 40314, total_loss: 2.271623134613037
training step: 40315, total_loss: 1.72092604637146
training step: 40316, total_loss: 0.7447274327278137
training step: 40317, total_loss: 3.3826680183410645
training step: 40318, total_loss: 2.5329205989837646
training step: 40319, total_loss: 1.2301276922225952
training step: 40320, total_loss: 1.5928715467453003
training step: 40321, total_loss: 0.644385576248169
training step: 40322, total_loss: 0.03599923104047775
training step: 40323, total_loss: 2.0941975116729736
training step: 40324, total_loss: 1.4343297481536865
training step: 40325, total_loss: 0.0021207821555435658
training step: 40326, total_loss: 1.2518196105957031
training step: 40327, total_loss: 0.6712073087692261
training step: 40328, total_loss: 4.3052544593811035
training step: 40329, total_loss: 2.8852698802948
training step: 40330, total_loss: 1.0531560182571411
training step: 40331, total_loss: 0.0027542030438780785
training step: 40332, total_loss: 1.1227657794952393
training step: 40333, total_loss: 1.5712611675262451
training step: 40334, total_loss: 0.980209231376648
training step: 40335, total_loss: 0.823943018913269
training step: 40336, total_loss: 0.5406303405761719
training step: 40337, total_loss: 0.6249772310256958
training step: 40338, total_loss: 1.543259859085083
training step: 40339, total_loss: 3.2473602294921875
training step: 40340, total_loss: 0.6131317019462585
training step: 40341, total_loss: 1.471659541130066
training step: 40342, total_loss: 2.9376087188720703
training step: 40343, total_loss: 1.205641508102417
training step: 40344, total_loss: 0.8174511790275574
training step: 40345, total_loss: 2.045708656311035
training step: 40346, total_loss: 2.202127456665039
training step: 40347, total_loss: 1.1197714805603027
training step: 40348, total_loss: 2.003108024597168
training step: 40349, total_loss: 0.2987467646598816
training step: 40350, total_loss: 2.237245559692383
training step: 40351, total_loss: 3.5493760108947754
training step: 40352, total_loss: 0.09495451301336288
training step: 40353, total_loss: 0.5393475890159607
training step: 40354, total_loss: 2.051652431488037
training step: 40355, total_loss: 0.9427283406257629
training step: 40356, total_loss: 0.30024781823158264
training step: 40357, total_loss: 0.12470714747905731
training step: 40358, total_loss: 1.5064252614974976
training step: 40359, total_loss: 0.33128470182418823
training step: 40360, total_loss: 0.8393361568450928
training step: 40361, total_loss: 1.5011048316955566
training step: 40362, total_loss: 3.064061164855957
training step: 40363, total_loss: 0.035677459090948105
training step: 40364, total_loss: 1.0872552394866943
training step: 40365, total_loss: 0.3036578893661499
training step: 40366, total_loss: 1.5364054441452026
training step: 40367, total_loss: 0.8867356181144714
training step: 40368, total_loss: 1.0470839738845825
training step: 40369, total_loss: 2.623203754425049
training step: 40370, total_loss: 2.6637051105499268
training step: 40371, total_loss: 2.871743679046631
training step: 40372, total_loss: 1.111732006072998
training step: 40373, total_loss: 0.0019627446308732033
training step: 40374, total_loss: 2.7504892349243164
training step: 40375, total_loss: 2.5928852558135986
training step: 40376, total_loss: 4.73982572555542
training step: 40377, total_loss: 1.2359145879745483
training step: 40378, total_loss: 0.8092679381370544
training step: 40379, total_loss: 5.974356174468994
training step: 40380, total_loss: 1.4866517782211304
training step: 40381, total_loss: 1.283642292022705
training step: 40382, total_loss: 1.0741806030273438
training step: 40383, total_loss: 0.22676676511764526
training step: 40384, total_loss: 2.0017471313476562
training step: 40385, total_loss: 1.294337272644043
training step: 40386, total_loss: 5.434983253479004
training step: 40387, total_loss: 4.608519554138184
training step: 40388, total_loss: 1.1752164363861084
training step: 40389, total_loss: 2.8967227935791016
training step: 40390, total_loss: 0.35356405377388
training step: 40391, total_loss: 1.3290913105010986
training step: 40392, total_loss: 1.0388753414154053
training step: 40393, total_loss: 2.0762622356414795
training step: 40394, total_loss: 1.8400933742523193
training step: 40395, total_loss: 9.490049362182617
training step: 40396, total_loss: 1.7310948371887207
training step: 40397, total_loss: 0.031388796865940094
training step: 40398, total_loss: 1.2147955894470215
training step: 40399, total_loss: 3.8797507286071777
training step: 40400, total_loss: 2.0760512351989746
training step: 40401, total_loss: 0.34580734372138977
training step: 40402, total_loss: 1.2145226001739502
training step: 40403, total_loss: 3.3771514892578125
training step: 40404, total_loss: 1.2079403400421143
training step: 40405, total_loss: 0.1213875263929367
training step: 40406, total_loss: 1.1304551362991333
training step: 40407, total_loss: 1.1770120859146118
training step: 40408, total_loss: 2.6676011085510254
training step: 40409, total_loss: 1.1332114934921265
training step: 40410, total_loss: 1.4711723327636719
training step: 40411, total_loss: 2.0735926628112793
training step: 40412, total_loss: 2.60941219329834
training step: 40413, total_loss: 2.8009371757507324
training step: 40414, total_loss: 0.24656732380390167
training step: 40415, total_loss: 1.2179125547409058
training step: 40416, total_loss: 2.5235133171081543
training step: 40417, total_loss: 0.5443181991577148
training step: 40418, total_loss: 1.7156428098678589
training step: 40419, total_loss: 0.7249818444252014
training step: 40420, total_loss: 0.32235464453697205
training step: 40421, total_loss: 1.5494105815887451
training step: 40422, total_loss: 3.9299826622009277
training step: 40423, total_loss: 2.5738019943237305
training step: 40424, total_loss: 0.34913259744644165
training step: 40425, total_loss: 0.09216398745775223
training step: 40426, total_loss: 1.3579885959625244
training step: 40427, total_loss: 1.7495062351226807
training step: 40428, total_loss: 2.548809766769409
training step: 40429, total_loss: 3.4396960735321045
training step: 40430, total_loss: 1.7785286903381348
training step: 40431, total_loss: 1.0435175895690918
training step: 40432, total_loss: 0.842673659324646
training step: 40433, total_loss: 2.7064318656921387
training step: 40434, total_loss: 2.043391704559326
training step: 40435, total_loss: 3.1451282501220703
training step: 40436, total_loss: 1.3559808731079102
training step: 40437, total_loss: 2.542057991027832
training step: 40438, total_loss: 1.9068572521209717
training step: 40439, total_loss: 1.1827774047851562
training step: 40440, total_loss: 2.26802921295166
training step: 40441, total_loss: 0.7361890077590942
training step: 40442, total_loss: 0.9985040426254272
training step: 40443, total_loss: 0.39946794509887695
training step: 40444, total_loss: 0.7915900349617004
training step: 40445, total_loss: 5.538273811340332
training step: 40446, total_loss: 0.8316276669502258
training step: 40447, total_loss: 2.204249143600464
training step: 40448, total_loss: 2.7935032844543457
training step: 40449, total_loss: 0.7794924974441528
training step: 40450, total_loss: 1.9403016567230225
training step: 40451, total_loss: 1.7021299600601196
training step: 40452, total_loss: 0.143214613199234
training step: 40453, total_loss: 0.15423697233200073
training step: 40454, total_loss: 0.8773099184036255
training step: 40455, total_loss: 2.117969036102295
training step: 40456, total_loss: 0.22406473755836487
training step: 40457, total_loss: 1.8075835704803467
training step: 40458, total_loss: 1.7279372215270996
training step: 40459, total_loss: 0.8467845320701599
training step: 40460, total_loss: 1.37239408493042
training step: 40461, total_loss: 1.8141814470291138
training step: 40462, total_loss: 3.44362735748291
training step: 40463, total_loss: 0.7019640803337097
training step: 40464, total_loss: 0.03507700935006142
training step: 40465, total_loss: 0.8968075513839722
training step: 40466, total_loss: 5.346590042114258
training step: 40467, total_loss: 1.0335873365402222
training step: 40468, total_loss: 1.5314522981643677
training step: 40469, total_loss: 1.1884925365447998
training step: 40470, total_loss: 0.525114893913269
training step: 40471, total_loss: 0.6778395175933838
training step: 40472, total_loss: 1.3401559591293335
training step: 40473, total_loss: 2.3306570053100586
training step: 40474, total_loss: 3.0620932579040527
training step: 40475, total_loss: 1.9337351322174072
training step: 40476, total_loss: 2.555769443511963
training step: 40477, total_loss: 0.01042370405048132
training step: 40478, total_loss: 0.01708524115383625
training step: 40479, total_loss: 1.0026183128356934
training step: 40480, total_loss: 0.34224480390548706
training step: 40481, total_loss: 1.9998219013214111
training step: 40482, total_loss: 4.096652984619141
training step: 40483, total_loss: 0.7732991576194763
training step: 40484, total_loss: 3.2415778636932373
training step: 40485, total_loss: 1.7710182666778564
training step: 40486, total_loss: 1.7623976469039917
training step: 40487, total_loss: 1.2039638757705688
training step: 40488, total_loss: 0.9594897031784058
training step: 40489, total_loss: 0.6483263373374939
training step: 40490, total_loss: 2.0231337547302246
training step: 40491, total_loss: 0.8563812375068665
training step: 40492, total_loss: 0.8763816356658936
training step: 40493, total_loss: 1.5538625717163086
training step: 40494, total_loss: 2.482522487640381
training step: 40495, total_loss: 4.011576175689697
training step: 40496, total_loss: 1.8373651504516602
training step: 40497, total_loss: 0.9867066144943237
training step: 40498, total_loss: 0.9098179340362549
training step: 40499, total_loss: 1.9781726598739624
training step: 40500, total_loss: 1.4907143115997314
training step: 40501, total_loss: 0.37726467847824097
training step: 40502, total_loss: 0.8521481156349182
training step: 40503, total_loss: 0.8122169971466064
training step: 40504, total_loss: 1.7980263233184814
training step: 40505, total_loss: 0.5055784583091736
training step: 40506, total_loss: 1.0506352186203003
training step: 40507, total_loss: 1.149763822555542
training step: 40508, total_loss: 2.6571407318115234
training step: 40509, total_loss: 0.2436465173959732
training step: 40510, total_loss: 0.20898091793060303
training step: 40511, total_loss: 0.13517926633358002
training step: 40512, total_loss: 0.3372335135936737
training step: 40513, total_loss: 1.1676901578903198
training step: 40514, total_loss: 2.2827863693237305
training step: 40515, total_loss: 0.10839397460222244
training step: 40516, total_loss: 1.2235589027404785
training step: 40517, total_loss: 1.9854018688201904
training step: 40518, total_loss: 3.201077699661255
training step: 40519, total_loss: 2.66280460357666
training step: 40520, total_loss: 1.0736955404281616
training step: 40521, total_loss: 0.004653862677514553
training step: 40522, total_loss: 2.505254030227661
training step: 40523, total_loss: 0.5045933723449707
training step: 40524, total_loss: 0.014620931819081306
training step: 40525, total_loss: 0.4508357644081116
training step: 40526, total_loss: 5.019542694091797
training step: 40527, total_loss: 0.049271758645772934
training step: 40528, total_loss: 3.9355220794677734
training step: 40529, total_loss: 2.7774178981781006
training step: 40530, total_loss: 2.080734968185425
training step: 40531, total_loss: 0.8026829361915588
training step: 40532, total_loss: 2.7275757789611816
training step: 40533, total_loss: 2.7862162590026855
training step: 40534, total_loss: 1.51938796043396
training step: 40535, total_loss: 2.5977325439453125
training step: 40536, total_loss: 1.628357172012329
training step: 40537, total_loss: 1.9906169176101685
training step: 40538, total_loss: 0.866340696811676
training step: 40539, total_loss: 0.10624684393405914
training step: 40540, total_loss: 1.7718586921691895
training step: 40541, total_loss: 0.6465128660202026
training step: 40542, total_loss: 1.041563630104065
training step: 40543, total_loss: 3.1444902420043945
training step: 40544, total_loss: 0.0031939526088535786
training step: 40545, total_loss: 1.1722956895828247
training step: 40546, total_loss: 0.6777324080467224
training step: 40547, total_loss: 0.14569254219532013
training step: 40548, total_loss: 1.2014737129211426
training step: 40549, total_loss: 0.12912756204605103
training step: 40550, total_loss: 2.3856608867645264
training step: 40551, total_loss: 2.903027057647705
training step: 40552, total_loss: 1.1493077278137207
training step: 40553, total_loss: 0.35617730021476746
training step: 40554, total_loss: 2.161017894744873
training step: 40555, total_loss: 2.469010829925537
training step: 40556, total_loss: 0.004616170655936003
training step: 40557, total_loss: 2.0219063758850098
training step: 40558, total_loss: 0.7746554613113403
training step: 40559, total_loss: 3.1829004287719727
training step: 40560, total_loss: 1.3053624629974365
training step: 40561, total_loss: 2.994328498840332
training step: 40562, total_loss: 2.1855292320251465
training step: 40563, total_loss: 2.2255420684814453
training step: 40564, total_loss: 3.6010773181915283
training step: 40565, total_loss: 1.8352806568145752
training step: 40566, total_loss: 1.715023398399353
training step: 40567, total_loss: 0.004241378512233496
training step: 40568, total_loss: 0.42925119400024414
training step: 40569, total_loss: 1.9342947006225586
training step: 40570, total_loss: 3.3029768466949463
training step: 40571, total_loss: 2.431274652481079
training step: 40572, total_loss: 2.8059964179992676
training step: 40573, total_loss: 0.7473359704017639
training step: 40574, total_loss: 2.0567634105682373
training step: 40575, total_loss: 2.4351353645324707
training step: 40576, total_loss: 1.3120558261871338
training step: 40577, total_loss: 3.9358131885528564
training step: 40578, total_loss: 2.8368303775787354
training step: 40579, total_loss: 0.14341220259666443
training step: 40580, total_loss: 1.2984371185302734
training step: 40581, total_loss: 2.0863196849823
training step: 40582, total_loss: 2.243330955505371
training step: 40583, total_loss: 1.1811569929122925
training step: 40584, total_loss: 3.4817347526550293
training step: 40585, total_loss: 3.1080403327941895
training step: 40586, total_loss: 1.8475749492645264
training step: 40587, total_loss: 1.6187872886657715
training step: 40588, total_loss: 0.9353662729263306
training step: 40589, total_loss: 1.4509048461914062
training step: 40590, total_loss: 1.1451292037963867
training step: 40591, total_loss: 0.3704841732978821
training step: 40592, total_loss: 1.5673893690109253
training step: 40593, total_loss: 0.04262378811836243
training step: 40594, total_loss: 0.8164694309234619
training step: 40595, total_loss: 0.6287413835525513
training step: 40596, total_loss: 1.9512958526611328
training step: 40597, total_loss: 2.145484685897827
training step: 40598, total_loss: 0.19491425156593323
training step: 40599, total_loss: 0.117851123213768
training step: 40600, total_loss: 1.7912676334381104
training step: 40601, total_loss: 1.7278907299041748
training step: 40602, total_loss: 1.3595331907272339
training step: 40603, total_loss: 0.3686043322086334
training step: 40604, total_loss: 1.5791943073272705
training step: 40605, total_loss: 0.9617886543273926
training step: 40606, total_loss: 2.071267604827881
training step: 40607, total_loss: 1.869208812713623
training step: 40608, total_loss: 0.057887930423021317
training step: 40609, total_loss: 0.48513033986091614
training step: 40610, total_loss: 0.6569728851318359
training step: 40611, total_loss: 2.2255001068115234
training step: 40612, total_loss: 1.109588623046875
training step: 40613, total_loss: 1.0366384983062744
training step: 40614, total_loss: 2.591576099395752
training step: 40615, total_loss: 2.996087074279785
training step: 40616, total_loss: 3.89003324508667
training step: 40617, total_loss: 2.3407816886901855
training step: 40618, total_loss: 2.0132668018341064
training step: 40619, total_loss: 1.7455577850341797
training step: 40620, total_loss: 1.4561010599136353
training step: 40621, total_loss: 1.4567471742630005
training step: 40622, total_loss: 2.05901837348938
training step: 40623, total_loss: 0.4944281578063965
training step: 40624, total_loss: 0.4020613431930542
training step: 40625, total_loss: 0.8801378011703491
training step: 40626, total_loss: 2.9392824172973633
training step: 40627, total_loss: 2.0579452514648438
training step: 40628, total_loss: 1.3187836408615112
training step: 40629, total_loss: 0.6000515222549438
training step: 40630, total_loss: 0.2377205491065979
training step: 40631, total_loss: 1.823439121246338
training step: 40632, total_loss: 2.2861711978912354
training step: 40633, total_loss: 4.140035629272461
training step: 40634, total_loss: 1.4843205213546753
training step: 40635, total_loss: 1.3847228288650513
training step: 40636, total_loss: 0.8888534307479858
training step: 40637, total_loss: 1.8034899234771729
training step: 40638, total_loss: 0.07270364463329315
training step: 40639, total_loss: 0.1491549015045166
training step: 40640, total_loss: 0.6269278526306152
training step: 40641, total_loss: 1.9548269510269165
training step: 40642, total_loss: 1.265960931777954
training step: 40643, total_loss: 1.048048734664917
training step: 40644, total_loss: 2.7867631912231445
training step: 40645, total_loss: 1.5940682888031006
training step: 40646, total_loss: 1.3546985387802124
training step: 40647, total_loss: 0.3530174493789673
training step: 40648, total_loss: 1.3722782135009766
training step: 40649, total_loss: 1.0193673372268677
training step: 40650, total_loss: 1.8764513731002808
training step: 40651, total_loss: 1.2471106052398682
training step: 40652, total_loss: 4.246818542480469
training step: 40653, total_loss: 1.9735674858093262
training step: 40654, total_loss: 1.6746102571487427
training step: 40655, total_loss: 3.474008083343506
training step: 40656, total_loss: 0.05664850026369095
training step: 40657, total_loss: 1.1975243091583252
training step: 40658, total_loss: 0.8398182392120361
training step: 40659, total_loss: 3.2077646255493164
training step: 40660, total_loss: 2.4273953437805176
training step: 40661, total_loss: 3.552058219909668
training step: 40662, total_loss: 3.8064322471618652
training step: 40663, total_loss: 0.11878074705600739
training step: 40664, total_loss: 0.958243727684021
training step: 40665, total_loss: 1.6956770420074463
training step: 40666, total_loss: 1.4829713106155396
training step: 40667, total_loss: 0.00617299834266305
training step: 40668, total_loss: 3.6405091285705566
training step: 40669, total_loss: 1.0965415239334106
training step: 40670, total_loss: 0.4756014049053192
training step: 40671, total_loss: 2.274646282196045
training step: 40672, total_loss: 2.705801010131836
training step: 40673, total_loss: 1.3384225368499756
training step: 40674, total_loss: 1.3668808937072754
training step: 40675, total_loss: 3.1712372303009033
training step: 40676, total_loss: 1.262972116470337
training step: 40677, total_loss: 0.5957604050636292
training step: 40678, total_loss: 2.724764823913574
training step: 40679, total_loss: 1.5876636505126953
training step: 40680, total_loss: 1.0257726907730103
training step: 40681, total_loss: 0.9382110834121704
training step: 40682, total_loss: 1.4845466613769531
training step: 40683, total_loss: 2.4677228927612305
training step: 40684, total_loss: 2.3457131385803223
training step: 40685, total_loss: 1.6973600387573242
training step: 40686, total_loss: 4.0619354248046875
training step: 40687, total_loss: 1.9170269966125488
training step: 40688, total_loss: 0.8555750250816345
training step: 40689, total_loss: 1.108384132385254
training step: 40690, total_loss: 1.3174710273742676
training step: 40691, total_loss: 1.5778285264968872
training step: 40692, total_loss: 2.505315065383911
training step: 40693, total_loss: 2.9010424613952637
training step: 40694, total_loss: 2.991572141647339
training step: 40695, total_loss: 1.5097453594207764
training step: 40696, total_loss: 2.381070613861084
training step: 40697, total_loss: 1.9110218286514282
training step: 40698, total_loss: 1.5781888961791992
training step: 40699, total_loss: 2.5467963218688965
training step: 40700, total_loss: 1.1786866188049316
training step: 40701, total_loss: 1.6141375303268433
training step: 40702, total_loss: 0.008542108349502087
training step: 40703, total_loss: 0.05262939631938934
training step: 40704, total_loss: 1.4040074348449707
training step: 40705, total_loss: 0.6462699174880981
training step: 40706, total_loss: 3.6954965591430664
training step: 40707, total_loss: 2.21243953704834
training step: 40708, total_loss: 1.9853030443191528
training step: 40709, total_loss: 0.46282827854156494
training step: 40710, total_loss: 0.27701976895332336
training step: 40711, total_loss: 0.07173137366771698
training step: 40712, total_loss: 2.7279953956604004
training step: 40713, total_loss: 0.44967496395111084
training step: 40714, total_loss: 2.8365612030029297
training step: 40715, total_loss: 0.19820518791675568
training step: 40716, total_loss: 2.060880661010742
training step: 40717, total_loss: 0.36617469787597656
training step: 40718, total_loss: 0.04275151714682579
training step: 40719, total_loss: 1.6928560733795166
training step: 40720, total_loss: 2.1100642681121826
training step: 40721, total_loss: 2.083005428314209
training step: 40722, total_loss: 5.275576591491699
training step: 40723, total_loss: 0.08881372213363647
training step: 40724, total_loss: 1.8584051132202148
training step: 40725, total_loss: 1.7513978481292725
training step: 40726, total_loss: 1.7070581912994385
training step: 40727, total_loss: 4.135449409484863
training step: 40728, total_loss: 2.552325487136841
training step: 40729, total_loss: 0.024465860798954964
training step: 40730, total_loss: 4.42410945892334
training step: 40731, total_loss: 2.2797141075134277
training step: 40732, total_loss: 2.530672550201416
training step: 40733, total_loss: 0.002774054417386651
training step: 40734, total_loss: 1.5154945850372314
training step: 40735, total_loss: 2.505389451980591
training step: 40736, total_loss: 0.9663950800895691
training step: 40737, total_loss: 1.1613857746124268
training step: 40738, total_loss: 3.2413835525512695
training step: 40739, total_loss: 4.512488842010498
training step: 40740, total_loss: 1.2740504741668701
training step: 40741, total_loss: 1.6755599975585938
training step: 40742, total_loss: 1.630964756011963
training step: 40743, total_loss: 1.7032110691070557
training step: 40744, total_loss: 0.6098269820213318
training step: 40745, total_loss: 0.45582127571105957
training step: 40746, total_loss: 1.2452729940414429
training step: 40747, total_loss: 0.6664820909500122
training step: 40748, total_loss: 1.2652673721313477
training step: 40749, total_loss: 0.6143463850021362
training step: 40750, total_loss: 0.2779141664505005
training step: 40751, total_loss: 0.2944088876247406
training step: 40752, total_loss: 1.166762113571167
training step: 40753, total_loss: 3.3691134452819824
training step: 40754, total_loss: 2.882401943206787
training step: 40755, total_loss: 3.5494542121887207
training step: 40756, total_loss: 4.647059440612793
training step: 40757, total_loss: 1.399338960647583
training step: 40758, total_loss: 1.4568729400634766
training step: 40759, total_loss: 0.06386852264404297
training step: 40760, total_loss: 0.25708603858947754
training step: 40761, total_loss: 0.33024972677230835
training step: 40762, total_loss: 1.5536956787109375
training step: 40763, total_loss: 2.3395137786865234
training step: 40764, total_loss: 1.9545295238494873
training step: 40765, total_loss: 0.0021393592469394207
training step: 40766, total_loss: 3.6701619625091553
training step: 40767, total_loss: 0.7321933507919312
training step: 40768, total_loss: 1.4375793933868408
training step: 40769, total_loss: 2.5823974609375
training step: 40770, total_loss: 3.0488131046295166
training step: 40771, total_loss: 0.07006879150867462
training step: 40772, total_loss: 3.497220993041992
training step: 40773, total_loss: 0.5590188503265381
training step: 40774, total_loss: 0.34120967984199524
training step: 40775, total_loss: 1.875830888748169
training step: 40776, total_loss: 1.162370204925537
training step: 40777, total_loss: 2.000189781188965
training step: 40778, total_loss: 0.01971481367945671
training step: 40779, total_loss: 2.1703944206237793
training step: 40780, total_loss: 0.04606879502534866
training step: 40781, total_loss: 4.108187675476074
training step: 40782, total_loss: 2.680799722671509
training step: 40783, total_loss: 2.1755197048187256
training step: 40784, total_loss: 1.3134814500808716
training step: 40785, total_loss: 0.9620860815048218
training step: 40786, total_loss: 0.13858003914356232
training step: 40787, total_loss: 2.7753090858459473
training step: 40788, total_loss: 1.7708007097244263
training step: 40789, total_loss: 3.0927586555480957
training step: 40790, total_loss: 0.358209490776062
training step: 40791, total_loss: 2.11668062210083
training step: 40792, total_loss: 1.683002233505249
training step: 40793, total_loss: 3.427016496658325
training step: 40794, total_loss: 2.46053409576416
training step: 40795, total_loss: 0.8376287817955017
training step: 40796, total_loss: 2.776901960372925
training step: 40797, total_loss: 2.428915500640869
training step: 40798, total_loss: 0.8065406084060669
training step: 40799, total_loss: 0.00066601880826056
training step: 40800, total_loss: 2.2271318435668945
training step: 40801, total_loss: 2.8092522621154785
training step: 40802, total_loss: 1.7736046314239502
training step: 40803, total_loss: 3.858635663986206
training step: 40804, total_loss: 1.2970209121704102
training step: 40805, total_loss: 0.5581594109535217
training step: 40806, total_loss: 2.439248561859131
training step: 40807, total_loss: 0.502020001411438
training step: 40808, total_loss: 0.13481220602989197
training step: 40809, total_loss: 0.08376039564609528
training step: 40810, total_loss: 2.0107169151306152
training step: 40811, total_loss: 1.9830907583236694
training step: 40812, total_loss: 0.6665547490119934
training step: 40813, total_loss: 0.6422961950302124
training step: 40814, total_loss: 1.2591056823730469
training step: 40815, total_loss: 0.15629249811172485
training step: 40816, total_loss: 0.5911945104598999
training step: 40817, total_loss: 0.39463889598846436
training step: 40818, total_loss: 0.7260639667510986
training step: 40819, total_loss: 0.8507194519042969
training step: 40820, total_loss: 2.753692150115967
training step: 40821, total_loss: 3.302001476287842
training step: 40822, total_loss: 3.6523847579956055
training step: 40823, total_loss: 2.0626718997955322
training step: 40824, total_loss: 2.758829116821289
training step: 40825, total_loss: 0.8185842037200928
training step: 40826, total_loss: 2.5748138427734375
training step: 40827, total_loss: 0.4198833405971527
training step: 40828, total_loss: 0.215241938829422
training step: 40829, total_loss: 1.3930269479751587
training step: 40830, total_loss: 0.07307581603527069
training step: 40831, total_loss: 1.6697876453399658
training step: 40832, total_loss: 2.599724769592285
training step: 40833, total_loss: 1.5382188558578491
training step: 40834, total_loss: 0.698877215385437
training step: 40835, total_loss: 1.8653473854064941
training step: 40836, total_loss: 0.8411434292793274
training step: 40837, total_loss: 3.6147069931030273
training step: 40838, total_loss: 0.000665323983412236
training step: 40839, total_loss: 0.6390988230705261
training step: 40840, total_loss: 2.772707939147949
training step: 40841, total_loss: 1.61862051486969
training step: 40842, total_loss: 0.0766189694404602
training step: 40843, total_loss: 1.7950661182403564
training step: 40844, total_loss: 1.474074363708496
training step: 40845, total_loss: 2.1013545989990234
training step: 40846, total_loss: 0.834430456161499
training step: 40847, total_loss: 0.006331339478492737
training step: 40848, total_loss: 1.2279720306396484
training step: 40849, total_loss: 0.17237588763237
training step: 40850, total_loss: 0.43852850794792175
training step: 40851, total_loss: 0.9136258959770203
training step: 40852, total_loss: 1.7164239883422852
training step: 40853, total_loss: 1.5998040437698364
training step: 40854, total_loss: 0.8600586652755737
training step: 40855, total_loss: 3.56929874420166
training step: 40856, total_loss: 0.6467376947402954
training step: 40857, total_loss: 4.71715784072876
training step: 40858, total_loss: 3.1315131187438965
training step: 40859, total_loss: 1.3560686111450195
training step: 40860, total_loss: 0.724927544593811
training step: 40861, total_loss: 2.103273868560791
training step: 40862, total_loss: 0.3312912881374359
training step: 40863, total_loss: 0.5723029375076294
training step: 40864, total_loss: 0.279690146446228
training step: 40865, total_loss: 1.9039303064346313
training step: 40866, total_loss: 0.2606636881828308
training step: 40867, total_loss: 1.6555557250976562
training step: 40868, total_loss: 0.11413279175758362
training step: 40869, total_loss: 1.5417488813400269
training step: 40870, total_loss: 1.1191530227661133
training step: 40871, total_loss: 0.027162548154592514
training step: 40872, total_loss: 2.2040719985961914
training step: 40873, total_loss: 0.3035390377044678
training step: 40874, total_loss: 7.198955535888672
training step: 40875, total_loss: 0.026466134935617447
training step: 40876, total_loss: 0.13969209790229797
training step: 40877, total_loss: 1.3799254894256592
training step: 40878, total_loss: 4.094146728515625
training step: 40879, total_loss: 0.5356069803237915
training step: 40880, total_loss: 0.27559277415275574
training step: 40881, total_loss: 0.20512348413467407
training step: 40882, total_loss: 0.2796110510826111
training step: 40883, total_loss: 4.825435161590576
training step: 40884, total_loss: 1.2385510206222534
training step: 40885, total_loss: 1.80507493019104
training step: 40886, total_loss: 0.6076331734657288
training step: 40887, total_loss: 3.7650766372680664
training step: 40888, total_loss: 0.8739986419677734
training step: 40889, total_loss: 0.9143928289413452
training step: 40890, total_loss: 6.954044342041016
training step: 40891, total_loss: 2.5940723419189453
training step: 40892, total_loss: 0.22484172880649567
training step: 40893, total_loss: 2.4620604515075684
training step: 40894, total_loss: 0.8665030002593994
training step: 40895, total_loss: 0.009376227855682373
training step: 40896, total_loss: 0.31173089146614075
training step: 40897, total_loss: 0.06711344420909882
training step: 40898, total_loss: 2.6495425701141357
training step: 40899, total_loss: 0.9363343119621277
training step: 40900, total_loss: 1.5134961605072021
training step: 40901, total_loss: 3.1422834396362305
training step: 40902, total_loss: 0.12384583801031113
training step: 40903, total_loss: 0.10374686866998672
training step: 40904, total_loss: 3.662308692932129
training step: 40905, total_loss: 2.5818910598754883
training step: 40906, total_loss: 2.2313456535339355
training step: 40907, total_loss: 1.1890252828598022
training step: 40908, total_loss: 0.009428578428924084
training step: 40909, total_loss: 1.8370122909545898
training step: 40910, total_loss: 1.0127339363098145
training step: 40911, total_loss: 3.0327558517456055
training step: 40912, total_loss: 2.9090824127197266
training step: 40913, total_loss: 0.5552220344543457
training step: 40914, total_loss: 2.7608704566955566
training step: 40915, total_loss: 2.31978702545166
training step: 40916, total_loss: 1.5202592611312866
training step: 40917, total_loss: 0.6263834238052368
training step: 40918, total_loss: 0.3646674156188965
training step: 40919, total_loss: 0.10931475460529327
training step: 40920, total_loss: 0.08549008518457413INFO:tensorflow:Writing predictions to: test_output/predictions_41000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_41000.json

training step: 40921, total_loss: 1.5804660320281982
training step: 40922, total_loss: 0.7180761098861694
training step: 40923, total_loss: 3.0655341148376465
training step: 40924, total_loss: 1.4185967445373535
training step: 40925, total_loss: 2.855586528778076
training step: 40926, total_loss: 1.5406713485717773
training step: 40927, total_loss: 0.08512142300605774
training step: 40928, total_loss: 2.5805978775024414
training step: 40929, total_loss: 0.21681717038154602
training step: 40930, total_loss: 5.8449320793151855
training step: 40931, total_loss: 1.6653262376785278
training step: 40932, total_loss: 0.22450855374336243
training step: 40933, total_loss: 1.5440514087677002
training step: 40934, total_loss: 1.7149639129638672
training step: 40935, total_loss: 1.2281992435455322
training step: 40936, total_loss: 0.3688245117664337
training step: 40937, total_loss: 2.8376917839050293
training step: 40938, total_loss: 0.4620286524295807
training step: 40939, total_loss: 1.9738709926605225
training step: 40940, total_loss: 1.4995653629302979
training step: 40941, total_loss: 0.0190744549036026
training step: 40942, total_loss: 0.6646502017974854
training step: 40943, total_loss: 0.9503960013389587
training step: 40944, total_loss: 1.818588376045227
training step: 40945, total_loss: 2.182868480682373
training step: 40946, total_loss: 1.2755825519561768
training step: 40947, total_loss: 0.027109673246741295
training step: 40948, total_loss: 3.513277053833008
training step: 40949, total_loss: 2.555927276611328
training step: 40950, total_loss: 0.35061660408973694
training step: 40951, total_loss: 0.0008670943789184093
training step: 40952, total_loss: 0.0019957744516432285
training step: 40953, total_loss: 0.48482781648635864
training step: 40954, total_loss: 0.4058969020843506
training step: 40955, total_loss: 0.48085689544677734
training step: 40956, total_loss: 3.248170852661133
training step: 40957, total_loss: 0.002495372435078025
training step: 40958, total_loss: 0.5978503227233887
training step: 40959, total_loss: 2.723517894744873
training step: 40960, total_loss: 0.021513937041163445
training step: 40961, total_loss: 1.4897658824920654
training step: 40962, total_loss: 1.530606746673584
training step: 40963, total_loss: 1.0684263706207275
training step: 40964, total_loss: 0.24418902397155762
training step: 40965, total_loss: 1.8949620723724365
training step: 40966, total_loss: 0.07432451844215393
training step: 40967, total_loss: 0.23150946199893951
training step: 40968, total_loss: 3.8009660243988037
training step: 40969, total_loss: 1.7225821018218994
training step: 40970, total_loss: 0.9588541388511658
training step: 40971, total_loss: 1.2064322233200073
training step: 40972, total_loss: 0.5830543637275696
training step: 40973, total_loss: 3.331260919570923
training step: 40974, total_loss: 2.897284984588623
training step: 40975, total_loss: 3.006295680999756
training step: 40976, total_loss: 0.9943259358406067
training step: 40977, total_loss: 0.7851341962814331
training step: 40978, total_loss: 7.921993255615234
training step: 40979, total_loss: 1.5656927824020386
training step: 40980, total_loss: 0.3731361925601959
training step: 40981, total_loss: 2.0185799598693848
training step: 40982, total_loss: 5.063170433044434
training step: 40983, total_loss: 0.07080408930778503
training step: 40984, total_loss: 1.9725549221038818
training step: 40985, total_loss: 1.7138950824737549
training step: 40986, total_loss: 2.81354022026062
training step: 40987, total_loss: 0.00015335058560594916
training step: 40988, total_loss: 0.47998061776161194
training step: 40989, total_loss: 1.0441789627075195
training step: 40990, total_loss: 0.10964938998222351
training step: 40991, total_loss: 1.892808198928833
training step: 40992, total_loss: 1.3252437114715576
training step: 40993, total_loss: 4.115385055541992
training step: 40994, total_loss: 0.5477311015129089
training step: 40995, total_loss: 1.2640068531036377
training step: 40996, total_loss: 1.2201356887817383
training step: 40997, total_loss: 0.40522176027297974
training step: 40998, total_loss: 4.00131893157959
training step: 40999, total_loss: 0.16756615042686462
training step: 41000, total_loss: 2.5404915809631348
epoch finished! shuffle=False
evaluation: 5000, total_loss: 2.1019248962402344, f1: 51.9646200540205, followup: 28.632283584360263, yesno: 79.03544804503271, heq: 45.80861098432983, dheq: 2.2

Model saved in path test_output//model_41000.ckpt
training step: 41001, total_loss: 3.0923690795898438
training step: 41002, total_loss: 6.939394950866699
training step: 41003, total_loss: 0.9745121002197266
training step: 41004, total_loss: 1.894072413444519
training step: 41005, total_loss: 0.2574641704559326
training step: 41006, total_loss: 3.5151565074920654
training step: 41007, total_loss: 3.312913179397583
training step: 41008, total_loss: 2.6648049354553223
training step: 41009, total_loss: 1.0176191329956055
training step: 41010, total_loss: 2.657296895980835
training step: 41011, total_loss: 1.5907829999923706
training step: 41012, total_loss: 4.835163593292236
training step: 41013, total_loss: 7.769223213195801
training step: 41014, total_loss: 1.3216383457183838
training step: 41015, total_loss: 0.0008743535727262497
training step: 41016, total_loss: 3.739722728729248
training step: 41017, total_loss: 0.8995351791381836
training step: 41018, total_loss: 1.1373164653778076
training step: 41019, total_loss: 5.503684043884277
training step: 41020, total_loss: 3.543510913848877
training step: 41021, total_loss: 2.918672561645508
training step: 41022, total_loss: 0.696989119052887
training step: 41023, total_loss: 0.6519762277603149
training step: 41024, total_loss: 1.189218521118164
training step: 41025, total_loss: 0.6525793075561523
training step: 41026, total_loss: 3.1535491943359375
training step: 41027, total_loss: 0.6763492822647095
training step: 41028, total_loss: 1.7822973728179932
training step: 41029, total_loss: 0.30793851613998413
training step: 41030, total_loss: 2.2590060234069824
training step: 41031, total_loss: 2.3806142807006836
training step: 41032, total_loss: 1.0882008075714111
training step: 41033, total_loss: 0.14224234223365784
training step: 41034, total_loss: 0.5844539403915405
training step: 41035, total_loss: 3.1410574913024902
training step: 41036, total_loss: 1.507630705833435
training step: 41037, total_loss: 3.787107467651367
training step: 41038, total_loss: 1.648368239402771
training step: 41039, total_loss: 3.052818775177002
training step: 41040, total_loss: 3.3193106651306152
training step: 41041, total_loss: 0.40149766206741333
training step: 41042, total_loss: 1.6570385694503784
training step: 41043, total_loss: 1.2978359460830688
training step: 41044, total_loss: 1.414038062095642
training step: 41045, total_loss: 0.620414674282074
training step: 41046, total_loss: 1.4948232173919678
training step: 41047, total_loss: 2.7565791606903076
training step: 41048, total_loss: 2.018913745880127
training step: 41049, total_loss: 0.872908353805542
training step: 41050, total_loss: 5.07870626449585
training step: 41051, total_loss: 2.1176600456237793
training step: 41052, total_loss: 2.1892247200012207
training step: 41053, total_loss: 1.4861682653427124
training step: 41054, total_loss: 1.2836910486221313
training step: 41055, total_loss: 0.5125519037246704
training step: 41056, total_loss: 1.52986741065979
training step: 41057, total_loss: 1.5091803073883057
training step: 41058, total_loss: 0.6723300218582153
training step: 41059, total_loss: 2.407512903213501
training step: 41060, total_loss: 1.8067986965179443
training step: 41061, total_loss: 1.0584436655044556
training step: 41062, total_loss: 1.966033697128296
training step: 41063, total_loss: 0.29172784090042114
training step: 41064, total_loss: 2.2927069664001465
training step: 41065, total_loss: 1.8104326725006104
training step: 41066, total_loss: 4.277581214904785
training step: 41067, total_loss: 1.8199894428253174
training step: 41068, total_loss: 1.6639755964279175
training step: 41069, total_loss: 2.365105628967285
training step: 41070, total_loss: 3.0822596549987793
training step: 41071, total_loss: 0.7432523965835571
training step: 41072, total_loss: 0.9649621248245239
training step: 41073, total_loss: 1.7936688661575317
training step: 41074, total_loss: 1.7218016386032104
training step: 41075, total_loss: 1.760681390762329
training step: 41076, total_loss: 1.6054352521896362
training step: 41077, total_loss: 2.8957650661468506
training step: 41078, total_loss: 1.1744972467422485
training step: 41079, total_loss: 0.2710839807987213
training step: 41080, total_loss: 4.209550380706787
training step: 41081, total_loss: 2.1852216720581055
training step: 41082, total_loss: 3.7625043392181396
training step: 41083, total_loss: 0.7466592788696289
training step: 41084, total_loss: 1.6061866283416748
training step: 41085, total_loss: 3.8236420154571533
training step: 41086, total_loss: 2.212949752807617
training step: 41087, total_loss: 2.1675212383270264
training step: 41088, total_loss: 0.27501824498176575
training step: 41089, total_loss: 1.1055397987365723
training step: 41090, total_loss: 0.060829415917396545
training step: 41091, total_loss: 1.3111982345581055
training step: 41092, total_loss: 1.5087453126907349
training step: 41093, total_loss: 4.005122184753418
training step: 41094, total_loss: 3.0967929363250732
training step: 41095, total_loss: 2.6316490173339844
training step: 41096, total_loss: 2.0545670986175537
training step: 41097, total_loss: 2.7021892070770264
training step: 41098, total_loss: 4.168522357940674
training step: 41099, total_loss: 1.7407848834991455
training step: 41100, total_loss: 5.827690124511719
training step: 41101, total_loss: 0.8225944638252258
training step: 41102, total_loss: 2.681265354156494
training step: 41103, total_loss: 1.3927711248397827
training step: 41104, total_loss: 1.2627151012420654
training step: 41105, total_loss: 2.8784334659576416
training step: 41106, total_loss: 1.428893804550171
training step: 41107, total_loss: 1.379044532775879
training step: 41108, total_loss: 3.0563783645629883
training step: 41109, total_loss: 3.3534178733825684
training step: 41110, total_loss: 1.7017241716384888
training step: 41111, total_loss: 1.2518101930618286
training step: 41112, total_loss: 2.3884787559509277
training step: 41113, total_loss: 2.3198494911193848
training step: 41114, total_loss: 2.8185973167419434
training step: 41115, total_loss: 1.6678024530410767
training step: 41116, total_loss: 3.0583930015563965
training step: 41117, total_loss: 2.8901660442352295
training step: 41118, total_loss: 1.141935110092163
training step: 41119, total_loss: 2.4969189167022705
training step: 41120, total_loss: 1.0252807140350342
training step: 41121, total_loss: 2.4818360805511475
training step: 41122, total_loss: 0.7571748495101929
training step: 41123, total_loss: 3.284505605697632
training step: 41124, total_loss: 1.3892133235931396
training step: 41125, total_loss: 2.3488948345184326
training step: 41126, total_loss: 1.0486485958099365
training step: 41127, total_loss: 1.1847769021987915
training step: 41128, total_loss: 1.2404522895812988
training step: 41129, total_loss: 2.2976527214050293
training step: 41130, total_loss: 1.2353434562683105
training step: 41131, total_loss: 0.9966921806335449
training step: 41132, total_loss: 2.65803861618042
training step: 41133, total_loss: 1.4292043447494507
training step: 41134, total_loss: 2.2741546630859375
training step: 41135, total_loss: 2.5953049659729004
training step: 41136, total_loss: 4.652857303619385
training step: 41137, total_loss: 0.7080186605453491
training step: 41138, total_loss: 2.340738534927368
training step: 41139, total_loss: 3.3471813201904297
training step: 41140, total_loss: 0.7476580739021301
training step: 41141, total_loss: 2.5271143913269043
training step: 41142, total_loss: 0.30324870347976685
training step: 41143, total_loss: 1.9890793561935425
training step: 41144, total_loss: 0.8472987413406372
training step: 41145, total_loss: 0.6863992810249329
training step: 41146, total_loss: 1.7985117435455322
training step: 41147, total_loss: 1.0982294082641602
training step: 41148, total_loss: 1.3223216533660889
training step: 41149, total_loss: 2.8911333084106445
training step: 41150, total_loss: 1.9971551895141602
training step: 41151, total_loss: 1.5092201232910156
training step: 41152, total_loss: 2.2798871994018555
training step: 41153, total_loss: 0.6912903189659119
training step: 41154, total_loss: 4.3458991050720215
training step: 41155, total_loss: 3.9379472732543945
training step: 41156, total_loss: 2.5276293754577637
training step: 41157, total_loss: 1.566210150718689
training step: 41158, total_loss: 0.28168153762817383
training step: 41159, total_loss: 0.4954064190387726
training step: 41160, total_loss: 3.1570467948913574
training step: 41161, total_loss: 2.510117292404175
training step: 41162, total_loss: 0.8638018369674683
training step: 41163, total_loss: 2.0210204124450684
training step: 41164, total_loss: 0.8533509969711304
training step: 41165, total_loss: 3.0119881629943848
training step: 41166, total_loss: 1.1190588474273682
training step: 41167, total_loss: 1.0398120880126953
training step: 41168, total_loss: 1.2922210693359375
training step: 41169, total_loss: 2.492919921875
training step: 41170, total_loss: 1.6677128076553345
training step: 41171, total_loss: 4.19052791595459
training step: 41172, total_loss: 2.355189800262451
training step: 41173, total_loss: 2.5002377033233643
training step: 41174, total_loss: 6.128187656402588
training step: 41175, total_loss: 2.5739846229553223
training step: 41176, total_loss: 0.22928492724895477
training step: 41177, total_loss: 0.5993193984031677
training step: 41178, total_loss: 2.1960458755493164
training step: 41179, total_loss: 1.4513599872589111
training step: 41180, total_loss: 5.2973246574401855
training step: 41181, total_loss: 2.6537322998046875
training step: 41182, total_loss: 1.7514369487762451
training step: 41183, total_loss: 0.7321183681488037
training step: 41184, total_loss: 2.338068962097168
training step: 41185, total_loss: 1.1024600267410278
training step: 41186, total_loss: 2.653578042984009
training step: 41187, total_loss: 2.5092406272888184
training step: 41188, total_loss: 4.056916236877441
training step: 41189, total_loss: 0.8237019181251526
training step: 41190, total_loss: 1.769551157951355
training step: 41191, total_loss: 1.436349868774414
training step: 41192, total_loss: 1.0911798477172852
training step: 41193, total_loss: 0.16990657150745392
training step: 41194, total_loss: 1.4074680805206299
training step: 41195, total_loss: 0.9218032360076904
training step: 41196, total_loss: 0.8574889898300171
training step: 41197, total_loss: 2.4069764614105225
training step: 41198, total_loss: 2.767998456954956
training step: 41199, total_loss: 2.6974642276763916
training step: 41200, total_loss: 3.8231120109558105
training step: 41201, total_loss: 1.2115662097930908
training step: 41202, total_loss: 1.3212363719940186
training step: 41203, total_loss: 1.1780340671539307
training step: 41204, total_loss: 1.9748106002807617
training step: 41205, total_loss: 1.2022883892059326
training step: 41206, total_loss: 2.2127609252929688
training step: 41207, total_loss: 2.0296387672424316
training step: 41208, total_loss: 3.030028820037842
training step: 41209, total_loss: 0.9124364852905273
training step: 41210, total_loss: 4.117585182189941
training step: 41211, total_loss: 1.8447134494781494
training step: 41212, total_loss: 0.8317908048629761
training step: 41213, total_loss: 0.8185325860977173
training step: 41214, total_loss: 6.064910888671875
training step: 41215, total_loss: 0.7956745624542236
training step: 41216, total_loss: 3.0978879928588867
training step: 41217, total_loss: 0.9824633002281189
training step: 41218, total_loss: 0.5076804161071777
training step: 41219, total_loss: 0.28446340560913086
training step: 41220, total_loss: 0.3915380537509918
training step: 41221, total_loss: 0.9668907523155212
training step: 41222, total_loss: 1.9959607124328613
training step: 41223, total_loss: 1.6953155994415283
training step: 41224, total_loss: 2.949204921722412
training step: 41225, total_loss: 2.082406759262085
training step: 41226, total_loss: 1.8460665941238403
training step: 41227, total_loss: 0.7689569592475891
training step: 41228, total_loss: 3.504056453704834
training step: 41229, total_loss: 0.688890278339386
training step: 41230, total_loss: 1.215063452720642
training step: 41231, total_loss: 1.2636661529541016
training step: 41232, total_loss: 1.727830171585083
training step: 41233, total_loss: 1.2797679901123047
training step: 41234, total_loss: 0.8892430067062378
training step: 41235, total_loss: 5.556808948516846
training step: 41236, total_loss: 0.7414542436599731
training step: 41237, total_loss: 1.6769981384277344
training step: 41238, total_loss: 3.337397336959839
training step: 41239, total_loss: 2.8185229301452637
training step: 41240, total_loss: 1.8606998920440674
training step: 41241, total_loss: 1.198277473449707
training step: 41242, total_loss: 0.22026461362838745
training step: 41243, total_loss: 0.8829178214073181
training step: 41244, total_loss: 1.6036255359649658
training step: 41245, total_loss: 1.957397222518921
training step: 41246, total_loss: 1.8628475666046143
training step: 41247, total_loss: 0.7238761186599731
training step: 41248, total_loss: 2.742020606994629
training step: 41249, total_loss: 2.0346622467041016
training step: 41250, total_loss: 1.780423879623413
training step: 41251, total_loss: 1.843569278717041
training step: 41252, total_loss: 1.3788070678710938
training step: 41253, total_loss: 0.5394209027290344
training step: 41254, total_loss: 1.329063892364502
training step: 41255, total_loss: 2.282715320587158
training step: 41256, total_loss: 2.959211826324463
training step: 41257, total_loss: 0.5197629332542419
training step: 41258, total_loss: 1.806751012802124
training step: 41259, total_loss: 1.4950785636901855
training step: 41260, total_loss: 0.9549800157546997
training step: 41261, total_loss: 4.481568813323975
training step: 41262, total_loss: 2.4390511512756348
training step: 41263, total_loss: 0.3406374454498291
training step: 41264, total_loss: 0.31009069085121155
training step: 41265, total_loss: 0.8262463212013245
training step: 41266, total_loss: 2.7997236251831055
training step: 41267, total_loss: 4.610625267028809
training step: 41268, total_loss: 1.6401118040084839
training step: 41269, total_loss: 1.5764038562774658
training step: 41270, total_loss: 3.255592107772827
training step: 41271, total_loss: 0.027022987604141235
training step: 41272, total_loss: 2.0476832389831543
training step: 41273, total_loss: 0.515095591545105
training step: 41274, total_loss: 2.4762611389160156
training step: 41275, total_loss: 0.0003300284733995795
training step: 41276, total_loss: 2.3720757961273193
training step: 41277, total_loss: 2.507854461669922
training step: 41278, total_loss: 2.28450870513916
training step: 41279, total_loss: 0.12237636744976044
training step: 41280, total_loss: 0.6067323684692383
training step: 41281, total_loss: 3.212829828262329
training step: 41282, total_loss: 1.1229376792907715
training step: 41283, total_loss: 0.3921787738800049
training step: 41284, total_loss: 2.856131076812744
training step: 41285, total_loss: 2.604680061340332
training step: 41286, total_loss: 2.5823676586151123
training step: 41287, total_loss: 4.2336883544921875
training step: 41288, total_loss: 1.4298229217529297
training step: 41289, total_loss: 0.6088002920150757
training step: 41290, total_loss: 2.389281749725342
training step: 41291, total_loss: 2.3229119777679443
training step: 41292, total_loss: 1.9360231161117554
training step: 41293, total_loss: 2.145677089691162
training step: 41294, total_loss: 2.7916908264160156
training step: 41295, total_loss: 1.452960729598999
training step: 41296, total_loss: 2.690281391143799
training step: 41297, total_loss: 2.0649757385253906
training step: 41298, total_loss: 2.2575876712799072
training step: 41299, total_loss: 2.054966688156128
training step: 41300, total_loss: 0.4390924572944641
training step: 41301, total_loss: 1.8200807571411133
training step: 41302, total_loss: 1.8501057624816895
training step: 41303, total_loss: 0.9866301417350769
training step: 41304, total_loss: 2.7139477729797363
training step: 41305, total_loss: 3.343132495880127
training step: 41306, total_loss: 1.3909456729888916
training step: 41307, total_loss: 2.036425828933716
training step: 41308, total_loss: 4.933385848999023
training step: 41309, total_loss: 1.2590901851654053
training step: 41310, total_loss: 2.4145116806030273
training step: 41311, total_loss: 0.3639667332172394
training step: 41312, total_loss: 2.8516054153442383
training step: 41313, total_loss: 1.3267371654510498
training step: 41314, total_loss: 3.2798614501953125
training step: 41315, total_loss: 0.004195404704660177
training step: 41316, total_loss: 7.460508346557617
training step: 41317, total_loss: 2.9965763092041016
training step: 41318, total_loss: 2.5195488929748535
training step: 41319, total_loss: 1.3853141069412231
training step: 41320, total_loss: 1.5111427307128906
training step: 41321, total_loss: 2.644986629486084
training step: 41322, total_loss: 0.7157180309295654
training step: 41323, total_loss: 3.918107748031616
training step: 41324, total_loss: 3.3880505561828613
training step: 41325, total_loss: 0.5988541841506958
training step: 41326, total_loss: 1.305242657661438
training step: 41327, total_loss: 2.2761709690093994
training step: 41328, total_loss: 4.4949541091918945
training step: 41329, total_loss: 1.7766567468643188
training step: 41330, total_loss: 1.393096685409546
training step: 41331, total_loss: 1.1802531480789185
training step: 41332, total_loss: 1.564112663269043
training step: 41333, total_loss: 2.1970369815826416
training step: 41334, total_loss: 1.7197532653808594
training step: 41335, total_loss: 1.5056952238082886
training step: 41336, total_loss: 2.031611204147339
training step: 41337, total_loss: 1.1404380798339844
training step: 41338, total_loss: 1.5455632209777832
training step: 41339, total_loss: 1.4743928909301758
training step: 41340, total_loss: 1.8693006038665771
training step: 41341, total_loss: 0.8237928152084351
training step: 41342, total_loss: 2.528494119644165
training step: 41343, total_loss: 1.6660614013671875
training step: 41344, total_loss: 3.731937885284424
training step: 41345, total_loss: 2.16256046295166
training step: 41346, total_loss: 1.028095006942749
training step: 41347, total_loss: 2.1598830223083496
training step: 41348, total_loss: 2.96626877784729
training step: 41349, total_loss: 1.1953463554382324
training step: 41350, total_loss: 0.7547882795333862
training step: 41351, total_loss: 0.05524317920207977
training step: 41352, total_loss: 1.3130829334259033
training step: 41353, total_loss: 1.1420589685440063
training step: 41354, total_loss: 2.8558053970336914
training step: 41355, total_loss: 2.386716842651367
training step: 41356, total_loss: 2.6842191219329834
training step: 41357, total_loss: 1.0605659484863281
training step: 41358, total_loss: 4.862098217010498
training step: 41359, total_loss: 0.5518035292625427
training step: 41360, total_loss: 1.0951433181762695
training step: 41361, total_loss: 1.2172505855560303
training step: 41362, total_loss: 0.895490288734436
training step: 41363, total_loss: 1.9964241981506348
training step: 41364, total_loss: 1.763398289680481
training step: 41365, total_loss: 0.5338886976242065
training step: 41366, total_loss: 0.6104717254638672
training step: 41367, total_loss: 1.3181051015853882
training step: 41368, total_loss: 3.1343138217926025
training step: 41369, total_loss: 3.29274845123291
training step: 41370, total_loss: 4.818148612976074
training step: 41371, total_loss: 2.3020853996276855
training step: 41372, total_loss: 0.5038506984710693
training step: 41373, total_loss: 2.2291922569274902
training step: 41374, total_loss: 3.3829116821289062
training step: 41375, total_loss: 0.04047650098800659
training step: 41376, total_loss: 0.2979179620742798
training step: 41377, total_loss: 3.6329212188720703
training step: 41378, total_loss: 2.2041845321655273
training step: 41379, total_loss: 3.0545103549957275
training step: 41380, total_loss: 2.029440402984619
training step: 41381, total_loss: 1.8861682415008545
training step: 41382, total_loss: 0.4766521453857422
training step: 41383, total_loss: 2.3024392127990723
training step: 41384, total_loss: 1.9909776449203491
training step: 41385, total_loss: 1.9100512266159058
training step: 41386, total_loss: 0.4912414252758026
training step: 41387, total_loss: 0.07408569753170013
training step: 41388, total_loss: 0.7439379692077637
training step: 41389, total_loss: 1.0332826375961304
training step: 41390, total_loss: 3.7548887729644775
training step: 41391, total_loss: 2.846652030944824
training step: 41392, total_loss: 0.9743695259094238
training step: 41393, total_loss: 2.825773239135742
training step: 41394, total_loss: 3.8766212463378906
training step: 41395, total_loss: 1.989621639251709
training step: 41396, total_loss: 2.145146369934082
training step: 41397, total_loss: 0.9230952262878418
training step: 41398, total_loss: 0.5950508713722229
training step: 41399, total_loss: 3.1370015144348145
training step: 41400, total_loss: 3.901825189590454
training step: 41401, total_loss: 7.458456993103027
training step: 41402, total_loss: 0.38855791091918945
training step: 41403, total_loss: 2.10560941696167
training step: 41404, total_loss: 1.411139726638794
training step: 41405, total_loss: 1.8790820837020874
training step: 41406, total_loss: 3.4174444675445557
training step: 41407, total_loss: 1.9352760314941406
training step: 41408, total_loss: 1.9079060554504395
training step: 41409, total_loss: 2.2422351837158203
training step: 41410, total_loss: 1.7008476257324219
training step: 41411, total_loss: 0.9593790769577026
training step: 41412, total_loss: 1.4819767475128174
training step: 41413, total_loss: 1.2063076496124268
training step: 41414, total_loss: 0.3089899718761444
training step: 41415, total_loss: 1.330963134765625
training step: 41416, total_loss: 1.2754515409469604
training step: 41417, total_loss: 2.022397756576538
training step: 41418, total_loss: 3.6161210536956787
training step: 41419, total_loss: 1.143926739692688
training step: 41420, total_loss: 1.961256504058838
training step: 41421, total_loss: 1.4796112775802612
training step: 41422, total_loss: 2.666614055633545
training step: 41423, total_loss: 1.9949967861175537
training step: 41424, total_loss: 0.48022010922431946
training step: 41425, total_loss: 0.7509265542030334
training step: 41426, total_loss: 2.834266424179077
training step: 41427, total_loss: 1.1758067607879639
training step: 41428, total_loss: 2.683124542236328
training step: 41429, total_loss: 3.7644059658050537
training step: 41430, total_loss: 1.4589502811431885
training step: 41431, total_loss: 0.15002413094043732
training step: 41432, total_loss: 1.113791584968567
training step: 41433, total_loss: 1.431993007659912
training step: 41434, total_loss: 1.4422473907470703
training step: 41435, total_loss: 4.278893947601318
training step: 41436, total_loss: 3.753330945968628
training step: 41437, total_loss: 0.7014551162719727
training step: 41438, total_loss: 1.6674634218215942
training step: 41439, total_loss: 2.016852855682373
training step: 41440, total_loss: 1.725050926208496
training step: 41441, total_loss: 0.2956932783126831
training step: 41442, total_loss: 0.5470328330993652
training step: 41443, total_loss: 1.7476201057434082
training step: 41444, total_loss: 0.76436847448349
training step: 41445, total_loss: 3.7108592987060547
training step: 41446, total_loss: 1.5651366710662842
training step: 41447, total_loss: 1.9078285694122314
training step: 41448, total_loss: 2.67325758934021
training step: 41449, total_loss: 0.9254060983657837
training step: 41450, total_loss: 2.5035452842712402
training step: 41451, total_loss: 2.3691091537475586
training step: 41452, total_loss: 3.45334529876709
training step: 41453, total_loss: 1.0125620365142822
training step: 41454, total_loss: 3.4769058227539062
training step: 41455, total_loss: 2.87365984916687
training step: 41456, total_loss: 4.254771709442139
training step: 41457, total_loss: 2.458238363265991
training step: 41458, total_loss: 3.015860080718994
training step: 41459, total_loss: 3.8197665214538574
training step: 41460, total_loss: 0.16188952326774597
training step: 41461, total_loss: 4.612743377685547
training step: 41462, total_loss: 1.7959671020507812
training step: 41463, total_loss: 5.851682662963867
training step: 41464, total_loss: 4.915401935577393
training step: 41465, total_loss: 2.5475993156433105
training step: 41466, total_loss: 0.8385785222053528
training step: 41467, total_loss: 3.089668035507202
training step: 41468, total_loss: 1.5932292938232422
training step: 41469, total_loss: 1.4592351913452148
training step: 41470, total_loss: 2.3164358139038086
training step: 41471, total_loss: 1.8913674354553223
training step: 41472, total_loss: 1.7808048725128174
training step: 41473, total_loss: 0.5328584313392639
training step: 41474, total_loss: 2.3916454315185547
training step: 41475, total_loss: 2.033566951751709
training step: 41476, total_loss: 3.37869930267334
training step: 41477, total_loss: 4.182293891906738
training step: 41478, total_loss: 2.3863987922668457
training step: 41479, total_loss: 2.844574213027954
training step: 41480, total_loss: 2.5931074619293213
training step: 41481, total_loss: 0.7576409578323364
training step: 41482, total_loss: 1.6301844120025635
training step: 41483, total_loss: 2.8748011589050293
training step: 41484, total_loss: 0.6486287713050842
training step: 41485, total_loss: 3.0094830989837646
training step: 41486, total_loss: 0.06030173599720001
training step: 41487, total_loss: 2.040773391723633
training step: 41488, total_loss: 3.35697078704834
training step: 41489, total_loss: 1.163184642791748
training step: 41490, total_loss: 1.9049783945083618
training step: 41491, total_loss: 3.769770622253418
training step: 41492, total_loss: 0.8335739374160767
training step: 41493, total_loss: 1.7535074949264526
training step: 41494, total_loss: 1.263411045074463
training step: 41495, total_loss: 0.9117043018341064
training step: 41496, total_loss: 2.285367965698242
training step: 41497, total_loss: 3.38325834274292
training step: 41498, total_loss: 0.6594291925430298
training step: 41499, total_loss: 2.1177029609680176
training step: 41500, total_loss: 0.9114444255828857
training step: 41501, total_loss: 0.2754201292991638
training step: 41502, total_loss: 1.841505527496338
training step: 41503, total_loss: 4.60518217086792
training step: 41504, total_loss: 1.8578565120697021
training step: 41505, total_loss: 2.2984163761138916
training step: 41506, total_loss: 1.8933820724487305
training step: 41507, total_loss: 1.0646252632141113
training step: 41508, total_loss: 1.3441879749298096
training step: 41509, total_loss: 2.297037363052368
training step: 41510, total_loss: 1.7772626876831055
training step: 41511, total_loss: 1.4823989868164062
training step: 41512, total_loss: 1.0290871858596802
training step: 41513, total_loss: 2.8437700271606445
training step: 41514, total_loss: 1.5507335662841797
training step: 41515, total_loss: 2.312354803085327
training step: 41516, total_loss: 2.180177688598633
training step: 41517, total_loss: 3.188558340072632
training step: 41518, total_loss: 2.9027161598205566
training step: 41519, total_loss: 2.004688262939453
training step: 41520, total_loss: 2.056330442428589
training step: 41521, total_loss: 3.2452633380889893
training step: 41522, total_loss: 1.2772109508514404
training step: 41523, total_loss: 2.1853854656219482
training step: 41524, total_loss: 1.2642579078674316
training step: 41525, total_loss: 1.2686351537704468
training step: 41526, total_loss: 1.0943562984466553
training step: 41527, total_loss: 2.382908582687378
training step: 41528, total_loss: 5.4575581550598145
training step: 41529, total_loss: 1.4433685541152954
training step: 41530, total_loss: 1.7465084791183472
training step: 41531, total_loss: 0.5678755640983582
training step: 41532, total_loss: 1.4793696403503418
training step: 41533, total_loss: 0.9723119735717773
training step: 41534, total_loss: 3.5564005374908447
training step: 41535, total_loss: 1.6886851787567139
training step: 41536, total_loss: 2.0566017627716064
training step: 41537, total_loss: 1.673520565032959
training step: 41538, total_loss: 1.999952793121338
training step: 41539, total_loss: 1.696410059928894
training step: 41540, total_loss: 0.28065571188926697
training step: 41541, total_loss: 2.1760036945343018
training step: 41542, total_loss: 0.9856340885162354
training step: 41543, total_loss: 5.4783935546875
training step: 41544, total_loss: 0.5755287408828735
training step: 41545, total_loss: 0.581466794013977
training step: 41546, total_loss: 2.8623692989349365
training step: 41547, total_loss: 0.006937960162758827
training step: 41548, total_loss: 2.487428903579712
training step: 41549, total_loss: 0.6414257884025574
training step: 41550, total_loss: 2.566746711730957
training step: 41551, total_loss: 3.4669318199157715
training step: 41552, total_loss: 3.2167303562164307
training step: 41553, total_loss: 5.33820915222168
training step: 41554, total_loss: 1.1176867485046387
training step: 41555, total_loss: 0.9047228097915649
training step: 41556, total_loss: 0.6162822842597961
training step: 41557, total_loss: 3.5951592922210693
training step: 41558, total_loss: 1.9664238691329956
training step: 41559, total_loss: 3.797646999359131
training step: 41560, total_loss: 3.9645180702209473
training step: 41561, total_loss: 1.4471768140792847
training step: 41562, total_loss: 0.9032495021820068
training step: 41563, total_loss: 3.0868964195251465
training step: 41564, total_loss: 1.3389352560043335
training step: 41565, total_loss: 2.0643067359924316
training step: 41566, total_loss: 1.169174075126648
training step: 41567, total_loss: 2.633967638015747
training step: 41568, total_loss: 2.9758243560791016
training step: 41569, total_loss: 1.2391492128372192
training step: 41570, total_loss: 2.641152858734131
training step: 41571, total_loss: 0.4070000648498535
training step: 41572, total_loss: 3.0514419078826904
training step: 41573, total_loss: 3.8015248775482178
training step: 41574, total_loss: 2.408923625946045
training step: 41575, total_loss: 1.37677001953125
training step: 41576, total_loss: 1.5507891178131104
training step: 41577, total_loss: 1.2794616222381592
training step: 41578, total_loss: 0.7505436539649963
training step: 41579, total_loss: 2.2385659217834473
training step: 41580, total_loss: 0.14962564408779144
training step: 41581, total_loss: 1.3580294847488403
training step: 41582, total_loss: 1.1407313346862793
training step: 41583, total_loss: 4.518054962158203
training step: 41584, total_loss: 0.10395628958940506
training step: 41585, total_loss: 3.1434240341186523
training step: 41586, total_loss: 2.383026123046875
training step: 41587, total_loss: 2.0421524047851562
training step: 41588, total_loss: 3.5674760341644287
training step: 41589, total_loss: 1.7896959781646729
training step: 41590, total_loss: 2.682279586791992
training step: 41591, total_loss: 2.277393341064453
training step: 41592, total_loss: 0.03343866765499115
training step: 41593, total_loss: 0.7639216184616089
training step: 41594, total_loss: 4.582464218139648
training step: 41595, total_loss: 1.720449686050415
training step: 41596, total_loss: 1.5223828554153442
training step: 41597, total_loss: 1.5821049213409424
training step: 41598, total_loss: 0.5986260175704956
training step: 41599, total_loss: 1.3436648845672607
training step: 41600, total_loss: 2.1563503742218018
training step: 41601, total_loss: 1.3711477518081665
training step: 41602, total_loss: 1.894094467163086
training step: 41603, total_loss: 4.237546920776367
training step: 41604, total_loss: 2.8055200576782227
training step: 41605, total_loss: 1.5472290515899658
training step: 41606, total_loss: 3.1723012924194336
training step: 41607, total_loss: 1.4797735214233398
training step: 41608, total_loss: 1.6611273288726807
training step: 41609, total_loss: 1.116349220275879
training step: 41610, total_loss: 3.228508949279785
training step: 41611, total_loss: 1.4378273487091064
training step: 41612, total_loss: 3.0063066482543945
training step: 41613, total_loss: 3.372594118118286
training step: 41614, total_loss: 1.8717776536941528
training step: 41615, total_loss: 3.72554087638855
training step: 41616, total_loss: 2.9576945304870605
training step: 41617, total_loss: 0.8241159915924072
training step: 41618, total_loss: 1.0699694156646729
training step: 41619, total_loss: 0.8617711067199707
training step: 41620, total_loss: 0.5450716018676758
training step: 41621, total_loss: 2.789876937866211
training step: 41622, total_loss: 1.1935255527496338
training step: 41623, total_loss: 0.8249959945678711
training step: 41624, total_loss: 1.307984471321106
training step: 41625, total_loss: 2.64150333404541
training step: 41626, total_loss: 3.7984304428100586
training step: 41627, total_loss: 1.8265535831451416
training step: 41628, total_loss: 2.866743564605713
training step: 41629, total_loss: 0.4838431477546692
training step: 41630, total_loss: 1.37690269947052
training step: 41631, total_loss: 2.1393635272979736
training step: 41632, total_loss: 2.3284571170806885
training step: 41633, total_loss: 3.9083900451660156
training step: 41634, total_loss: 1.9564825296401978
training step: 41635, total_loss: 0.8291998505592346
training step: 41636, total_loss: 0.92512047290802
training step: 41637, total_loss: 0.75639808177948
training step: 41638, total_loss: 0.8263041973114014
training step: 41639, total_loss: 0.48218095302581787
training step: 41640, total_loss: 3.132761240005493
training step: 41641, total_loss: 3.9165515899658203
training step: 41642, total_loss: 2.4609217643737793
training step: 41643, total_loss: 4.008213996887207
training step: 41644, total_loss: 1.4035229682922363
training step: 41645, total_loss: 1.5625572204589844
training step: 41646, total_loss: 2.929286003112793
training step: 41647, total_loss: 1.600921630859375
training step: 41648, total_loss: 1.939139485359192
training step: 41649, total_loss: 3.335308074951172
training step: 41650, total_loss: 1.4884132146835327
training step: 41651, total_loss: 2.838836193084717
training step: 41652, total_loss: 3.098978281021118
training step: 41653, total_loss: 1.4068293571472168
training step: 41654, total_loss: 0.1990564465522766
training step: 41655, total_loss: 1.5976123809814453
training step: 41656, total_loss: 3.087430477142334
training step: 41657, total_loss: 1.418910264968872
training step: 41658, total_loss: 0.215352863073349
training step: 41659, total_loss: 1.7567155361175537
training step: 41660, total_loss: 0.0024351137690246105
training step: 41661, total_loss: 2.135525703430176
training step: 41662, total_loss: 1.020056962966919
training step: 41663, total_loss: 0.00240731006488204
training step: 41664, total_loss: 0.41222912073135376
training step: 41665, total_loss: 0.42440348863601685
training step: 41666, total_loss: 2.507467031478882
training step: 41667, total_loss: 4.111360549926758
training step: 41668, total_loss: 2.5517730712890625
training step: 41669, total_loss: 1.160072684288025
training step: 41670, total_loss: 3.819459915161133
training step: 41671, total_loss: 1.288353681564331
training step: 41672, total_loss: 1.7768328189849854
training step: 41673, total_loss: 1.726609468460083
training step: 41674, total_loss: 3.001858711242676
training step: 41675, total_loss: 4.258825778961182
training step: 41676, total_loss: 1.1635706424713135
training step: 41677, total_loss: 2.4305384159088135
training step: 41678, total_loss: 2.745295524597168
training step: 41679, total_loss: 2.2796430587768555
training step: 41680, total_loss: 3.244595527648926
training step: 41681, total_loss: 1.7954994440078735
training step: 41682, total_loss: 0.6194355487823486
training step: 41683, total_loss: 0.20248395204544067
training step: 41684, total_loss: 1.8578317165374756
training step: 41685, total_loss: 1.1818573474884033
training step: 41686, total_loss: 1.6484646797180176
training step: 41687, total_loss: 3.688356876373291
training step: 41688, total_loss: 2.5390915870666504
training step: 41689, total_loss: 2.0905678272247314
training step: 41690, total_loss: 1.5271140336990356
training step: 41691, total_loss: 0.3077102303504944
training step: 41692, total_loss: 1.8877971172332764
training step: 41693, total_loss: 1.4276385307312012
training step: 41694, total_loss: 2.721121311187744
training step: 41695, total_loss: 1.452085256576538
training step: 41696, total_loss: 1.463265299797058
training step: 41697, total_loss: 0.22893968224525452
training step: 41698, total_loss: 0.7552481293678284
training step: 41699, total_loss: 1.6592086553573608
training step: 41700, total_loss: 3.9251809120178223
training step: 41701, total_loss: 1.5739405155181885
training step: 41702, total_loss: 1.1510405540466309
training step: 41703, total_loss: 1.3618725538253784
training step: 41704, total_loss: 4.02139949798584
training step: 41705, total_loss: 0.19012759625911713
training step: 41706, total_loss: 4.786272048950195
training step: 41707, total_loss: 0.7307018041610718
training step: 41708, total_loss: 0.45049959421157837
training step: 41709, total_loss: 0.41218429803848267
training step: 41710, total_loss: 2.4243927001953125
training step: 41711, total_loss: 1.1812889575958252
training step: 41712, total_loss: 2.689131498336792
training step: 41713, total_loss: 3.070192337036133
training step: 41714, total_loss: 4.228461742401123
training step: 41715, total_loss: 3.1737570762634277
training step: 41716, total_loss: 1.4978537559509277
training step: 41717, total_loss: 1.6930608749389648
training step: 41718, total_loss: 3.0204784870147705
training step: 41719, total_loss: 0.42387866973876953
training step: 41720, total_loss: 3.145724058151245
training step: 41721, total_loss: 2.3067429065704346
training step: 41722, total_loss: 1.6372489929199219
training step: 41723, total_loss: 3.88808536529541
training step: 41724, total_loss: 1.7582131624221802
training step: 41725, total_loss: 3.845757007598877
training step: 41726, total_loss: 3.047549247741699
training step: 41727, total_loss: 0.007424390874803066
training step: 41728, total_loss: 2.2004733085632324
training step: 41729, total_loss: 1.5335769653320312
training step: 41730, total_loss: 1.0100804567337036
training step: 41731, total_loss: 0.3544256091117859
training step: 41732, total_loss: 2.345991849899292
training step: 41733, total_loss: 0.8204684257507324
training step: 41734, total_loss: 1.1212100982666016
training step: 41735, total_loss: 2.6202898025512695
training step: 41736, total_loss: 4.252140045166016
training step: 41737, total_loss: 1.9008855819702148
training step: 41738, total_loss: 1.9061264991760254
training step: 41739, total_loss: 0.0018920699367299676
training step: 41740, total_loss: 0.04391079023480415
training step: 41741, total_loss: 3.151177406311035
training step: 41742, total_loss: 2.2773542404174805
training step: 41743, total_loss: 3.586700916290283
training step: 41744, total_loss: 2.5042214393615723
training step: 41745, total_loss: 4.40595817565918
training step: 41746, total_loss: 2.1818110942840576
training step: 41747, total_loss: 0.824999988079071
training step: 41748, total_loss: 1.1475679874420166
training step: 41749, total_loss: 1.7546719312667847
training step: 41750, total_loss: 2.635655403137207
training step: 41751, total_loss: 1.7556426525115967
training step: 41752, total_loss: 0.8817497491836548
training step: 41753, total_loss: 2.1931893825531006
training step: 41754, total_loss: 2.8138208389282227
training step: 41755, total_loss: 3.0754401683807373
training step: 41756, total_loss: 1.3394731283187866
training step: 41757, total_loss: 1.6894564628601074
training step: 41758, total_loss: 3.027662515640259
training step: 41759, total_loss: 0.004991126712411642
training step: 41760, total_loss: 2.4469432830810547
training step: 41761, total_loss: 1.8062949180603027
training step: 41762, total_loss: 3.345343828201294
training step: 41763, total_loss: 2.491910934448242
training step: 41764, total_loss: 3.3888044357299805
training step: 41765, total_loss: 3.169830560684204
training step: 41766, total_loss: 1.0552608966827393
training step: 41767, total_loss: 2.925257682800293
training step: 41768, total_loss: 3.9034082889556885
training step: 41769, total_loss: 1.7735610008239746
training step: 41770, total_loss: 1.889005184173584
training step: 41771, total_loss: 3.5698490142822266
training step: 41772, total_loss: 1.9900727272033691
training step: 41773, total_loss: 3.2096612453460693
training step: 41774, total_loss: 0.1656811386346817
training step: 41775, total_loss: 1.49557363986969
training step: 41776, total_loss: 1.8003820180892944
training step: 41777, total_loss: 2.3785653114318848
training step: 41778, total_loss: 0.6149621605873108
training step: 41779, total_loss: 0.4063884913921356
training step: 41780, total_loss: 1.0619275569915771
training step: 41781, total_loss: 3.2732186317443848
training step: 41782, total_loss: 2.1824402809143066
training step: 41783, total_loss: 1.6678352355957031
training step: 41784, total_loss: 1.0881361961364746
training step: 41785, total_loss: 2.1452996730804443
training step: 41786, total_loss: 3.7527337074279785
training step: 41787, total_loss: 2.2348568439483643
training step: 41788, total_loss: 2.404348134994507
training step: 41789, total_loss: 1.4801383018493652
training step: 41790, total_loss: 1.918863296508789
training step: 41791, total_loss: 1.3994311094284058
training step: 41792, total_loss: 2.289903163909912
training step: 41793, total_loss: 2.3637099266052246
training step: 41794, total_loss: 0.6945145130157471
training step: 41795, total_loss: 2.4859800338745117
training step: 41796, total_loss: 1.7574281692504883
training step: 41797, total_loss: 4.715991497039795
training step: 41798, total_loss: 0.06882697343826294
training step: 41799, total_loss: 3.277489185333252
training step: 41800, total_loss: 1.6732275485992432
training step: 41801, total_loss: 2.839073657989502
training step: 41802, total_loss: 2.130917549133301
training step: 41803, total_loss: 3.297468662261963
training step: 41804, total_loss: 1.4408984184265137
training step: 41805, total_loss: 0.6108312606811523
training step: 41806, total_loss: 1.1445889472961426
training step: 41807, total_loss: 3.541311264038086
training step: 41808, total_loss: 0.014899525791406631
training step: 41809, total_loss: 2.6848199367523193
training step: 41810, total_loss: 2.9323859214782715
training step: 41811, total_loss: 2.0974621772766113
training step: 41812, total_loss: 0.1841977834701538
training step: 41813, total_loss: 1.203399896621704
training step: 41814, total_loss: 2.6314139366149902
training step: 41815, total_loss: 2.3825159072875977
training step: 41816, total_loss: 2.1337482929229736
training step: 41817, total_loss: 0.5904777646064758
training step: 41818, total_loss: 1.0737520456314087
training step: 41819, total_loss: 2.2034859657287598
training step: 41820, total_loss: 1.9206238985061646
training step: 41821, total_loss: 1.713320016860962
training step: 41822, total_loss: 1.2924773693084717
training step: 41823, total_loss: 1.9865269660949707
training step: 41824, total_loss: 1.6531918048858643
training step: 41825, total_loss: 2.173579692840576
training step: 41826, total_loss: 3.5562126636505127
training step: 41827, total_loss: 1.1484148502349854
training step: 41828, total_loss: 1.1957911252975464
training step: 41829, total_loss: 0.8899562358856201
training step: 41830, total_loss: 3.692019462585449
training step: 41831, total_loss: 0.6406035423278809
training step: 41832, total_loss: 1.2247040271759033
training step: 41833, total_loss: 1.8825783729553223
training step: 41834, total_loss: 0.7411713600158691
training step: 41835, total_loss: 0.41317009925842285
training step: 41836, total_loss: 2.92502498626709
training step: 41837, total_loss: 0.4424317181110382
training step: 41838, total_loss: 2.4061856269836426
training step: 41839, total_loss: 4.576779365539551
training step: 41840, total_loss: 1.577764868736267
training step: 41841, total_loss: 1.006317138671875
training step: 41842, total_loss: 3.768251895904541
training step: 41843, total_loss: 1.4453426599502563
training step: 41844, total_loss: 2.1779088973999023
training step: 41845, total_loss: 2.1059277057647705
training step: 41846, total_loss: 2.5907998085021973
training step: 41847, total_loss: 1.8115034103393555
training step: 41848, total_loss: 1.5027515888214111
training step: 41849, total_loss: 0.9199875593185425
training step: 41850, total_loss: 2.726630687713623
training step: 41851, total_loss: 3.199831008911133INFO:tensorflow:Writing predictions to: test_output/predictions_42000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_42000.json

training step: 41852, total_loss: 1.0684641599655151
training step: 41853, total_loss: 0.6360385417938232
training step: 41854, total_loss: 1.723650574684143
training step: 41855, total_loss: 3.3874053955078125
training step: 41856, total_loss: 1.206748127937317
training step: 41857, total_loss: 2.555744171142578
training step: 41858, total_loss: 1.2093753814697266
training step: 41859, total_loss: 3.1493732929229736
training step: 41860, total_loss: 3.7123160362243652
training step: 41861, total_loss: 0.7981420159339905
training step: 41862, total_loss: 2.6242125034332275
training step: 41863, total_loss: 1.9746943712234497
training step: 41864, total_loss: 1.1411972045898438
training step: 41865, total_loss: 0.011311179026961327
training step: 41866, total_loss: 1.5621514320373535
training step: 41867, total_loss: 3.934211254119873
training step: 41868, total_loss: 1.634964942932129
training step: 41869, total_loss: 0.25507235527038574
training step: 41870, total_loss: 2.5919508934020996
training step: 41871, total_loss: 2.2544121742248535
training step: 41872, total_loss: 3.7508764266967773
training step: 41873, total_loss: 1.273040533065796
training step: 41874, total_loss: 0.7903646230697632
training step: 41875, total_loss: 2.1571500301361084
training step: 41876, total_loss: 2.4325602054595947
training step: 41877, total_loss: 1.4907164573669434
training step: 41878, total_loss: 0.027843501418828964
training step: 41879, total_loss: 0.13494659960269928
training step: 41880, total_loss: 0.034098390489816666
training step: 41881, total_loss: 0.6821353435516357
training step: 41882, total_loss: 4.0014753341674805
training step: 41883, total_loss: 2.4274210929870605
training step: 41884, total_loss: 1.74576735496521
training step: 41885, total_loss: 1.842632532119751
training step: 41886, total_loss: 0.3187117278575897
training step: 41887, total_loss: 0.7394658923149109
training step: 41888, total_loss: 2.6468095779418945
training step: 41889, total_loss: 0.40589582920074463
training step: 41890, total_loss: 0.6061955690383911
training step: 41891, total_loss: 1.8439149856567383
training step: 41892, total_loss: 2.257841110229492
training step: 41893, total_loss: 0.80683833360672
training step: 41894, total_loss: 0.8086196780204773
training step: 41895, total_loss: 1.8059550523757935
training step: 41896, total_loss: 1.2042696475982666
training step: 41897, total_loss: 2.281867742538452
training step: 41898, total_loss: 0.0035474183969199657
training step: 41899, total_loss: 2.2425594329833984
training step: 41900, total_loss: 1.7182445526123047
training step: 41901, total_loss: 3.4910082817077637
training step: 41902, total_loss: 6.705451011657715
training step: 41903, total_loss: 0.507472038269043
training step: 41904, total_loss: 1.379124641418457
training step: 41905, total_loss: 0.8040093183517456
training step: 41906, total_loss: 2.371338367462158
training step: 41907, total_loss: 1.3923492431640625
training step: 41908, total_loss: 3.0252342224121094
training step: 41909, total_loss: 1.9419121742248535
training step: 41910, total_loss: 3.7032222747802734
training step: 41911, total_loss: 2.9573912620544434
training step: 41912, total_loss: 2.9202260971069336
training step: 41913, total_loss: 1.7113633155822754
training step: 41914, total_loss: 1.5828806161880493
training step: 41915, total_loss: 1.773775339126587
training step: 41916, total_loss: 5.946953296661377
training step: 41917, total_loss: 0.5880751013755798
training step: 41918, total_loss: 1.8836543560028076
training step: 41919, total_loss: 0.8103259205818176
training step: 41920, total_loss: 2.392596960067749
training step: 41921, total_loss: 2.640042543411255
training step: 41922, total_loss: 3.3849916458129883
training step: 41923, total_loss: 2.120988130569458
training step: 41924, total_loss: 3.083756446838379
training step: 41925, total_loss: 3.112226963043213
training step: 41926, total_loss: 1.8805670738220215
training step: 41927, total_loss: 4.033231735229492
training step: 41928, total_loss: 2.413450002670288
training step: 41929, total_loss: 2.3640096187591553
training step: 41930, total_loss: 2.0620791912078857
training step: 41931, total_loss: 3.272675037384033
training step: 41932, total_loss: 0.6774202585220337
training step: 41933, total_loss: 1.0543817281723022
training step: 41934, total_loss: 2.8138232231140137
training step: 41935, total_loss: 1.0369412899017334
training step: 41936, total_loss: 1.7244150638580322
training step: 41937, total_loss: 2.2961878776550293
training step: 41938, total_loss: 2.449625015258789
training step: 41939, total_loss: 1.5819789171218872
training step: 41940, total_loss: 1.7314651012420654
training step: 41941, total_loss: 2.3560280799865723
training step: 41942, total_loss: 0.7521654367446899
training step: 41943, total_loss: 0.9286491870880127
training step: 41944, total_loss: 1.2620691061019897
training step: 41945, total_loss: 1.7597829103469849
training step: 41946, total_loss: 2.4117586612701416
training step: 41947, total_loss: 2.2563579082489014
training step: 41948, total_loss: 2.528461217880249
training step: 41949, total_loss: 2.304961681365967
training step: 41950, total_loss: 1.648697018623352
training step: 41951, total_loss: 2.8113739490509033
training step: 41952, total_loss: 2.1152100563049316
training step: 41953, total_loss: 3.389019250869751
training step: 41954, total_loss: 2.876905679702759
training step: 41955, total_loss: 1.2160390615463257
training step: 41956, total_loss: 4.735193252563477
training step: 41957, total_loss: 0.9276330471038818
training step: 41958, total_loss: 3.153341770172119
training step: 41959, total_loss: 0.973922073841095
training step: 41960, total_loss: 2.1412062644958496
training step: 41961, total_loss: 2.8797659873962402
training step: 41962, total_loss: 1.3469526767730713
training step: 41963, total_loss: 0.5171882510185242
training step: 41964, total_loss: 1.8870906829833984
training step: 41965, total_loss: 1.1993334293365479
training step: 41966, total_loss: 2.6766512393951416
training step: 41967, total_loss: 2.135092258453369
training step: 41968, total_loss: 2.950697898864746
training step: 41969, total_loss: 2.559894323348999
training step: 41970, total_loss: 3.189274311065674
training step: 41971, total_loss: 1.9534876346588135
training step: 41972, total_loss: 0.9319109916687012
training step: 41973, total_loss: 2.229262351989746
training step: 41974, total_loss: 2.854294776916504
training step: 41975, total_loss: 2.1498546600341797
training step: 41976, total_loss: 1.394933819770813
training step: 41977, total_loss: 1.3980438709259033
training step: 41978, total_loss: 2.9128551483154297
training step: 41979, total_loss: 0.1734161227941513
training step: 41980, total_loss: 1.1415762901306152
training step: 41981, total_loss: 2.870755195617676
training step: 41982, total_loss: 1.866550087928772
training step: 41983, total_loss: 0.7554236054420471
training step: 41984, total_loss: 0.346561074256897
training step: 41985, total_loss: 1.7323791980743408
training step: 41986, total_loss: 3.411381721496582
training step: 41987, total_loss: 2.6732425689697266
training step: 41988, total_loss: 2.628779411315918
training step: 41989, total_loss: 2.412510871887207
training step: 41990, total_loss: 1.4343286752700806
training step: 41991, total_loss: 0.6470685005187988
training step: 41992, total_loss: 1.5464929342269897
training step: 41993, total_loss: 1.3607146739959717
training step: 41994, total_loss: 0.9196471571922302
training step: 41995, total_loss: 2.411851644515991
training step: 41996, total_loss: 3.4034976959228516
training step: 41997, total_loss: 2.1413092613220215
training step: 41998, total_loss: 2.2870497703552246
training step: 41999, total_loss: 1.379463791847229
training step: 42000, total_loss: 1.5398814678192139
epoch finished! shuffle=False
evaluation: 6000, total_loss: 1.7720845937728882, f1: 50.93400200444674, followup: 28.90613114255287, yesno: 79.00502053856687, heq: 45.504335919671384, dheq: 2.0

Model saved in path test_output//model_42000.ckpt
training step: 42001, total_loss: 1.6448216438293457
training step: 42002, total_loss: 0.6362901926040649
training step: 42003, total_loss: 1.8094875812530518
training step: 42004, total_loss: 2.138824701309204
training step: 42005, total_loss: 2.255457878112793
training step: 42006, total_loss: 0.5658814311027527
training step: 42007, total_loss: 2.3019444942474365
training step: 42008, total_loss: 3.135073184967041
training step: 42009, total_loss: 1.6547212600708008
training step: 42010, total_loss: 1.785848617553711
training step: 42011, total_loss: 5.204668045043945
training step: 42012, total_loss: 4.041247367858887
training step: 42013, total_loss: 1.0735747814178467
training step: 42014, total_loss: 7.712684631347656
training step: 42015, total_loss: 1.549957036972046
training step: 42016, total_loss: 4.1875
training step: 42017, total_loss: 1.5868098735809326
training step: 42018, total_loss: 4.1710710525512695
training step: 42019, total_loss: 6.507577896118164
training step: 42020, total_loss: 2.0219225883483887
training step: 42021, total_loss: 0.4078586995601654
training step: 42022, total_loss: 1.0562877655029297
training step: 42023, total_loss: 4.599837779998779
training step: 42024, total_loss: 2.7334818840026855
training step: 42025, total_loss: 3.7357068061828613
training step: 42026, total_loss: 0.29533886909484863
training step: 42027, total_loss: 2.2166829109191895
training step: 42028, total_loss: 1.3221077919006348
training step: 42029, total_loss: 2.8207144737243652
training step: 42030, total_loss: 0.38765770196914673
training step: 42031, total_loss: 2.474757194519043
training step: 42032, total_loss: 1.1598141193389893
training step: 42033, total_loss: 2.3343582153320312
training step: 42034, total_loss: 1.6264406442642212
training step: 42035, total_loss: 0.7513532042503357
training step: 42036, total_loss: 1.1426591873168945
training step: 42037, total_loss: 1.182978868484497
training step: 42038, total_loss: 0.36156249046325684
training step: 42039, total_loss: 5.120598793029785
training step: 42040, total_loss: 2.9355621337890625
training step: 42041, total_loss: 1.6337312459945679
training step: 42042, total_loss: 0.8444533348083496
training step: 42043, total_loss: 1.8545225858688354
training step: 42044, total_loss: 0.193869948387146
training step: 42045, total_loss: 1.0205235481262207
training step: 42046, total_loss: 1.2143054008483887
training step: 42047, total_loss: 1.9874529838562012
training step: 42048, total_loss: 1.9875760078430176
training step: 42049, total_loss: 0.009147616103291512
training step: 42050, total_loss: 0.9450414180755615
training step: 42051, total_loss: 1.0358390808105469
training step: 42052, total_loss: 0.9878702163696289
training step: 42053, total_loss: 1.8691141605377197
training step: 42054, total_loss: 1.8956360816955566
training step: 42055, total_loss: 0.16905488073825836
training step: 42056, total_loss: 0.6709437370300293
training step: 42057, total_loss: 1.6374759674072266
training step: 42058, total_loss: 0.8032574653625488
training step: 42059, total_loss: 1.5560691356658936
training step: 42060, total_loss: 0.042984090745449066
training step: 42061, total_loss: 3.1801514625549316
training step: 42062, total_loss: 4.1941094398498535
training step: 42063, total_loss: 0.6728568077087402
training step: 42064, total_loss: 4.563536643981934
training step: 42065, total_loss: 1.3130419254302979
training step: 42066, total_loss: 3.4927546977996826
training step: 42067, total_loss: 1.6887054443359375
training step: 42068, total_loss: 4.015341281890869
training step: 42069, total_loss: 0.12937651574611664
training step: 42070, total_loss: 0.8784842491149902
training step: 42071, total_loss: 4.870407581329346
training step: 42072, total_loss: 3.4703807830810547
training step: 42073, total_loss: 3.1916892528533936
training step: 42074, total_loss: 0.33266663551330566
training step: 42075, total_loss: 0.5641251802444458
training step: 42076, total_loss: 1.1678128242492676
training step: 42077, total_loss: 0.41405847668647766
training step: 42078, total_loss: 1.8035461902618408
training step: 42079, total_loss: 2.1820294857025146
training step: 42080, total_loss: 0.28545475006103516
training step: 42081, total_loss: 2.582399368286133
training step: 42082, total_loss: 5.099921226501465
training step: 42083, total_loss: 1.9158568382263184
training step: 42084, total_loss: 3.671015739440918
training step: 42085, total_loss: 2.5978407859802246
training step: 42086, total_loss: 3.104287624359131
training step: 42087, total_loss: 3.4605908393859863
training step: 42088, total_loss: 0.2376718521118164
training step: 42089, total_loss: 3.3601818084716797
training step: 42090, total_loss: 1.2923061847686768
training step: 42091, total_loss: 2.2553293704986572
training step: 42092, total_loss: 1.0533273220062256
training step: 42093, total_loss: 2.125486373901367
training step: 42094, total_loss: 1.2275879383087158
training step: 42095, total_loss: 3.3601067066192627
training step: 42096, total_loss: 0.9533068537712097
training step: 42097, total_loss: 1.0259888172149658
training step: 42098, total_loss: 0.20338431000709534
training step: 42099, total_loss: 1.0558527708053589
training step: 42100, total_loss: 1.7701122760772705
training step: 42101, total_loss: 3.0329179763793945
training step: 42102, total_loss: 0.8830772042274475
training step: 42103, total_loss: 4.298464775085449
training step: 42104, total_loss: 1.2075175046920776
training step: 42105, total_loss: 3.4354496002197266
training step: 42106, total_loss: 0.002528577111661434
training step: 42107, total_loss: 0.02700071781873703
training step: 42108, total_loss: 1.0889005661010742
training step: 42109, total_loss: 3.3706116676330566
training step: 42110, total_loss: 1.5893752574920654
training step: 42111, total_loss: 2.5608372688293457
training step: 42112, total_loss: 2.1324923038482666
training step: 42113, total_loss: 0.16777920722961426
training step: 42114, total_loss: 1.0190502405166626
training step: 42115, total_loss: 4.368025779724121
training step: 42116, total_loss: 4.286725044250488
training step: 42117, total_loss: 1.5192759037017822
training step: 42118, total_loss: 1.86391019821167
training step: 42119, total_loss: 0.33962482213974
training step: 42120, total_loss: 0.21984794735908508
training step: 42121, total_loss: 0.8256745934486389
training step: 42122, total_loss: 2.459045171737671
training step: 42123, total_loss: 2.6682419776916504
training step: 42124, total_loss: 3.3389153480529785
training step: 42125, total_loss: 2.948546886444092
training step: 42126, total_loss: 1.935241460800171
training step: 42127, total_loss: 0.24994277954101562
training step: 42128, total_loss: 3.642321825027466
training step: 42129, total_loss: 2.6768860816955566
training step: 42130, total_loss: 0.28709495067596436
training step: 42131, total_loss: 4.388100624084473
training step: 42132, total_loss: 2.6881494522094727
training step: 42133, total_loss: 2.6029443740844727
training step: 42134, total_loss: 2.8882060050964355
training step: 42135, total_loss: 3.879425287246704
training step: 42136, total_loss: 2.9134902954101562
training step: 42137, total_loss: 3.45058012008667
training step: 42138, total_loss: 2.82324481010437
training step: 42139, total_loss: 3.4625515937805176
training step: 42140, total_loss: 0.9500859379768372
training step: 42141, total_loss: 1.4720594882965088
training step: 42142, total_loss: 3.018134117126465
training step: 42143, total_loss: 3.102853536605835
training step: 42144, total_loss: 1.5364654064178467
training step: 42145, total_loss: 0.27999603748321533
training step: 42146, total_loss: 3.037769079208374
training step: 42147, total_loss: 1.9484784603118896
training step: 42148, total_loss: 1.716447353363037
training step: 42149, total_loss: 1.451901912689209
training step: 42150, total_loss: 2.734060049057007
training step: 42151, total_loss: 0.856838047504425
training step: 42152, total_loss: 1.1244151592254639
training step: 42153, total_loss: 1.8432902097702026
training step: 42154, total_loss: 1.0107635259628296
training step: 42155, total_loss: 1.6562861204147339
training step: 42156, total_loss: 1.198329210281372
training step: 42157, total_loss: 0.7628376483917236
training step: 42158, total_loss: 3.0620298385620117
training step: 42159, total_loss: 2.772287130355835
training step: 42160, total_loss: 2.2415246963500977
training step: 42161, total_loss: 1.9902715682983398
training step: 42162, total_loss: 0.8794261813163757
training step: 42163, total_loss: 1.1880916357040405
training step: 42164, total_loss: 1.6174463033676147
training step: 42165, total_loss: 1.9047305583953857
training step: 42166, total_loss: 0.5133023262023926
training step: 42167, total_loss: 3.0528106689453125
training step: 42168, total_loss: 0.23967722058296204
training step: 42169, total_loss: 1.7064309120178223
training step: 42170, total_loss: 1.8430099487304688
training step: 42171, total_loss: 1.4015438556671143
training step: 42172, total_loss: 2.605441093444824
training step: 42173, total_loss: 0.3291318416595459
training step: 42174, total_loss: 1.8853819370269775
training step: 42175, total_loss: 3.5385208129882812
training step: 42176, total_loss: 2.5200371742248535
training step: 42177, total_loss: 2.154557228088379
training step: 42178, total_loss: 1.0036202669143677
training step: 42179, total_loss: 0.5755379796028137
training step: 42180, total_loss: 2.493643045425415
training step: 42181, total_loss: 2.591989517211914
training step: 42182, total_loss: 2.1218080520629883
training step: 42183, total_loss: 0.033771589398384094
training step: 42184, total_loss: 1.3800196647644043
training step: 42185, total_loss: 0.20946574211120605
training step: 42186, total_loss: 0.18559913337230682
training step: 42187, total_loss: 0.014342385344207287
training step: 42188, total_loss: 1.1030173301696777
training step: 42189, total_loss: 1.2587664127349854
training step: 42190, total_loss: 0.9868885278701782
training step: 42191, total_loss: 3.703253746032715
training step: 42192, total_loss: 1.282278060913086
training step: 42193, total_loss: 1.3598514795303345
training step: 42194, total_loss: 2.384337902069092
training step: 42195, total_loss: 0.8111629486083984
training step: 42196, total_loss: 0.6561110019683838
training step: 42197, total_loss: 0.5234606266021729
training step: 42198, total_loss: 0.19467031955718994
training step: 42199, total_loss: 0.24687382578849792
training step: 42200, total_loss: 2.9039130210876465
training step: 42201, total_loss: 2.3298611640930176
training step: 42202, total_loss: 2.940034866333008
training step: 42203, total_loss: 1.3539074659347534
training step: 42204, total_loss: 1.2432525157928467
training step: 42205, total_loss: 3.412317991256714
training step: 42206, total_loss: 5.223074436187744
training step: 42207, total_loss: 1.5054640769958496
training step: 42208, total_loss: 0.4045718312263489
training step: 42209, total_loss: 2.7893190383911133
training step: 42210, total_loss: 3.1418118476867676
training step: 42211, total_loss: 3.5964407920837402
training step: 42212, total_loss: 5.991011619567871
training step: 42213, total_loss: 0.8685296773910522
training step: 42214, total_loss: 2.5961527824401855
training step: 42215, total_loss: 1.7562419176101685
training step: 42216, total_loss: 0.10376860201358795
training step: 42217, total_loss: 1.3296958208084106
training step: 42218, total_loss: 2.4498767852783203
training step: 42219, total_loss: 3.7601375579833984
training step: 42220, total_loss: 0.0717499777674675
training step: 42221, total_loss: 2.6310696601867676
training step: 42222, total_loss: 1.1882086992263794
training step: 42223, total_loss: 0.808283805847168
training step: 42224, total_loss: 1.0545909404754639
training step: 42225, total_loss: 2.0415871143341064
training step: 42226, total_loss: 2.5282139778137207
training step: 42227, total_loss: 2.159376859664917
training step: 42228, total_loss: 2.1297450065612793
training step: 42229, total_loss: 0.07267288118600845
training step: 42230, total_loss: 0.052909426391124725
training step: 42231, total_loss: 1.1366896629333496
training step: 42232, total_loss: 0.8195937871932983
training step: 42233, total_loss: 3.4767634868621826
training step: 42234, total_loss: 3.887208938598633
training step: 42235, total_loss: 0.7901662588119507
training step: 42236, total_loss: 0.42194777727127075
training step: 42237, total_loss: 1.9507800340652466
training step: 42238, total_loss: 1.7145252227783203
training step: 42239, total_loss: 0.361725389957428
training step: 42240, total_loss: 0.6752598285675049
training step: 42241, total_loss: 2.4848382472991943
training step: 42242, total_loss: 1.3298512697219849
training step: 42243, total_loss: 2.843555450439453
training step: 42244, total_loss: 2.16336727142334
training step: 42245, total_loss: 0.8842508792877197
training step: 42246, total_loss: 0.8481900691986084
training step: 42247, total_loss: 1.0023490190505981
training step: 42248, total_loss: 0.9616307020187378
training step: 42249, total_loss: 0.4045126736164093
training step: 42250, total_loss: 5.006519317626953
training step: 42251, total_loss: 3.8764026165008545
training step: 42252, total_loss: 1.3107082843780518
training step: 42253, total_loss: 1.9653650522232056
training step: 42254, total_loss: 2.004561185836792
training step: 42255, total_loss: 0.5885312557220459
training step: 42256, total_loss: 2.484457492828369
training step: 42257, total_loss: 0.015058324672281742
training step: 42258, total_loss: 3.0323476791381836
training step: 42259, total_loss: 0.10616743564605713
training step: 42260, total_loss: 1.1020011901855469
training step: 42261, total_loss: 2.3760762214660645
training step: 42262, total_loss: 5.023094177246094
training step: 42263, total_loss: 2.8077754974365234
training step: 42264, total_loss: 1.1682536602020264
training step: 42265, total_loss: 0.35322120785713196
training step: 42266, total_loss: 3.5511879920959473
training step: 42267, total_loss: 2.184509754180908
training step: 42268, total_loss: 0.5841832160949707
training step: 42269, total_loss: 1.6882648468017578
training step: 42270, total_loss: 0.27284711599349976
training step: 42271, total_loss: 1.8037430047988892
training step: 42272, total_loss: 3.1646575927734375
training step: 42273, total_loss: 0.8074038028717041
training step: 42274, total_loss: 2.3441522121429443
training step: 42275, total_loss: 1.2051455974578857
training step: 42276, total_loss: 2.995939254760742
training step: 42277, total_loss: 1.1076830625534058
training step: 42278, total_loss: 1.8098708391189575
training step: 42279, total_loss: 2.160623073577881
training step: 42280, total_loss: 2.613196849822998
training step: 42281, total_loss: 0.16187378764152527
training step: 42282, total_loss: 8.394153594970703
training step: 42283, total_loss: 0.7935778498649597
training step: 42284, total_loss: 1.9614818096160889
training step: 42285, total_loss: 2.7044475078582764
training step: 42286, total_loss: 1.8836289644241333
training step: 42287, total_loss: 1.094504475593567
training step: 42288, total_loss: 0.342032253742218
training step: 42289, total_loss: 1.4997570514678955
training step: 42290, total_loss: 1.6270313262939453
training step: 42291, total_loss: 4.071203231811523
training step: 42292, total_loss: 2.30832576751709
training step: 42293, total_loss: 3.267594814300537
training step: 42294, total_loss: 2.926962375640869
training step: 42295, total_loss: 3.3239378929138184
training step: 42296, total_loss: 1.3426328897476196
training step: 42297, total_loss: 0.33852553367614746
training step: 42298, total_loss: 0.5117568969726562
training step: 42299, total_loss: 2.065756320953369
training step: 42300, total_loss: 3.4968132972717285
training step: 42301, total_loss: 3.136578321456909
training step: 42302, total_loss: 3.0373966693878174
training step: 42303, total_loss: 3.0331544876098633
training step: 42304, total_loss: 1.6292798519134521
training step: 42305, total_loss: 1.7095816135406494
training step: 42306, total_loss: 2.824828863143921
training step: 42307, total_loss: 1.9860422611236572
training step: 42308, total_loss: 2.743129014968872
training step: 42309, total_loss: 1.6337995529174805
training step: 42310, total_loss: 1.6465859413146973
training step: 42311, total_loss: 1.611270785331726
training step: 42312, total_loss: 2.3745980262756348
training step: 42313, total_loss: 1.3844159841537476
training step: 42314, total_loss: 1.227790355682373
training step: 42315, total_loss: 2.117513418197632
training step: 42316, total_loss: 0.5271064043045044
training step: 42317, total_loss: 2.6837408542633057
training step: 42318, total_loss: 2.1998417377471924
training step: 42319, total_loss: 1.128676414489746
training step: 42320, total_loss: 3.0516817569732666
training step: 42321, total_loss: 2.093998908996582
training step: 42322, total_loss: 2.2009429931640625
training step: 42323, total_loss: 1.7602362632751465
training step: 42324, total_loss: 1.266883373260498
training step: 42325, total_loss: 3.200425624847412
training step: 42326, total_loss: 0.9251944422721863
training step: 42327, total_loss: 0.2528853118419647
training step: 42328, total_loss: 3.4050416946411133
training step: 42329, total_loss: 2.534806251525879
training step: 42330, total_loss: 0.6159626245498657
training step: 42331, total_loss: 1.6592581272125244
training step: 42332, total_loss: 1.6085563898086548
training step: 42333, total_loss: 1.9361531734466553
training step: 42334, total_loss: 1.819991946220398
training step: 42335, total_loss: 1.6656913757324219
training step: 42336, total_loss: 1.0488454103469849
training step: 42337, total_loss: 0.689757227897644
training step: 42338, total_loss: 2.392516613006592
training step: 42339, total_loss: 3.081228733062744
training step: 42340, total_loss: 2.344244956970215
training step: 42341, total_loss: 1.3984408378601074
training step: 42342, total_loss: 1.7532129287719727
training step: 42343, total_loss: 1.3728388547897339
training step: 42344, total_loss: 1.2176893949508667
training step: 42345, total_loss: 3.0342044830322266
training step: 42346, total_loss: 0.3357202410697937
training step: 42347, total_loss: 1.4624011516571045
training step: 42348, total_loss: 3.2687625885009766
training step: 42349, total_loss: 2.9736714363098145
training step: 42350, total_loss: 3.142104387283325
training step: 42351, total_loss: 3.425757646560669
training step: 42352, total_loss: 2.006021499633789
training step: 42353, total_loss: 4.747807502746582
training step: 42354, total_loss: 0.34623172879219055
training step: 42355, total_loss: 2.3481526374816895
training step: 42356, total_loss: 1.6569311618804932
training step: 42357, total_loss: 1.5673176050186157
training step: 42358, total_loss: 4.360451698303223
training step: 42359, total_loss: 1.758695363998413
training step: 42360, total_loss: 1.157030701637268
training step: 42361, total_loss: 0.6527340412139893
training step: 42362, total_loss: 1.6127958297729492
training step: 42363, total_loss: 1.03533935546875
training step: 42364, total_loss: 1.1969293355941772
training step: 42365, total_loss: 1.2311878204345703
training step: 42366, total_loss: 1.4324469566345215
training step: 42367, total_loss: 2.9296276569366455
training step: 42368, total_loss: 0.4279831349849701
training step: 42369, total_loss: 1.4843995571136475
training step: 42370, total_loss: 2.093682050704956
training step: 42371, total_loss: 0.32445836067199707
training step: 42372, total_loss: 1.8020141124725342
training step: 42373, total_loss: 1.656926155090332
training step: 42374, total_loss: 0.8876022100448608
training step: 42375, total_loss: 0.8415844440460205
training step: 42376, total_loss: 1.4403390884399414
training step: 42377, total_loss: 1.5086523294448853
training step: 42378, total_loss: 0.5013657808303833
training step: 42379, total_loss: 3.1703100204467773
training step: 42380, total_loss: 1.8920282125473022
training step: 42381, total_loss: 2.413315773010254
training step: 42382, total_loss: 5.283773899078369
training step: 42383, total_loss: 2.851410388946533
training step: 42384, total_loss: 0.580125093460083
training step: 42385, total_loss: 1.2982591390609741
training step: 42386, total_loss: 2.845409870147705
training step: 42387, total_loss: 0.3479105830192566
training step: 42388, total_loss: 0.39203205704689026
training step: 42389, total_loss: 0.026620572432875633
training step: 42390, total_loss: 2.1277196407318115
training step: 42391, total_loss: 1.769155740737915
training step: 42392, total_loss: 3.6136088371276855
training step: 42393, total_loss: 0.42806804180145264
training step: 42394, total_loss: 1.682250738143921
training step: 42395, total_loss: 3.3977794647216797
training step: 42396, total_loss: 0.3542075455188751
training step: 42397, total_loss: 0.03170958161354065
training step: 42398, total_loss: 1.5772355794906616
training step: 42399, total_loss: 1.2078633308410645
training step: 42400, total_loss: 1.4944994449615479
training step: 42401, total_loss: 3.5443344116210938
training step: 42402, total_loss: 7.449824333190918
training step: 42403, total_loss: 1.0651156902313232
training step: 42404, total_loss: 2.2902135848999023
training step: 42405, total_loss: 2.073734760284424
training step: 42406, total_loss: 0.13518893718719482
training step: 42407, total_loss: 1.3711209297180176
training step: 42408, total_loss: 1.0055947303771973
training step: 42409, total_loss: 0.23086588084697723
training step: 42410, total_loss: 0.9820519685745239
training step: 42411, total_loss: 3.9328994750976562
training step: 42412, total_loss: 2.318671226501465
training step: 42413, total_loss: 1.0561413764953613
training step: 42414, total_loss: 0.0575975626707077
training step: 42415, total_loss: 1.251319169998169
training step: 42416, total_loss: 2.248231887817383
training step: 42417, total_loss: 1.9956928491592407
training step: 42418, total_loss: 0.05212986096739769
training step: 42419, total_loss: 0.39178818464279175
training step: 42420, total_loss: 1.6051777601242065
training step: 42421, total_loss: 0.4176948666572571
training step: 42422, total_loss: 0.09487637132406235
training step: 42423, total_loss: 0.6444104313850403
training step: 42424, total_loss: 2.088329315185547
training step: 42425, total_loss: 0.5873533487319946
training step: 42426, total_loss: 0.7043755650520325
training step: 42427, total_loss: 1.2281339168548584
training step: 42428, total_loss: 1.1678425073623657
training step: 42429, total_loss: 2.2761034965515137
training step: 42430, total_loss: 2.6696205139160156
training step: 42431, total_loss: 0.016636136919260025
training step: 42432, total_loss: 2.0871095657348633
training step: 42433, total_loss: 1.2635666131973267
training step: 42434, total_loss: 4.146929740905762
training step: 42435, total_loss: 1.5454978942871094
training step: 42436, total_loss: 0.3361790180206299
training step: 42437, total_loss: 0.35233891010284424
training step: 42438, total_loss: 2.8542747497558594
training step: 42439, total_loss: 0.8283462524414062
training step: 42440, total_loss: 3.5280532836914062
training step: 42441, total_loss: 2.989151954650879
training step: 42442, total_loss: 0.6120117902755737
training step: 42443, total_loss: 1.608433485031128
training step: 42444, total_loss: 1.9633619785308838
training step: 42445, total_loss: 1.7723312377929688
training step: 42446, total_loss: 1.4504156112670898
training step: 42447, total_loss: 3.9565978050231934
training step: 42448, total_loss: 2.7123639583587646
training step: 42449, total_loss: 0.003278796561062336
training step: 42450, total_loss: 0.3178713321685791
training step: 42451, total_loss: 1.3738257884979248
training step: 42452, total_loss: 1.3483558893203735
training step: 42453, total_loss: 3.00840425491333
training step: 42454, total_loss: 1.2016512155532837
training step: 42455, total_loss: 1.3736412525177002
training step: 42456, total_loss: 3.6880760192871094
training step: 42457, total_loss: 2.8930559158325195
training step: 42458, total_loss: 3.4931225776672363
training step: 42459, total_loss: 1.0250511169433594
training step: 42460, total_loss: 1.681097388267517
training step: 42461, total_loss: 0.5604074001312256
training step: 42462, total_loss: 2.565861701965332
training step: 42463, total_loss: 0.585878849029541
training step: 42464, total_loss: 1.8158020973205566
training step: 42465, total_loss: 3.44468355178833
training step: 42466, total_loss: 1.039947748184204
training step: 42467, total_loss: 2.2884767055511475
training step: 42468, total_loss: 1.3723562955856323
training step: 42469, total_loss: 0.04081907123327255
training step: 42470, total_loss: 5.363685607910156
training step: 42471, total_loss: 1.9456064701080322
training step: 42472, total_loss: 2.5980002880096436
training step: 42473, total_loss: 1.0252004861831665
training step: 42474, total_loss: 2.1518452167510986
training step: 42475, total_loss: 2.5611050128936768
training step: 42476, total_loss: 1.556864857673645
training step: 42477, total_loss: 1.076035976409912
training step: 42478, total_loss: 2.871464252471924
training step: 42479, total_loss: 3.615720510482788
training step: 42480, total_loss: 1.9551564455032349
training step: 42481, total_loss: 0.69501131772995
training step: 42482, total_loss: 4.381070137023926
training step: 42483, total_loss: 2.8110191822052
training step: 42484, total_loss: 1.4011571407318115
training step: 42485, total_loss: 2.6312146186828613
training step: 42486, total_loss: 3.1937899589538574
training step: 42487, total_loss: 3.206509590148926
training step: 42488, total_loss: 2.4272687435150146
training step: 42489, total_loss: 0.9787739515304565
training step: 42490, total_loss: 1.2626093626022339
training step: 42491, total_loss: 5.369934558868408
training step: 42492, total_loss: 0.11440347135066986
training step: 42493, total_loss: 3.176088333129883
training step: 42494, total_loss: 1.287300944328308
training step: 42495, total_loss: 2.482077121734619
training step: 42496, total_loss: 0.2631368339061737
training step: 42497, total_loss: 0.23339742422103882
training step: 42498, total_loss: 2.5196681022644043
training step: 42499, total_loss: 0.7918655872344971
training step: 42500, total_loss: 2.9631099700927734
training step: 42501, total_loss: 3.46283221244812
training step: 42502, total_loss: 2.8334908485412598
training step: 42503, total_loss: 2.4170308113098145
training step: 42504, total_loss: 2.413367748260498
training step: 42505, total_loss: 0.92807537317276
training step: 42506, total_loss: 3.758462429046631
training step: 42507, total_loss: 0.6044588685035706
training step: 42508, total_loss: 3.970268726348877
training step: 42509, total_loss: 2.40274715423584
training step: 42510, total_loss: 2.1774446964263916
training step: 42511, total_loss: 3.7868494987487793
training step: 42512, total_loss: 2.898193836212158
training step: 42513, total_loss: 0.009927123785018921
training step: 42514, total_loss: 0.0010580173693597317
training step: 42515, total_loss: 0.8878151774406433
training step: 42516, total_loss: 2.940448760986328
training step: 42517, total_loss: 1.6745256185531616
training step: 42518, total_loss: 0.27799463272094727
training step: 42519, total_loss: 1.3483161926269531
training step: 42520, total_loss: 0.10874336957931519
training step: 42521, total_loss: 1.464876413345337
training step: 42522, total_loss: 3.312673568725586
training step: 42523, total_loss: 2.1319611072540283
training step: 42524, total_loss: 2.688286304473877
training step: 42525, total_loss: 1.10000741481781
training step: 42526, total_loss: 0.0023261546157300472
training step: 42527, total_loss: 4.0144572257995605
training step: 42528, total_loss: 0.8453231453895569
training step: 42529, total_loss: 4.622282981872559
training step: 42530, total_loss: 0.18291175365447998
training step: 42531, total_loss: 2.4347124099731445
training step: 42532, total_loss: 5.451009750366211
training step: 42533, total_loss: 2.1722514629364014
training step: 42534, total_loss: 0.4159424602985382
training step: 42535, total_loss: 2.1220123767852783
training step: 42536, total_loss: 1.0880964994430542
training step: 42537, total_loss: 1.7890172004699707
training step: 42538, total_loss: 1.8762290477752686
training step: 42539, total_loss: 0.020185653120279312
training step: 42540, total_loss: 2.599639415740967
training step: 42541, total_loss: 1.324601411819458
training step: 42542, total_loss: 3.6135759353637695
training step: 42543, total_loss: 1.3923640251159668
training step: 42544, total_loss: 2.4738471508026123
training step: 42545, total_loss: 1.7818689346313477
training step: 42546, total_loss: 3.5456080436706543
training step: 42547, total_loss: 2.1278984546661377
training step: 42548, total_loss: 1.3561594486236572
training step: 42549, total_loss: 1.635469675064087
training step: 42550, total_loss: 1.3716397285461426
training step: 42551, total_loss: 0.2159445881843567
training step: 42552, total_loss: 3.0896334648132324
training step: 42553, total_loss: 2.201245069503784
training step: 42554, total_loss: 1.5973602533340454
training step: 42555, total_loss: 1.1608505249023438
training step: 42556, total_loss: 0.13484641909599304
training step: 42557, total_loss: 3.030911922454834
training step: 42558, total_loss: 1.5758451223373413
training step: 42559, total_loss: 1.9986684322357178
training step: 42560, total_loss: 2.6491870880126953
training step: 42561, total_loss: 3.603060483932495
training step: 42562, total_loss: 1.9248625040054321
training step: 42563, total_loss: 0.9113370180130005
training step: 42564, total_loss: 1.9985542297363281
training step: 42565, total_loss: 3.4690098762512207
training step: 42566, total_loss: 1.3932403326034546
training step: 42567, total_loss: 3.1631484031677246
training step: 42568, total_loss: 1.0431008338928223
training step: 42569, total_loss: 2.883624315261841
training step: 42570, total_loss: 2.6285078525543213
training step: 42571, total_loss: 0.5740401148796082
training step: 42572, total_loss: 1.409895420074463
training step: 42573, total_loss: 0.9068935513496399
training step: 42574, total_loss: 3.024714231491089
training step: 42575, total_loss: 2.053664445877075
training step: 42576, total_loss: 1.773388147354126
training step: 42577, total_loss: 2.4743194580078125
training step: 42578, total_loss: 1.7872675657272339
training step: 42579, total_loss: 1.6009716987609863
training step: 42580, total_loss: 0.5271884202957153
training step: 42581, total_loss: 2.130525588989258
training step: 42582, total_loss: 2.844600200653076
training step: 42583, total_loss: 1.7919198274612427
training step: 42584, total_loss: 0.7747496366500854
training step: 42585, total_loss: 3.0091962814331055
training step: 42586, total_loss: 1.6682742834091187
training step: 42587, total_loss: 2.0094871520996094
training step: 42588, total_loss: 2.576794385910034
training step: 42589, total_loss: 0.5891431570053101
training step: 42590, total_loss: 2.090726852416992
training step: 42591, total_loss: 2.674539089202881
training step: 42592, total_loss: 0.1922469437122345
training step: 42593, total_loss: 1.6226954460144043
training step: 42594, total_loss: 1.7077924013137817
training step: 42595, total_loss: 0.8652730584144592
training step: 42596, total_loss: 0.24432748556137085
training step: 42597, total_loss: 2.0844197273254395
training step: 42598, total_loss: 2.170081853866577
training step: 42599, total_loss: 5.029737949371338
training step: 42600, total_loss: 1.7207516431808472
training step: 42601, total_loss: 0.781951904296875
training step: 42602, total_loss: 1.3641273975372314
training step: 42603, total_loss: 6.720921516418457
training step: 42604, total_loss: 3.4019415378570557
training step: 42605, total_loss: 1.2635490894317627
training step: 42606, total_loss: 2.1227691173553467
training step: 42607, total_loss: 3.0497608184814453
training step: 42608, total_loss: 1.5556268692016602
training step: 42609, total_loss: 2.5679972171783447
training step: 42610, total_loss: 2.6130306720733643
training step: 42611, total_loss: 3.307541608810425
training step: 42612, total_loss: 4.823293685913086
training step: 42613, total_loss: 4.02749490737915
training step: 42614, total_loss: 1.2655284404754639
training step: 42615, total_loss: 2.474869966506958
training step: 42616, total_loss: 0.9136922359466553
training step: 42617, total_loss: 1.1907811164855957
training step: 42618, total_loss: 3.2148022651672363
training step: 42619, total_loss: 2.2745749950408936
training step: 42620, total_loss: 0.8436800241470337
training step: 42621, total_loss: 2.7643978595733643
training step: 42622, total_loss: 0.5698920488357544
training step: 42623, total_loss: 2.5087618827819824
training step: 42624, total_loss: 0.6475090980529785
training step: 42625, total_loss: 3.8681178092956543
training step: 42626, total_loss: 2.211421012878418
training step: 42627, total_loss: 3.0560264587402344
training step: 42628, total_loss: 0.41549742221832275
training step: 42629, total_loss: 2.129304885864258
training step: 42630, total_loss: 1.0877039432525635
training step: 42631, total_loss: 1.246971607208252
training step: 42632, total_loss: 0.9274088740348816
training step: 42633, total_loss: 1.124066948890686
training step: 42634, total_loss: 0.5205565690994263
training step: 42635, total_loss: 0.8005256056785583
training step: 42636, total_loss: 0.5878875255584717
training step: 42637, total_loss: 1.1426447629928589
training step: 42638, total_loss: 1.7527501583099365
training step: 42639, total_loss: 3.1618990898132324
training step: 42640, total_loss: 0.7586860060691833
training step: 42641, total_loss: 0.833532452583313
training step: 42642, total_loss: 0.9197586178779602
training step: 42643, total_loss: 1.3270766735076904
training step: 42644, total_loss: 1.8365191221237183
training step: 42645, total_loss: 1.9639780521392822
training step: 42646, total_loss: 0.7587347030639648
training step: 42647, total_loss: 2.0238752365112305
training step: 42648, total_loss: 3.9458107948303223
training step: 42649, total_loss: 2.845432996749878
training step: 42650, total_loss: 1.0255454778671265
training step: 42651, total_loss: 4.250981330871582
training step: 42652, total_loss: 1.379646897315979
training step: 42653, total_loss: 2.8164968490600586
training step: 42654, total_loss: 0.5560799241065979
training step: 42655, total_loss: 0.8045468330383301
training step: 42656, total_loss: 0.7637301683425903
training step: 42657, total_loss: 2.68731689453125
training step: 42658, total_loss: 3.016115427017212
training step: 42659, total_loss: 0.9969043731689453
training step: 42660, total_loss: 1.4175840616226196
training step: 42661, total_loss: 2.9728879928588867
training step: 42662, total_loss: 1.9840049743652344
training step: 42663, total_loss: 1.5171475410461426
training step: 42664, total_loss: 3.5987279415130615
training step: 42665, total_loss: 2.6274070739746094
training step: 42666, total_loss: 0.2128036916255951
training step: 42667, total_loss: 3.5930352210998535
training step: 42668, total_loss: 1.5507780313491821
training step: 42669, total_loss: 1.5925250053405762
training step: 42670, total_loss: 2.1106581687927246
training step: 42671, total_loss: 1.0039807558059692
training step: 42672, total_loss: 0.09714336693286896
training step: 42673, total_loss: 0.5486477017402649
training step: 42674, total_loss: 2.2750163078308105
training step: 42675, total_loss: 0.4554111063480377
training step: 42676, total_loss: 0.6960445642471313
training step: 42677, total_loss: 1.1894910335540771
training step: 42678, total_loss: 1.7094677686691284
training step: 42679, total_loss: 1.949987530708313
training step: 42680, total_loss: 1.8026478290557861
training step: 42681, total_loss: 1.2152583599090576
training step: 42682, total_loss: 1.8876885175704956
training step: 42683, total_loss: 2.662325382232666
training step: 42684, total_loss: 0.2662655711174011
training step: 42685, total_loss: 3.0567727088928223
training step: 42686, total_loss: 2.3368654251098633
training step: 42687, total_loss: 1.799072027206421
training step: 42688, total_loss: 4.535336971282959
training step: 42689, total_loss: 4.104620933532715
training step: 42690, total_loss: 5.905463218688965
training step: 42691, total_loss: 5.6796135902404785
training step: 42692, total_loss: 4.323164939880371
training step: 42693, total_loss: 1.4900362491607666
training step: 42694, total_loss: 2.1918551921844482
training step: 42695, total_loss: 2.0484323501586914
training step: 42696, total_loss: 2.497743606567383
training step: 42697, total_loss: 5.148287773132324
training step: 42698, total_loss: 1.6715295314788818
training step: 42699, total_loss: 0.859653115272522
training step: 42700, total_loss: 1.654987096786499
training step: 42701, total_loss: 1.1101570129394531
training step: 42702, total_loss: 3.6917426586151123
training step: 42703, total_loss: 2.0850367546081543
training step: 42704, total_loss: 2.2312705516815186
training step: 42705, total_loss: 2.972097396850586
training step: 42706, total_loss: 0.5365685820579529
training step: 42707, total_loss: 1.4492816925048828
training step: 42708, total_loss: 1.862358808517456
training step: 42709, total_loss: 1.1764285564422607
training step: 42710, total_loss: 2.361114025115967
training step: 42711, total_loss: 1.4570822715759277
training step: 42712, total_loss: 3.406909942626953
training step: 42713, total_loss: 1.8490633964538574
training step: 42714, total_loss: 1.424368977546692
training step: 42715, total_loss: 0.9847312569618225
training step: 42716, total_loss: 0.11577808856964111
training step: 42717, total_loss: 0.8643724918365479
training step: 42718, total_loss: 7.495739936828613
training step: 42719, total_loss: 3.1782801151275635
training step: 42720, total_loss: 2.868574619293213
training step: 42721, total_loss: 1.6356124877929688
training step: 42722, total_loss: 1.515261173248291
training step: 42723, total_loss: 2.7561211585998535
training step: 42724, total_loss: 1.8182086944580078
training step: 42725, total_loss: 2.3329620361328125
training step: 42726, total_loss: 2.5775067806243896
training step: 42727, total_loss: 0.028250662609934807
training step: 42728, total_loss: 2.332179546356201
training step: 42729, total_loss: 1.4578725099563599
training step: 42730, total_loss: 1.0370092391967773
training step: 42731, total_loss: 1.4600859880447388
training step: 42732, total_loss: 3.385880947113037
training step: 42733, total_loss: 2.867563009262085
training step: 42734, total_loss: 0.8315525054931641
training step: 42735, total_loss: 4.6554059982299805
training step: 42736, total_loss: 2.364469051361084
training step: 42737, total_loss: 1.9833970069885254
training step: 42738, total_loss: 3.54421329498291
training step: 42739, total_loss: 0.3503223955631256
training step: 42740, total_loss: 0.5007174611091614
training step: 42741, total_loss: 2.5536930561065674
training step: 42742, total_loss: 3.281224250793457
training step: 42743, total_loss: 2.1481645107269287
training step: 42744, total_loss: 2.33144211769104
training step: 42745, total_loss: 1.7059977054595947
training step: 42746, total_loss: 1.1649572849273682
training step: 42747, total_loss: 1.0880554914474487
training step: 42748, total_loss: 1.6944862604141235
training step: 42749, total_loss: 0.43803635239601135
training step: 42750, total_loss: 3.142688751220703
training step: 42751, total_loss: 2.4019627571105957
training step: 42752, total_loss: 2.1459381580352783
training step: 42753, total_loss: 1.6339521408081055
training step: 42754, total_loss: 1.502726435661316
training step: 42755, total_loss: 4.237329959869385
training step: 42756, total_loss: 0.940185546875
training step: 42757, total_loss: 1.7832001447677612
training step: 42758, total_loss: 0.950401782989502
training step: 42759, total_loss: 0.6014474630355835
training step: 42760, total_loss: 1.7698447704315186
training step: 42761, total_loss: 3.353206157684326
training step: 42762, total_loss: 1.9006478786468506
training step: 42763, total_loss: 2.004690647125244
training step: 42764, total_loss: 1.6509337425231934
training step: 42765, total_loss: 2.390906810760498
training step: 42766, total_loss: 1.7129290103912354
training step: 42767, total_loss: 0.37744322419166565
training step: 42768, total_loss: 0.8444399833679199
training step: 42769, total_loss: 2.70705246925354
training step: 42770, total_loss: 2.2334187030792236
training step: 42771, total_loss: 0.6350407004356384
training step: 42772, total_loss: 2.2487614154815674
training step: 42773, total_loss: 0.3365946114063263
training step: 42774, total_loss: 1.525996208190918
training step: 42775, total_loss: 0.5377067923545837
training step: 42776, total_loss: 1.8462321758270264
training step: 42777, total_loss: 1.2677052021026611
training step: 42778, total_loss: 1.1304945945739746
training step: 42779, total_loss: 3.0015509128570557
training step: 42780, total_loss: 1.8065959215164185
training step: 42781, total_loss: 2.8267440795898438
training step: 42782, total_loss: 0.6130895018577576
training step: 42783, total_loss: 0.8721688985824585
training step: 42784, total_loss: 1.9139342308044434
training step: 42785, total_loss: 1.8233002424240112
training step: 42786, total_loss: 2.209918737411499
training step: 42787, total_loss: 0.7787479162216187
training step: 42788, total_loss: 4.427785873413086
training step: 42789, total_loss: 1.3621764183044434
training step: 42790, total_loss: 2.7962446212768555
training step: 42791, total_loss: 3.8243556022644043
training step: 42792, total_loss: 1.2174301147460938
training step: 42793, total_loss: 0.03764205053448677
training step: 42794, total_loss: 0.02880011312663555
training step: 42795, total_loss: 2.492581844329834
training step: 42796, total_loss: 1.4678974151611328
training step: 42797, total_loss: 0.23562154173851013
training step: 42798, total_loss: 0.6930013298988342
training step: 42799, total_loss: 1.1180487871170044
training step: 42800, total_loss: 0.7034114599227905
training step: 42801, total_loss: 1.7199409008026123
training step: 42802, total_loss: 1.5831942558288574
training step: 42803, total_loss: 2.2996726036071777
training step: 42804, total_loss: 1.617882490158081
training step: 42805, total_loss: 0.6117458939552307
training step: 42806, total_loss: 0.023474453017115593
training step: 42807, total_loss: 0.6206333637237549
training step: 42808, total_loss: 0.6161423325538635
training step: 42809, total_loss: 1.1997597217559814
training step: 42810, total_loss: 1.102905511856079
training step: 42811, total_loss: 1.012291431427002
training step: 42812, total_loss: 0.006328573450446129
training step: 42813, total_loss: 0.6843253374099731
training step: 42814, total_loss: 2.5994200706481934
training step: 42815, total_loss: 1.3842471837997437
training step: 42816, total_loss: 0.8059793710708618
training step: 42817, total_loss: 1.0284490585327148
training step: 42818, total_loss: 3.3031885623931885
training step: 42819, total_loss: 3.0063557624816895
training step: 42820, total_loss: 0.49732619524002075
training step: 42821, total_loss: 1.594440221786499
training step: 42822, total_loss: 1.4157943725585938
training step: 42823, total_loss: 2.1717841625213623
training step: 42824, total_loss: 3.7910492420196533
training step: 42825, total_loss: 2.9111976623535156
training step: 42826, total_loss: 2.1237640380859375
training step: 42827, total_loss: 0.8388225436210632
training step: 42828, total_loss: 3.2493677139282227
training step: 42829, total_loss: 2.6827826499938965
training step: 42830, total_loss: 3.014516592025757
training step: 42831, total_loss: 1.455353021621704
training step: 42832, total_loss: 2.3268346786499023
training step: 42833, total_loss: 1.1512335538864136
training step: 42834, total_loss: 2.9763429164886475
training step: 42835, total_loss: 1.3241304159164429
training step: 42836, total_loss: 4.329527854919434
training step: 42837, total_loss: 1.7394366264343262
training step: 42838, total_loss: 1.4130170345306396
training step: 42839, total_loss: 3.0533056259155273
training step: 42840, total_loss: 1.916167974472046
training step: 42841, total_loss: 2.420851230621338
training step: 42842, total_loss: 1.15352201461792
training step: 42843, total_loss: 1.6896086931228638
training step: 42844, total_loss: 1.4665504693984985
training step: 42845, total_loss: 2.1720142364501953
training step: 42846, total_loss: 2.9757533073425293
training step: 42847, total_loss: 5.516429901123047
training step: 42848, total_loss: 0.649394690990448
training step: 42849, total_loss: 2.960721492767334
training step: 42850, total_loss: 0.4516565501689911
training step: 42851, total_loss: 2.5905628204345703
training step: 42852, total_loss: 1.837275743484497
training step: 42853, total_loss: 2.043025016784668
training step: 42854, total_loss: 2.7987890243530273
training step: 42855, total_loss: 0.06369408220052719
training step: 42856, total_loss: 1.495470404624939
training step: 42857, total_loss: 1.916687250137329
training step: 42858, total_loss: 0.8494359254837036
training step: 42859, total_loss: 0.7943883538246155
training step: 42860, total_loss: 1.0885474681854248
training step: 42861, total_loss: 1.1608916521072388
training step: 42862, total_loss: 2.741741418838501
training step: 42863, total_loss: 2.78804349899292
training step: 42864, total_loss: 2.026620388031006
training step: 42865, total_loss: 3.1644949913024902
training step: 42866, total_loss: 4.201114654541016
training step: 42867, total_loss: 0.04082661494612694
training step: 42868, total_loss: 1.4948594570159912
training step: 42869, total_loss: 1.8706419467926025
training step: 42870, total_loss: 2.10148549079895
training step: 42871, total_loss: 1.7004644870758057
training step: 42872, total_loss: 2.4483845233917236
training step: 42873, total_loss: 2.6259188652038574
training step: 42874, total_loss: 0.6455838680267334
training step: 42875, total_loss: 1.322866678237915
training step: 42876, total_loss: 1.1972588300704956
training step: 42877, total_loss: 1.288294792175293
training step: 42878, total_loss: 3.237199306488037
training step: 42879, total_loss: 1.450937271118164
training step: 42880, total_loss: 0.7851990461349487
training step: 42881, total_loss: 0.023239044472575188
training step: 42882, total_loss: 1.5545804500579834
training step: 42883, total_loss: 0.053310275077819824
training step: 42884, total_loss: 3.047058582305908
training step: 42885, total_loss: 6.873293399810791
training step: 42886, total_loss: 2.0577356815338135
training step: 42887, total_loss: 1.2763824462890625
training step: 42888, total_loss: 1.0639177560806274
training step: 42889, total_loss: 2.1070022583007812
training step: 42890, total_loss: 4.113048553466797
training step: 42891, total_loss: 2.4551033973693848
training step: 42892, total_loss: 0.49287277460098267
training step: 42893, total_loss: 0.28633856773376465
training step: 42894, total_loss: 0.29035186767578125
training step: 42895, total_loss: 1.755937099456787
training step: 42896, total_loss: 0.6628904342651367
training step: 42897, total_loss: 0.5761559009552002
training step: 42898, total_loss: 0.15643852949142456
training step: 42899, total_loss: 3.804133892059326
training step: 42900, total_loss: 0.9846813678741455
training step: 42901, total_loss: 2.5132012367248535
training step: 42902, total_loss: 1.3772509098052979
training step: 42903, total_loss: 0.005772607401013374
training step: 42904, total_loss: 0.23748838901519775
training step: 42905, total_loss: 1.23984694480896
training step: 42906, total_loss: 3.0863921642303467
training step: 42907, total_loss: 1.0008490085601807
training step: 42908, total_loss: 1.577628254890442
training step: 42909, total_loss: 2.482489585876465
training step: 42910, total_loss: 2.132638931274414
training step: 42911, total_loss: 3.284679889678955
training step: 42912, total_loss: 4.909994125366211
training step: 42913, total_loss: 4.153783798217773
training step: 42914, total_loss: 1.4617516994476318
training step: 42915, total_loss: 2.1226162910461426
training step: 42916, total_loss: 3.1157724857330322
training step: 42917, total_loss: 2.5766348838806152
training step: 42918, total_loss: 4.006082534790039
training step: 42919, total_loss: 1.4581750631332397
training step: 42920, total_loss: 1.082195520401001
training step: 42921, total_loss: 0.08967220783233643
training step: 42922, total_loss: 1.1673015356063843
training step: 42923, total_loss: 1.8274810314178467
training step: 42924, total_loss: 1.4081451892852783
training step: 42925, total_loss: 1.65096914768219
training step: 42926, total_loss: 2.0371551513671875
training step: 42927, total_loss: 0.34223881363868713
training step: 42928, total_loss: 2.565335273742676
training step: 42929, total_loss: 3.480400323867798
training step: 42930, total_loss: 2.2478294372558594
training step: 42931, total_loss: 2.139218807220459
training step: 42932, total_loss: 0.06512806564569473
training step: 42933, total_loss: 2.539767265319824
training step: 42934, total_loss: 0.11369048058986664
training step: 42935, total_loss: 3.541294574737549
training step: 42936, total_loss: 2.3193860054016113
training step: 42937, total_loss: 1.6192785501480103INFO:tensorflow:Writing predictions to: test_output/predictions_43000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_43000.json

training step: 42938, total_loss: 0.13598813116550446
training step: 42939, total_loss: 7.356501579284668
training step: 42940, total_loss: 1.825721025466919
training step: 42941, total_loss: 0.5772997736930847
training step: 42942, total_loss: 3.293945074081421
training step: 42943, total_loss: 1.6381056308746338
training step: 42944, total_loss: 1.7903940677642822
training step: 42945, total_loss: 1.2718898057937622
training step: 42946, total_loss: 5.793239593505859
training step: 42947, total_loss: 2.8401079177856445
training step: 42948, total_loss: 2.1522650718688965
training step: 42949, total_loss: 0.7393671274185181
training step: 42950, total_loss: 0.3214925527572632
training step: 42951, total_loss: 0.9957085251808167
training step: 42952, total_loss: 1.290966272354126
training step: 42953, total_loss: 2.385666847229004
training step: 42954, total_loss: 1.0910617113113403
training step: 42955, total_loss: 0.3584612011909485
training step: 42956, total_loss: 3.179966688156128
training step: 42957, total_loss: 1.7976963520050049
training step: 42958, total_loss: 1.302725076675415
training step: 42959, total_loss: 1.0774505138397217
training step: 42960, total_loss: 1.3784419298171997
training step: 42961, total_loss: 1.7610678672790527
training step: 42962, total_loss: 1.387660026550293
training step: 42963, total_loss: 3.0751919746398926
training step: 42964, total_loss: 1.7142549753189087
training step: 42965, total_loss: 2.9942169189453125
training step: 42966, total_loss: 6.255136489868164
training step: 42967, total_loss: 3.5454955101013184
training step: 42968, total_loss: 1.5274951457977295
training step: 42969, total_loss: 2.77982759475708
training step: 42970, total_loss: 2.31857967376709
training step: 42971, total_loss: 1.7698482275009155
training step: 42972, total_loss: 2.2776267528533936
training step: 42973, total_loss: 4.034476280212402
training step: 42974, total_loss: 1.6419169902801514
training step: 42975, total_loss: 2.352245807647705
training step: 42976, total_loss: 2.047075033187866
training step: 42977, total_loss: 1.3728466033935547
training step: 42978, total_loss: 2.905954360961914
training step: 42979, total_loss: 2.877346992492676
training step: 42980, total_loss: 3.7377452850341797
training step: 42981, total_loss: 2.3735246658325195
training step: 42982, total_loss: 0.6933693885803223
training step: 42983, total_loss: 3.0827248096466064
training step: 42984, total_loss: 0.7944997549057007
training step: 42985, total_loss: 5.756050109863281
training step: 42986, total_loss: 0.4529067873954773
training step: 42987, total_loss: 1.3074109554290771
training step: 42988, total_loss: 1.9638574123382568
training step: 42989, total_loss: 2.2781686782836914
training step: 42990, total_loss: 0.3873472213745117
training step: 42991, total_loss: 1.97089421749115
training step: 42992, total_loss: 1.4935919046401978
training step: 42993, total_loss: 1.1076070070266724
training step: 42994, total_loss: 4.653623580932617
training step: 42995, total_loss: 1.4352030754089355
training step: 42996, total_loss: 0.15145155787467957
training step: 42997, total_loss: 0.006081589497625828
training step: 42998, total_loss: 3.6884939670562744
training step: 42999, total_loss: 1.6274349689483643
training step: 43000, total_loss: 4.049896240234375
epoch finished! shuffle=False
evaluation: 7000, total_loss: 1.765017032623291, f1: 55.11000142743246, followup: 22.881484862315535, yesno: 79.03544804503271, heq: 49.55119427962878, dheq: 3.1

Model saved in path test_output//model_43000.ckpt
training step: 43001, total_loss: 0.547776997089386
training step: 43002, total_loss: 0.8323385715484619
training step: 43003, total_loss: 1.8707995414733887
training step: 43004, total_loss: 1.9741384983062744
training step: 43005, total_loss: 1.695173978805542
training step: 43006, total_loss: 0.863362193107605
training step: 43007, total_loss: 2.606037139892578
training step: 43008, total_loss: 1.8542664051055908
training step: 43009, total_loss: 0.6127309203147888
training step: 43010, total_loss: 4.739900588989258
training step: 43011, total_loss: 0.2226017713546753
training step: 43012, total_loss: 1.0287169218063354
training step: 43013, total_loss: 1.2869805097579956
training step: 43014, total_loss: 2.729980945587158
training step: 43015, total_loss: 2.034731388092041
training step: 43016, total_loss: 2.408865213394165
training step: 43017, total_loss: 0.5713073015213013
training step: 43018, total_loss: 0.8518131971359253
training step: 43019, total_loss: 2.0882487297058105
training step: 43020, total_loss: 2.090149402618408
training step: 43021, total_loss: 1.3948287963867188
training step: 43022, total_loss: 2.2455015182495117
training step: 43023, total_loss: 1.9014973640441895
training step: 43024, total_loss: 0.8395165205001831
training step: 43025, total_loss: 1.2494913339614868
training step: 43026, total_loss: 1.4936082363128662
training step: 43027, total_loss: 3.04179310798645
training step: 43028, total_loss: 0.7608135938644409
training step: 43029, total_loss: 2.0237395763397217
training step: 43030, total_loss: 2.816732883453369
training step: 43031, total_loss: 1.9257580041885376
training step: 43032, total_loss: 3.427058458328247
training step: 43033, total_loss: 5.970795631408691
training step: 43034, total_loss: 0.8427155613899231
training step: 43035, total_loss: 1.5287747383117676
training step: 43036, total_loss: 2.929938793182373
training step: 43037, total_loss: 0.012389115989208221
training step: 43038, total_loss: 1.8319252729415894
training step: 43039, total_loss: 2.608767032623291
training step: 43040, total_loss: 0.8727542161941528
training step: 43041, total_loss: 1.8874250650405884
training step: 43042, total_loss: 0.29387781023979187
training step: 43043, total_loss: 0.34193557500839233
training step: 43044, total_loss: 2.9060986042022705
training step: 43045, total_loss: 0.3063260018825531
training step: 43046, total_loss: 1.5458009243011475
training step: 43047, total_loss: 1.868506908416748
training step: 43048, total_loss: 0.6353429555892944
training step: 43049, total_loss: 1.5273847579956055
training step: 43050, total_loss: 1.7131822109222412
training step: 43051, total_loss: 1.5267672538757324
training step: 43052, total_loss: 6.163519859313965
training step: 43053, total_loss: 3.028834104537964
training step: 43054, total_loss: 4.58148193359375
training step: 43055, total_loss: 0.21065688133239746
training step: 43056, total_loss: 1.3586549758911133
training step: 43057, total_loss: 4.47811222076416
training step: 43058, total_loss: 3.8284170627593994
training step: 43059, total_loss: 3.9940404891967773
training step: 43060, total_loss: 3.178943157196045
training step: 43061, total_loss: 1.7399051189422607
training step: 43062, total_loss: 0.4192671775817871
training step: 43063, total_loss: 1.851058006286621
training step: 43064, total_loss: 1.754700779914856
training step: 43065, total_loss: 2.6952693462371826
training step: 43066, total_loss: 0.8088807463645935
training step: 43067, total_loss: 4.404940605163574
training step: 43068, total_loss: 1.1910362243652344
training step: 43069, total_loss: 0.3596811294555664
training step: 43070, total_loss: 2.547459363937378
training step: 43071, total_loss: 1.5787872076034546
training step: 43072, total_loss: 1.441574215888977
training step: 43073, total_loss: 2.5509753227233887
training step: 43074, total_loss: 2.173685073852539
training step: 43075, total_loss: 0.8309370279312134
training step: 43076, total_loss: 0.37694019079208374
training step: 43077, total_loss: 2.9070773124694824
training step: 43078, total_loss: 1.5049660205841064
training step: 43079, total_loss: 1.551159143447876
training step: 43080, total_loss: 2.198261260986328
training step: 43081, total_loss: 2.3877413272857666
training step: 43082, total_loss: 1.0748639106750488
training step: 43083, total_loss: 0.3524059057235718
training step: 43084, total_loss: 1.4864494800567627
training step: 43085, total_loss: 2.0932977199554443
training step: 43086, total_loss: 1.27923583984375
training step: 43087, total_loss: 2.9698097705841064
training step: 43088, total_loss: 1.9456766843795776
training step: 43089, total_loss: 0.7290513515472412
training step: 43090, total_loss: 0.2913663387298584
training step: 43091, total_loss: 1.3518941402435303
training step: 43092, total_loss: 3.41859769821167
training step: 43093, total_loss: 3.2980308532714844
training step: 43094, total_loss: 1.189103364944458
training step: 43095, total_loss: 1.6055777072906494
training step: 43096, total_loss: 2.6068146228790283
training step: 43097, total_loss: 0.9616912007331848
training step: 43098, total_loss: 1.4829483032226562
training step: 43099, total_loss: 2.6201119422912598
training step: 43100, total_loss: 1.8726229667663574
training step: 43101, total_loss: 1.8488640785217285
training step: 43102, total_loss: 3.115931272506714
training step: 43103, total_loss: 1.5289413928985596
training step: 43104, total_loss: 2.8122124671936035
training step: 43105, total_loss: 2.1495749950408936
training step: 43106, total_loss: 1.2430126667022705
training step: 43107, total_loss: 0.1891283392906189
training step: 43108, total_loss: 3.5425539016723633
training step: 43109, total_loss: 2.876951217651367
training step: 43110, total_loss: 2.3381457328796387
training step: 43111, total_loss: 1.8076543807983398
training step: 43112, total_loss: 0.9799502491950989
training step: 43113, total_loss: 2.9426393508911133
training step: 43114, total_loss: 4.07863712310791
training step: 43115, total_loss: 1.5849828720092773
training step: 43116, total_loss: 3.330244541168213
training step: 43117, total_loss: 0.047875385731458664
training step: 43118, total_loss: 2.1001482009887695
training step: 43119, total_loss: 2.2262330055236816
training step: 43120, total_loss: 3.3362250328063965
training step: 43121, total_loss: 2.978818416595459
training step: 43122, total_loss: 2.686530113220215
training step: 43123, total_loss: 3.444060802459717
training step: 43124, total_loss: 1.315893292427063
training step: 43125, total_loss: 0.8609195947647095
training step: 43126, total_loss: 2.5400757789611816
training step: 43127, total_loss: 0.10278920829296112
training step: 43128, total_loss: 3.0426673889160156
training step: 43129, total_loss: 0.002435884438455105
training step: 43130, total_loss: 0.8187680244445801
training step: 43131, total_loss: 0.9531337022781372
training step: 43132, total_loss: 0.6494739055633545
training step: 43133, total_loss: 1.653147578239441
training step: 43134, total_loss: 5.171695232391357
training step: 43135, total_loss: 3.1830437183380127
training step: 43136, total_loss: 1.1766963005065918
training step: 43137, total_loss: 4.388896942138672
training step: 43138, total_loss: 2.194948434829712
training step: 43139, total_loss: 1.293712854385376
training step: 43140, total_loss: 3.295584201812744
training step: 43141, total_loss: 2.8851497173309326
training step: 43142, total_loss: 2.4664440155029297
training step: 43143, total_loss: 0.15238627791404724
training step: 43144, total_loss: 3.339890956878662
training step: 43145, total_loss: 0.7934033870697021
training step: 43146, total_loss: 2.561131000518799
training step: 43147, total_loss: 0.7771060466766357
training step: 43148, total_loss: 0.730628252029419
training step: 43149, total_loss: 2.843665838241577
training step: 43150, total_loss: 1.9451532363891602
training step: 43151, total_loss: 1.4138216972351074
training step: 43152, total_loss: 1.595057487487793
training step: 43153, total_loss: 1.4159772396087646
training step: 43154, total_loss: 0.8432261943817139
training step: 43155, total_loss: 3.371366262435913
training step: 43156, total_loss: 2.6677026748657227
training step: 43157, total_loss: 1.7861526012420654
training step: 43158, total_loss: 1.563328504562378
training step: 43159, total_loss: 1.4770009517669678
training step: 43160, total_loss: 0.12253865599632263
training step: 43161, total_loss: 3.7461354732513428
training step: 43162, total_loss: 0.726193904876709
training step: 43163, total_loss: 2.666512966156006
training step: 43164, total_loss: 5.144323348999023
training step: 43165, total_loss: 0.12715108692646027
training step: 43166, total_loss: 9.269096374511719
training step: 43167, total_loss: 0.25770100951194763
training step: 43168, total_loss: 3.055696487426758
training step: 43169, total_loss: 3.017935276031494
training step: 43170, total_loss: 1.2776545286178589
training step: 43171, total_loss: 2.182858467102051
training step: 43172, total_loss: 0.14102929830551147
training step: 43173, total_loss: 1.354285478591919
training step: 43174, total_loss: 1.0446851253509521
training step: 43175, total_loss: 2.3139944076538086
training step: 43176, total_loss: 0.9484027624130249
training step: 43177, total_loss: 4.5364251136779785
training step: 43178, total_loss: 2.3370001316070557
training step: 43179, total_loss: 0.7036526203155518
training step: 43180, total_loss: 0.31776922941207886
training step: 43181, total_loss: 1.0538876056671143
training step: 43182, total_loss: 1.7081115245819092
training step: 43183, total_loss: 1.579636573791504
training step: 43184, total_loss: 1.0917918682098389
training step: 43185, total_loss: 0.511408805847168
training step: 43186, total_loss: 2.8778624534606934
training step: 43187, total_loss: 2.2949085235595703
training step: 43188, total_loss: 2.617436408996582
training step: 43189, total_loss: 1.7167472839355469
training step: 43190, total_loss: 2.1372110843658447
training step: 43191, total_loss: 2.8050012588500977
training step: 43192, total_loss: 0.21447324752807617
training step: 43193, total_loss: 1.092717170715332
training step: 43194, total_loss: 1.2307089567184448
training step: 43195, total_loss: 2.649038314819336
training step: 43196, total_loss: 3.334845542907715
training step: 43197, total_loss: 3.738337993621826
training step: 43198, total_loss: 2.3984899520874023
training step: 43199, total_loss: 1.1171807050704956
training step: 43200, total_loss: 0.2766761779785156
training step: 43201, total_loss: 2.489152669906616
training step: 43202, total_loss: 2.012716054916382
training step: 43203, total_loss: 2.035264253616333
training step: 43204, total_loss: 2.7176811695098877
training step: 43205, total_loss: 0.9667425155639648
training step: 43206, total_loss: 1.3139535188674927
training step: 43207, total_loss: 2.1804943084716797
training step: 43208, total_loss: 1.7413204908370972
training step: 43209, total_loss: 2.738858222961426
training step: 43210, total_loss: 2.436572790145874
training step: 43211, total_loss: 2.473268985748291
training step: 43212, total_loss: 0.844430685043335
training step: 43213, total_loss: 1.241213321685791
training step: 43214, total_loss: 1.9052209854125977
training step: 43215, total_loss: 2.0681710243225098
training step: 43216, total_loss: 1.4942874908447266
training step: 43217, total_loss: 2.9772021770477295
training step: 43218, total_loss: 3.28757643699646
training step: 43219, total_loss: 2.346235990524292
training step: 43220, total_loss: 1.4374783039093018
training step: 43221, total_loss: 2.7778072357177734
training step: 43222, total_loss: 2.1217126846313477
training step: 43223, total_loss: 5.093640327453613
training step: 43224, total_loss: 1.3200206756591797
training step: 43225, total_loss: 3.1029953956604004
training step: 43226, total_loss: 1.055098533630371
training step: 43227, total_loss: 2.5847959518432617
training step: 43228, total_loss: 3.9582133293151855
training step: 43229, total_loss: 0.19640597701072693
training step: 43230, total_loss: 0.7396875023841858
training step: 43231, total_loss: 1.042784333229065
training step: 43232, total_loss: 0.508953869342804
training step: 43233, total_loss: 0.2265283316373825
training step: 43234, total_loss: 0.04037414491176605
training step: 43235, total_loss: 1.969888687133789
training step: 43236, total_loss: 1.4131088256835938
training step: 43237, total_loss: 1.012923002243042
training step: 43238, total_loss: 0.9047505259513855
training step: 43239, total_loss: 0.4553396701812744
training step: 43240, total_loss: 3.3446850776672363
training step: 43241, total_loss: 2.888418674468994
training step: 43242, total_loss: 0.0909605622291565
training step: 43243, total_loss: 0.7007550001144409
training step: 43244, total_loss: 2.1971840858459473
training step: 43245, total_loss: 2.250993490219116
training step: 43246, total_loss: 4.51294469833374
training step: 43247, total_loss: 2.6536848545074463
training step: 43248, total_loss: 2.101863384246826
training step: 43249, total_loss: 1.9578402042388916
training step: 43250, total_loss: 1.3210675716400146
training step: 43251, total_loss: 1.0833330154418945
training step: 43252, total_loss: 1.281884789466858
training step: 43253, total_loss: 0.294904887676239
training step: 43254, total_loss: 1.7899723052978516
training step: 43255, total_loss: 2.9969539642333984
training step: 43256, total_loss: 0.702297031879425
training step: 43257, total_loss: 4.209534645080566
training step: 43258, total_loss: 0.002562569919973612
training step: 43259, total_loss: 6.024548530578613
training step: 43260, total_loss: 1.988084316253662
training step: 43261, total_loss: 2.056088924407959
training step: 43262, total_loss: 0.8063051700592041
training step: 43263, total_loss: 1.8631922006607056
training step: 43264, total_loss: 1.7649496793746948
training step: 43265, total_loss: 1.0821850299835205
training step: 43266, total_loss: 2.150235652923584
training step: 43267, total_loss: 1.0537872314453125
training step: 43268, total_loss: 1.5818047523498535
training step: 43269, total_loss: 1.114027500152588
training step: 43270, total_loss: 1.8278093338012695
training step: 43271, total_loss: 0.9601740837097168
training step: 43272, total_loss: 0.7290490865707397
training step: 43273, total_loss: 2.661858081817627
training step: 43274, total_loss: 0.043457090854644775
training step: 43275, total_loss: 1.8401849269866943
training step: 43276, total_loss: 1.2198442220687866
training step: 43277, total_loss: 2.8824024200439453
training step: 43278, total_loss: 2.047776937484741
training step: 43279, total_loss: 0.13614852726459503
training step: 43280, total_loss: 1.2191182374954224
training step: 43281, total_loss: 1.2643027305603027
training step: 43282, total_loss: 3.1645989418029785
training step: 43283, total_loss: 1.8424148559570312
training step: 43284, total_loss: 1.510129451751709
training step: 43285, total_loss: 2.7884750366210938
training step: 43286, total_loss: 1.8081920146942139
training step: 43287, total_loss: 0.6294939517974854
training step: 43288, total_loss: 3.0465736389160156
training step: 43289, total_loss: 2.610274076461792
training step: 43290, total_loss: 2.3210158348083496
training step: 43291, total_loss: 2.9508633613586426
training step: 43292, total_loss: 2.908071994781494
training step: 43293, total_loss: 0.9590772986412048
training step: 43294, total_loss: 2.018710136413574
training step: 43295, total_loss: 2.7231361865997314
training step: 43296, total_loss: 0.9524102210998535
training step: 43297, total_loss: 1.1198174953460693
training step: 43298, total_loss: 1.8508399724960327
training step: 43299, total_loss: 1.8234204053878784
training step: 43300, total_loss: 0.52129065990448
training step: 43301, total_loss: 0.6575908064842224
training step: 43302, total_loss: 1.5358299016952515
training step: 43303, total_loss: 3.5367679595947266
training step: 43304, total_loss: 2.8054606914520264
training step: 43305, total_loss: 1.576585292816162
training step: 43306, total_loss: 3.1921777725219727
training step: 43307, total_loss: 2.6268820762634277
training step: 43308, total_loss: 0.6242854595184326
training step: 43309, total_loss: 3.393345355987549
training step: 43310, total_loss: 1.5391548871994019
training step: 43311, total_loss: 6.008970737457275
training step: 43312, total_loss: 3.0732622146606445
training step: 43313, total_loss: 1.3091232776641846
training step: 43314, total_loss: 1.288894772529602
training step: 43315, total_loss: 3.859820604324341
training step: 43316, total_loss: 2.577995777130127
training step: 43317, total_loss: 1.3409918546676636
training step: 43318, total_loss: 1.4109699726104736
training step: 43319, total_loss: 1.4938762187957764
training step: 43320, total_loss: 1.9461473226547241
training step: 43321, total_loss: 1.7712889909744263
training step: 43322, total_loss: 0.9540722966194153
training step: 43323, total_loss: 0.4963018298149109
training step: 43324, total_loss: 6.010977745056152
training step: 43325, total_loss: 1.8388540744781494
training step: 43326, total_loss: 0.874865710735321
training step: 43327, total_loss: 4.013722896575928
training step: 43328, total_loss: 1.5312814712524414
training step: 43329, total_loss: 0.22062355279922485
training step: 43330, total_loss: 1.5429582595825195
training step: 43331, total_loss: 1.1098854541778564
training step: 43332, total_loss: 1.8390989303588867
training step: 43333, total_loss: 0.9626908898353577
training step: 43334, total_loss: 0.9660075306892395
training step: 43335, total_loss: 1.5764240026474
training step: 43336, total_loss: 1.3839225769042969
training step: 43337, total_loss: 2.016988515853882
training step: 43338, total_loss: 1.4312024116516113
training step: 43339, total_loss: 0.6531407833099365
training step: 43340, total_loss: 0.027094373479485512
training step: 43341, total_loss: 1.5968544483184814
training step: 43342, total_loss: 1.8261910676956177
training step: 43343, total_loss: 6.314654350280762
training step: 43344, total_loss: 1.59640371799469
training step: 43345, total_loss: 2.2086148262023926
training step: 43346, total_loss: 1.526627540588379
training step: 43347, total_loss: 2.714080572128296
training step: 43348, total_loss: 1.4437603950500488
training step: 43349, total_loss: 0.9736642241477966
training step: 43350, total_loss: 5.3605241775512695
training step: 43351, total_loss: 2.2760658264160156
training step: 43352, total_loss: 2.149259567260742
training step: 43353, total_loss: 3.2027204036712646
training step: 43354, total_loss: 1.4499595165252686
training step: 43355, total_loss: 1.9586868286132812
training step: 43356, total_loss: 0.5041825771331787
training step: 43357, total_loss: 1.7708961963653564
training step: 43358, total_loss: 3.437350034713745
training step: 43359, total_loss: 0.9371529817581177
training step: 43360, total_loss: 1.2105228900909424
training step: 43361, total_loss: 1.3003758192062378
training step: 43362, total_loss: 4.05821418762207
training step: 43363, total_loss: 0.21430470049381256
training step: 43364, total_loss: 3.025872230529785
training step: 43365, total_loss: 3.0925395488739014
training step: 43366, total_loss: 1.2706630229949951
training step: 43367, total_loss: 0.6372281312942505
training step: 43368, total_loss: 1.0589561462402344
training step: 43369, total_loss: 2.8809030055999756
training step: 43370, total_loss: 2.1268882751464844
training step: 43371, total_loss: 1.9447603225708008
training step: 43372, total_loss: 0.002160618081688881
training step: 43373, total_loss: 1.6679556369781494
training step: 43374, total_loss: 4.357090950012207
training step: 43375, total_loss: 1.4475812911987305
training step: 43376, total_loss: 3.2331337928771973
training step: 43377, total_loss: 2.5557425022125244
training step: 43378, total_loss: 1.028869390487671
training step: 43379, total_loss: 6.138983726501465
training step: 43380, total_loss: 1.4992282390594482
training step: 43381, total_loss: 2.2572708129882812
training step: 43382, total_loss: 3.2170162200927734
training step: 43383, total_loss: 1.2675862312316895
training step: 43384, total_loss: 1.6271432638168335
training step: 43385, total_loss: 0.5917333364486694
training step: 43386, total_loss: 1.6360138654708862
training step: 43387, total_loss: 1.268372654914856
training step: 43388, total_loss: 2.4296681880950928
training step: 43389, total_loss: 0.16318492591381073
training step: 43390, total_loss: 2.965618133544922
training step: 43391, total_loss: 0.24758294224739075
training step: 43392, total_loss: 2.1725378036499023
training step: 43393, total_loss: 0.5219694375991821
training step: 43394, total_loss: 2.029275894165039
training step: 43395, total_loss: 1.7659680843353271
training step: 43396, total_loss: 1.7123242616653442
training step: 43397, total_loss: 3.475745916366577
training step: 43398, total_loss: 1.9558879137039185
training step: 43399, total_loss: 1.3790878057479858
training step: 43400, total_loss: 1.472599983215332
training step: 43401, total_loss: 3.064913749694824
training step: 43402, total_loss: 1.4383296966552734
training step: 43403, total_loss: 1.6029069423675537
training step: 43404, total_loss: 1.1520512104034424
training step: 43405, total_loss: 2.3050131797790527
training step: 43406, total_loss: 2.121927261352539
training step: 43407, total_loss: 1.1234800815582275
training step: 43408, total_loss: 1.4832502603530884
training step: 43409, total_loss: 1.9630398750305176
training step: 43410, total_loss: 2.487985610961914
training step: 43411, total_loss: 1.553572654724121
training step: 43412, total_loss: 0.9659132361412048
training step: 43413, total_loss: 2.230165958404541
training step: 43414, total_loss: 2.984334945678711
training step: 43415, total_loss: 3.1125450134277344
training step: 43416, total_loss: 1.6571924686431885
training step: 43417, total_loss: 1.7659409046173096
training step: 43418, total_loss: 1.52315092086792
training step: 43419, total_loss: 1.4064228534698486
training step: 43420, total_loss: 1.172858715057373
training step: 43421, total_loss: 3.0772430896759033
training step: 43422, total_loss: 8.170172691345215
training step: 43423, total_loss: 7.292652130126953
training step: 43424, total_loss: 0.9854872226715088
training step: 43425, total_loss: 2.5828723907470703
training step: 43426, total_loss: 0.6520546674728394
training step: 43427, total_loss: 2.6925253868103027
training step: 43428, total_loss: 3.027122974395752
training step: 43429, total_loss: 0.3123958706855774
training step: 43430, total_loss: 0.6524580717086792
training step: 43431, total_loss: 3.496769428253174
training step: 43432, total_loss: 2.256178855895996
training step: 43433, total_loss: 1.0294716358184814
training step: 43434, total_loss: 1.7882583141326904
training step: 43435, total_loss: 3.748903274536133
training step: 43436, total_loss: 2.7151331901550293
training step: 43437, total_loss: 2.194523334503174
training step: 43438, total_loss: 1.9248411655426025
training step: 43439, total_loss: 2.4648845195770264
training step: 43440, total_loss: 4.536313056945801
training step: 43441, total_loss: 0.8782267570495605
training step: 43442, total_loss: 5.048967361450195
training step: 43443, total_loss: 2.3088672161102295
training step: 43444, total_loss: 2.7402567863464355
training step: 43445, total_loss: 1.657691478729248
training step: 43446, total_loss: 1.2491352558135986
training step: 43447, total_loss: 0.05692388489842415
training step: 43448, total_loss: 1.8383673429489136
training step: 43449, total_loss: 2.649287223815918
training step: 43450, total_loss: 4.39243221282959
training step: 43451, total_loss: 2.480563163757324
training step: 43452, total_loss: 0.5637510418891907
training step: 43453, total_loss: 1.613694667816162
training step: 43454, total_loss: 2.064573049545288
training step: 43455, total_loss: 2.0555434226989746
training step: 43456, total_loss: 1.2174980640411377
training step: 43457, total_loss: 2.205258369445801
training step: 43458, total_loss: 2.901805877685547
training step: 43459, total_loss: 1.9195144176483154
training step: 43460, total_loss: 1.643235445022583
training step: 43461, total_loss: 2.1790878772735596
training step: 43462, total_loss: 0.8783302903175354
training step: 43463, total_loss: 4.111272811889648
training step: 43464, total_loss: 0.5602167844772339
training step: 43465, total_loss: 1.7882881164550781
training step: 43466, total_loss: 1.915377140045166
training step: 43467, total_loss: 3.3680548667907715
training step: 43468, total_loss: 1.631063461303711
training step: 43469, total_loss: 4.275473117828369
training step: 43470, total_loss: 0.7061378359794617
training step: 43471, total_loss: 2.764030694961548
training step: 43472, total_loss: 3.3625240325927734
training step: 43473, total_loss: 3.5515847206115723
training step: 43474, total_loss: 2.9512288570404053
training step: 43475, total_loss: 1.8875112533569336
training step: 43476, total_loss: 1.1768914461135864
training step: 43477, total_loss: 2.1692700386047363
training step: 43478, total_loss: 0.7839375138282776
training step: 43479, total_loss: 1.2443292140960693
training step: 43480, total_loss: 1.2542486190795898
training step: 43481, total_loss: 2.4020943641662598
training step: 43482, total_loss: 1.3552935123443604
training step: 43483, total_loss: 1.2801823616027832
training step: 43484, total_loss: 0.7846879959106445
training step: 43485, total_loss: 1.4651867151260376
training step: 43486, total_loss: 1.2652673721313477
training step: 43487, total_loss: 0.13666683435440063
training step: 43488, total_loss: 0.16057026386260986
training step: 43489, total_loss: 1.63413667678833
training step: 43490, total_loss: 1.183269739151001
training step: 43491, total_loss: 3.073374032974243
training step: 43492, total_loss: 2.0663399696350098
training step: 43493, total_loss: 1.2484686374664307
training step: 43494, total_loss: 0.6437845826148987
training step: 43495, total_loss: 1.4428763389587402
training step: 43496, total_loss: 1.6235404014587402
training step: 43497, total_loss: 1.1033244132995605
training step: 43498, total_loss: 3.037262439727783
training step: 43499, total_loss: 4.699817657470703
training step: 43500, total_loss: 2.277580738067627
training step: 43501, total_loss: 0.7423198223114014
training step: 43502, total_loss: 0.20652514696121216
training step: 43503, total_loss: 2.5072202682495117
training step: 43504, total_loss: 1.5973336696624756
training step: 43505, total_loss: 1.028063178062439
training step: 43506, total_loss: 2.353750228881836
training step: 43507, total_loss: 2.468493700027466
training step: 43508, total_loss: 3.182940721511841
training step: 43509, total_loss: 1.129045009613037
training step: 43510, total_loss: 1.1420990228652954
training step: 43511, total_loss: 0.5327118635177612
training step: 43512, total_loss: 1.9295904636383057
training step: 43513, total_loss: 2.281979560852051
training step: 43514, total_loss: 0.5212740302085876
training step: 43515, total_loss: 4.269374370574951
training step: 43516, total_loss: 0.4640248417854309
training step: 43517, total_loss: 3.0121259689331055
training step: 43518, total_loss: 0.11742106080055237
training step: 43519, total_loss: 2.2675929069519043
training step: 43520, total_loss: 2.0525894165039062
training step: 43521, total_loss: 2.977536916732788
training step: 43522, total_loss: 2.1194353103637695
training step: 43523, total_loss: 1.835465431213379
training step: 43524, total_loss: 0.8106387853622437
training step: 43525, total_loss: 1.3659729957580566
training step: 43526, total_loss: 0.14785674214363098
training step: 43527, total_loss: 3.257094144821167
training step: 43528, total_loss: 2.731268882751465
training step: 43529, total_loss: 7.873285293579102
training step: 43530, total_loss: 0.9731613397598267
training step: 43531, total_loss: 1.9525128602981567
training step: 43532, total_loss: 2.1708195209503174
training step: 43533, total_loss: 1.3637056350708008
training step: 43534, total_loss: 3.0145187377929688
training step: 43535, total_loss: 1.6189310550689697
training step: 43536, total_loss: 1.8468785285949707
training step: 43537, total_loss: 1.908542513847351
training step: 43538, total_loss: 0.5092648267745972
training step: 43539, total_loss: 1.6898454427719116
training step: 43540, total_loss: 0.6677027940750122
training step: 43541, total_loss: 2.0696940422058105
training step: 43542, total_loss: 1.159987211227417
training step: 43543, total_loss: 1.732045292854309
training step: 43544, total_loss: 3.2478930950164795
training step: 43545, total_loss: 1.7029495239257812
training step: 43546, total_loss: 3.128011465072632
training step: 43547, total_loss: 2.249210834503174
training step: 43548, total_loss: 3.028846502304077
training step: 43549, total_loss: 0.10273321717977524
training step: 43550, total_loss: 1.865004301071167
training step: 43551, total_loss: 0.8743202090263367
training step: 43552, total_loss: 1.9905824661254883
training step: 43553, total_loss: 1.8289539813995361
training step: 43554, total_loss: 3.7502739429473877
training step: 43555, total_loss: 0.4074566960334778
training step: 43556, total_loss: 1.5329041481018066
training step: 43557, total_loss: 2.4856600761413574
training step: 43558, total_loss: 1.3978790044784546
training step: 43559, total_loss: 4.519701957702637
training step: 43560, total_loss: 2.0921101570129395
training step: 43561, total_loss: 1.490119218826294
training step: 43562, total_loss: 1.0867692232131958
training step: 43563, total_loss: 3.354902982711792
training step: 43564, total_loss: 4.205235481262207
training step: 43565, total_loss: 3.0447897911071777
training step: 43566, total_loss: 0.3385471701622009
training step: 43567, total_loss: 1.529748558998108
training step: 43568, total_loss: 5.409865379333496
training step: 43569, total_loss: 1.4070123434066772
training step: 43570, total_loss: 2.1836071014404297
training step: 43571, total_loss: 2.905935764312744
training step: 43572, total_loss: 0.9793875813484192
training step: 43573, total_loss: 3.572988748550415
training step: 43574, total_loss: 2.281951665878296
training step: 43575, total_loss: 4.706043243408203
training step: 43576, total_loss: 1.166813850402832
training step: 43577, total_loss: 1.0389795303344727
training step: 43578, total_loss: 0.21339476108551025
training step: 43579, total_loss: 2.4792885780334473
training step: 43580, total_loss: 2.95337176322937
training step: 43581, total_loss: 1.4189088344573975
training step: 43582, total_loss: 0.5409491062164307
training step: 43583, total_loss: 2.0147600173950195
training step: 43584, total_loss: 2.5308005809783936
training step: 43585, total_loss: 3.039032459259033
training step: 43586, total_loss: 0.699928879737854
training step: 43587, total_loss: 0.7513731718063354
training step: 43588, total_loss: 1.7812271118164062
training step: 43589, total_loss: 1.2968723773956299
training step: 43590, total_loss: 0.9547345042228699
training step: 43591, total_loss: 0.6544464230537415
training step: 43592, total_loss: 1.0914701223373413
training step: 43593, total_loss: 1.7027544975280762
training step: 43594, total_loss: 0.9048448204994202
training step: 43595, total_loss: 0.9613967537879944
training step: 43596, total_loss: 1.8350504636764526
training step: 43597, total_loss: 0.3965228199958801
training step: 43598, total_loss: 1.1019494533538818
training step: 43599, total_loss: 0.7448062896728516
training step: 43600, total_loss: 2.471310615539551
training step: 43601, total_loss: 0.041435543447732925
training step: 43602, total_loss: 0.7815688848495483
training step: 43603, total_loss: 0.1567292958498001
training step: 43604, total_loss: 3.5036544799804688
training step: 43605, total_loss: 0.6866810321807861
training step: 43606, total_loss: 4.182748794555664
training step: 43607, total_loss: 0.9633020758628845
training step: 43608, total_loss: 1.8238589763641357
training step: 43609, total_loss: 1.8387959003448486
training step: 43610, total_loss: 1.5109163522720337
training step: 43611, total_loss: 1.8861098289489746
training step: 43612, total_loss: 0.4628487825393677
training step: 43613, total_loss: 0.3374897539615631
training step: 43614, total_loss: 0.16848576068878174
training step: 43615, total_loss: 4.591904163360596
training step: 43616, total_loss: 2.07157301902771
training step: 43617, total_loss: 0.8730587959289551
training step: 43618, total_loss: 1.8187888860702515
training step: 43619, total_loss: 1.2506234645843506
training step: 43620, total_loss: 1.0996779203414917
training step: 43621, total_loss: 3.274566411972046
training step: 43622, total_loss: 3.515774726867676
training step: 43623, total_loss: 0.07303715497255325
training step: 43624, total_loss: 2.1273322105407715
training step: 43625, total_loss: 2.562917709350586
training step: 43626, total_loss: 4.371628761291504
training step: 43627, total_loss: 3.429619789123535
training step: 43628, total_loss: 2.7565927505493164
training step: 43629, total_loss: 3.4099621772766113
training step: 43630, total_loss: 1.6392409801483154
training step: 43631, total_loss: 1.543057918548584
training step: 43632, total_loss: 2.0234410762786865
training step: 43633, total_loss: 3.2078702449798584
training step: 43634, total_loss: 1.6954997777938843
training step: 43635, total_loss: 3.263962745666504
training step: 43636, total_loss: 1.1548511981964111
training step: 43637, total_loss: 1.2672228813171387
training step: 43638, total_loss: 2.2421929836273193
training step: 43639, total_loss: 0.8413616418838501
training step: 43640, total_loss: 2.273102045059204
training step: 43641, total_loss: 1.8093831539154053
training step: 43642, total_loss: 1.764223337173462
training step: 43643, total_loss: 0.9957945346832275
training step: 43644, total_loss: 3.661191701889038
training step: 43645, total_loss: 2.1386773586273193
training step: 43646, total_loss: 1.183567762374878
training step: 43647, total_loss: 1.2635449171066284
training step: 43648, total_loss: 2.5045549869537354
training step: 43649, total_loss: 5.4623637199401855
training step: 43650, total_loss: 0.4814962148666382
training step: 43651, total_loss: 1.6192656755447388
training step: 43652, total_loss: 5.8963117599487305
training step: 43653, total_loss: 1.1923643350601196
training step: 43654, total_loss: 1.6886603832244873
training step: 43655, total_loss: 0.021859776228666306
training step: 43656, total_loss: 1.6005513668060303
training step: 43657, total_loss: 2.9128544330596924
training step: 43658, total_loss: 3.1107306480407715
training step: 43659, total_loss: 1.6268603801727295
training step: 43660, total_loss: 2.2931814193725586
training step: 43661, total_loss: 2.154325485229492
training step: 43662, total_loss: 2.9617843627929688
training step: 43663, total_loss: 0.41072338819503784
training step: 43664, total_loss: 1.299078345298767
training step: 43665, total_loss: 1.2018496990203857
training step: 43666, total_loss: 3.3885560035705566
training step: 43667, total_loss: 2.8683197498321533
training step: 43668, total_loss: 1.7228991985321045
training step: 43669, total_loss: 1.787370204925537
training step: 43670, total_loss: 1.5229523181915283
training step: 43671, total_loss: 0.9708013534545898
training step: 43672, total_loss: 2.74314022064209
training step: 43673, total_loss: 0.12152315676212311
training step: 43674, total_loss: 3.0567264556884766
training step: 43675, total_loss: 0.9421172142028809
training step: 43676, total_loss: 3.419377326965332
training step: 43677, total_loss: 2.6715002059936523
training step: 43678, total_loss: 2.5631766319274902
training step: 43679, total_loss: 0.6728191375732422
training step: 43680, total_loss: 2.1638975143432617
training step: 43681, total_loss: 3.1519532203674316
training step: 43682, total_loss: 1.5788071155548096
training step: 43683, total_loss: 0.7752202749252319
training step: 43684, total_loss: 1.7055752277374268
training step: 43685, total_loss: 1.6129248142242432
training step: 43686, total_loss: 1.619740605354309
training step: 43687, total_loss: 1.2932019233703613
training step: 43688, total_loss: 1.8259614706039429
training step: 43689, total_loss: 2.075707197189331
training step: 43690, total_loss: 1.3619394302368164
training step: 43691, total_loss: 0.355255663394928
training step: 43692, total_loss: 2.909205675125122
training step: 43693, total_loss: 1.753871202468872
training step: 43694, total_loss: 3.8189735412597656
training step: 43695, total_loss: 2.8237690925598145
training step: 43696, total_loss: 0.8402146100997925
training step: 43697, total_loss: 1.589036226272583
training step: 43698, total_loss: 1.2234376668930054
training step: 43699, total_loss: 0.8571733832359314
training step: 43700, total_loss: 0.09871868789196014
training step: 43701, total_loss: 2.8963418006896973
training step: 43702, total_loss: 0.8662373423576355
training step: 43703, total_loss: 2.5047311782836914
training step: 43704, total_loss: 1.6206570863723755
training step: 43705, total_loss: 0.1764456182718277
training step: 43706, total_loss: 1.6760985851287842
training step: 43707, total_loss: 1.948610544204712
training step: 43708, total_loss: 1.9540197849273682
training step: 43709, total_loss: 2.242523193359375
training step: 43710, total_loss: 0.6360983848571777
training step: 43711, total_loss: 0.37944740056991577
training step: 43712, total_loss: 2.0976104736328125
training step: 43713, total_loss: 1.1595096588134766
training step: 43714, total_loss: 3.346381187438965
training step: 43715, total_loss: 2.660191535949707
training step: 43716, total_loss: 3.634110450744629
training step: 43717, total_loss: 2.983901023864746
training step: 43718, total_loss: 2.6118481159210205
training step: 43719, total_loss: 0.003323159646242857
training step: 43720, total_loss: 2.484395980834961
training step: 43721, total_loss: 1.870593547821045
training step: 43722, total_loss: 0.5661983489990234
training step: 43723, total_loss: 1.5395865440368652
training step: 43724, total_loss: 0.5417560338973999
training step: 43725, total_loss: 1.1836026906967163
training step: 43726, total_loss: 1.1383353471755981
training step: 43727, total_loss: 1.0511912107467651
training step: 43728, total_loss: 2.0925168991088867
training step: 43729, total_loss: 0.10857030749320984
training step: 43730, total_loss: 2.4670181274414062
training step: 43731, total_loss: 0.3949434161186218
training step: 43732, total_loss: 2.242997884750366
training step: 43733, total_loss: 0.4482616186141968
training step: 43734, total_loss: 2.3466169834136963
training step: 43735, total_loss: 2.17299222946167
training step: 43736, total_loss: 5.865176200866699
training step: 43737, total_loss: 0.7159839868545532
training step: 43738, total_loss: 3.4212355613708496
training step: 43739, total_loss: 1.698883295059204
training step: 43740, total_loss: 2.780184268951416
training step: 43741, total_loss: 0.873275876045227
training step: 43742, total_loss: 5.2283477783203125
training step: 43743, total_loss: 5.38177490234375
training step: 43744, total_loss: 0.5018255114555359
training step: 43745, total_loss: 0.3592761158943176
training step: 43746, total_loss: 1.610459804534912
training step: 43747, total_loss: 0.7320116758346558
training step: 43748, total_loss: 0.8222039341926575
training step: 43749, total_loss: 3.1198768615722656
training step: 43750, total_loss: 0.9699001312255859
training step: 43751, total_loss: 2.3224053382873535
training step: 43752, total_loss: 1.9352951049804688
training step: 43753, total_loss: 0.10254033654928207
training step: 43754, total_loss: 2.718179702758789
training step: 43755, total_loss: 0.17926687002182007
training step: 43756, total_loss: 1.0208477973937988
training step: 43757, total_loss: 4.110943794250488
training step: 43758, total_loss: 0.5239132046699524
training step: 43759, total_loss: 3.1881442070007324
training step: 43760, total_loss: 2.3758788108825684
training step: 43761, total_loss: 2.8526697158813477
training step: 43762, total_loss: 4.343468189239502
training step: 43763, total_loss: 2.002772569656372
training step: 43764, total_loss: 1.0527994632720947
training step: 43765, total_loss: 1.5619518756866455
training step: 43766, total_loss: 3.6646485328674316
training step: 43767, total_loss: 1.756474256515503
training step: 43768, total_loss: 2.8219475746154785
training step: 43769, total_loss: 3.389314651489258
training step: 43770, total_loss: 2.5877017974853516
training step: 43771, total_loss: 1.3265782594680786
training step: 43772, total_loss: 2.327519416809082
training step: 43773, total_loss: 0.9590435028076172
training step: 43774, total_loss: 1.8891977071762085
training step: 43775, total_loss: 0.1930682510137558
training step: 43776, total_loss: 0.4688139259815216
training step: 43777, total_loss: 0.06649105250835419
training step: 43778, total_loss: 0.850945770740509
training step: 43779, total_loss: 3.060567855834961
training step: 43780, total_loss: 2.805250644683838
training step: 43781, total_loss: 1.6261000633239746
training step: 43782, total_loss: 2.692180871963501
training step: 43783, total_loss: 4.816026210784912
training step: 43784, total_loss: 1.2209017276763916
training step: 43785, total_loss: 1.897015929222107
training step: 43786, total_loss: 1.5036165714263916
training step: 43787, total_loss: 3.7710602283477783
training step: 43788, total_loss: 0.9681535959243774
training step: 43789, total_loss: 1.746124267578125
training step: 43790, total_loss: 2.410257339477539
training step: 43791, total_loss: 0.7666048407554626
training step: 43792, total_loss: 0.0016499687917530537
training step: 43793, total_loss: 0.7980366945266724
training step: 43794, total_loss: 0.7043903470039368
training step: 43795, total_loss: 1.0972291231155396
training step: 43796, total_loss: 1.9471399784088135
training step: 43797, total_loss: 0.26188862323760986
training step: 43798, total_loss: 2.848001718521118
training step: 43799, total_loss: 1.671302318572998
training step: 43800, total_loss: 0.4648149609565735
training step: 43801, total_loss: 3.644610643386841
training step: 43802, total_loss: 2.021470785140991
training step: 43803, total_loss: 0.630579948425293
training step: 43804, total_loss: 1.2908722162246704
training step: 43805, total_loss: 1.026186466217041
training step: 43806, total_loss: 1.5392869710922241
training step: 43807, total_loss: 1.3781400918960571
training step: 43808, total_loss: 1.868390679359436
training step: 43809, total_loss: 1.0678917169570923
training step: 43810, total_loss: 0.7425212860107422
training step: 43811, total_loss: 1.8009575605392456
training step: 43812, total_loss: 1.7984552383422852
training step: 43813, total_loss: 1.2130155563354492
training step: 43814, total_loss: 2.974417209625244
training step: 43815, total_loss: 2.5135018825531006
training step: 43816, total_loss: 1.33918297290802
training step: 43817, total_loss: 0.19883641600608826
training step: 43818, total_loss: 1.8184645175933838
training step: 43819, total_loss: 4.303441524505615
training step: 43820, total_loss: 3.713548183441162
training step: 43821, total_loss: 2.292693853378296
training step: 43822, total_loss: 1.4686009883880615
training step: 43823, total_loss: 0.861735463142395
training step: 43824, total_loss: 2.834007501602173
training step: 43825, total_loss: 2.7100844383239746
training step: 43826, total_loss: 0.723055899143219
training step: 43827, total_loss: 1.9507981538772583
training step: 43828, total_loss: 0.9170801639556885
training step: 43829, total_loss: 4.216259956359863
training step: 43830, total_loss: 5.138782501220703
training step: 43831, total_loss: 1.5640790462493896
training step: 43832, total_loss: 1.8799402713775635
training step: 43833, total_loss: 0.9125639796257019
training step: 43834, total_loss: 2.5874409675598145
training step: 43835, total_loss: 0.24452662467956543
training step: 43836, total_loss: 0.05830786004662514
training step: 43837, total_loss: 0.015861911699175835
training step: 43838, total_loss: 2.77956223487854
training step: 43839, total_loss: 0.4346836805343628
training step: 43840, total_loss: 0.3810974359512329
training step: 43841, total_loss: 0.7958091497421265
training step: 43842, total_loss: 0.9432475566864014
training step: 43843, total_loss: 1.9538843631744385
training step: 43844, total_loss: 0.018020160496234894
training step: 43845, total_loss: 2.0252861976623535
training step: 43846, total_loss: 2.666083812713623
training step: 43847, total_loss: 0.5636047720909119
training step: 43848, total_loss: 2.781379461288452
training step: 43849, total_loss: 0.5330080986022949
training step: 43850, total_loss: 0.007609146647155285
training step: 43851, total_loss: 2.5935871601104736
training step: 43852, total_loss: 1.3575489521026611
training step: 43853, total_loss: 0.8562752604484558
training step: 43854, total_loss: 9.650123596191406
training step: 43855, total_loss: 1.0677177906036377
training step: 43856, total_loss: 1.3418368101119995
training step: 43857, total_loss: 1.3679780960083008
training step: 43858, total_loss: 1.769243836402893
training step: 43859, total_loss: 0.010346599854528904
training step: 43860, total_loss: 0.5863445401191711
training step: 43861, total_loss: 0.812916100025177
training step: 43862, total_loss: 2.0145885944366455
training step: 43863, total_loss: 2.5611066818237305
training step: 43864, total_loss: 0.694830060005188
training step: 43865, total_loss: 2.2332863807678223
training step: 43866, total_loss: 2.183563232421875
training step: 43867, total_loss: 2.2727293968200684
training step: 43868, total_loss: 4.132778167724609INFO:tensorflow:Writing predictions to: test_output/predictions_44000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_44000.json

training step: 43869, total_loss: 2.972216844558716
training step: 43870, total_loss: 0.658225953578949
training step: 43871, total_loss: 2.1922059059143066
training step: 43872, total_loss: 2.155134916305542
training step: 43873, total_loss: 1.0486538410186768
training step: 43874, total_loss: 2.495142936706543
training step: 43875, total_loss: 2.032982110977173
training step: 43876, total_loss: 0.7957335114479065
training step: 43877, total_loss: 4.773577690124512
training step: 43878, total_loss: 2.0032756328582764
training step: 43879, total_loss: 0.919674277305603
training step: 43880, total_loss: 1.7784088850021362
training step: 43881, total_loss: 0.9395557641983032
training step: 43882, total_loss: 0.22052910923957825
training step: 43883, total_loss: 1.9411226511001587
training step: 43884, total_loss: 2.380551815032959
training step: 43885, total_loss: 1.6236968040466309
training step: 43886, total_loss: 1.752565860748291
training step: 43887, total_loss: 1.371239185333252
training step: 43888, total_loss: 0.4860427975654602
training step: 43889, total_loss: 0.9007066488265991
training step: 43890, total_loss: 1.6231456995010376
training step: 43891, total_loss: 0.9927809834480286
training step: 43892, total_loss: 0.8645644187927246
training step: 43893, total_loss: 0.995661199092865
training step: 43894, total_loss: 0.004543950315564871
training step: 43895, total_loss: 1.386744499206543
training step: 43896, total_loss: 0.7637826204299927
training step: 43897, total_loss: 2.903010129928589
training step: 43898, total_loss: 0.9905902147293091
training step: 43899, total_loss: 2.417738914489746
training step: 43900, total_loss: 4.714813709259033
training step: 43901, total_loss: 1.1959998607635498
training step: 43902, total_loss: 1.0686798095703125
training step: 43903, total_loss: 1.5555434226989746
training step: 43904, total_loss: 1.412703514099121
training step: 43905, total_loss: 1.6469913721084595
training step: 43906, total_loss: 1.6324958801269531
training step: 43907, total_loss: 3.626826763153076
training step: 43908, total_loss: 1.0117477178573608
training step: 43909, total_loss: 0.3862996995449066
training step: 43910, total_loss: 2.367039680480957
training step: 43911, total_loss: 2.8142266273498535
training step: 43912, total_loss: 2.4390599727630615
training step: 43913, total_loss: 7.477485656738281
training step: 43914, total_loss: 1.3772364854812622
training step: 43915, total_loss: 2.082275390625
training step: 43916, total_loss: 0.04975004494190216
training step: 43917, total_loss: 0.29407501220703125
training step: 43918, total_loss: 2.994509220123291
training step: 43919, total_loss: 2.0543832778930664
training step: 43920, total_loss: 1.4067625999450684
training step: 43921, total_loss: 1.4487167596817017
training step: 43922, total_loss: 3.4639594554901123
training step: 43923, total_loss: 1.3073686361312866
training step: 43924, total_loss: 0.98359215259552
training step: 43925, total_loss: 0.7603200674057007
training step: 43926, total_loss: 0.9156092405319214
training step: 43927, total_loss: 3.1784658432006836
training step: 43928, total_loss: 2.2624716758728027
training step: 43929, total_loss: 2.0836071968078613
training step: 43930, total_loss: 1.212960958480835
training step: 43931, total_loss: 1.4100050926208496
training step: 43932, total_loss: 2.856013298034668
training step: 43933, total_loss: 0.16518178582191467
training step: 43934, total_loss: 3.07364559173584
training step: 43935, total_loss: 2.8734283447265625
training step: 43936, total_loss: 2.1881659030914307
training step: 43937, total_loss: 1.125443696975708
training step: 43938, total_loss: 2.847353219985962
training step: 43939, total_loss: 0.9717465043067932
training step: 43940, total_loss: 1.2892310619354248
training step: 43941, total_loss: 1.451910376548767
training step: 43942, total_loss: 0.8269084095954895
training step: 43943, total_loss: 1.6497328281402588
training step: 43944, total_loss: 2.64497971534729
training step: 43945, total_loss: 2.1079492568969727
training step: 43946, total_loss: 0.17611882090568542
training step: 43947, total_loss: 0.8203473091125488
training step: 43948, total_loss: 2.715151309967041
training step: 43949, total_loss: 2.614250659942627
training step: 43950, total_loss: 3.08748197555542
training step: 43951, total_loss: 1.020578145980835
training step: 43952, total_loss: 1.9514081478118896
training step: 43953, total_loss: 0.6966968774795532
training step: 43954, total_loss: 1.472167730331421
training step: 43955, total_loss: 2.165074586868286
training step: 43956, total_loss: 0.05527949333190918
training step: 43957, total_loss: 1.1186482906341553
training step: 43958, total_loss: 2.760629653930664
training step: 43959, total_loss: 1.4564005136489868
training step: 43960, total_loss: 0.31415659189224243
training step: 43961, total_loss: 2.2380177974700928
training step: 43962, total_loss: 1.8406463861465454
training step: 43963, total_loss: 6.8104448318481445
training step: 43964, total_loss: 1.2036914825439453
training step: 43965, total_loss: 3.3434925079345703
training step: 43966, total_loss: 1.0394102334976196
training step: 43967, total_loss: 1.0478053092956543
training step: 43968, total_loss: 3.0132524967193604
training step: 43969, total_loss: 1.1646091938018799
training step: 43970, total_loss: 1.606637954711914
training step: 43971, total_loss: 0.2804465889930725
training step: 43972, total_loss: 2.6429805755615234
training step: 43973, total_loss: 0.7544034719467163
training step: 43974, total_loss: 2.6364636421203613
training step: 43975, total_loss: 2.9604568481445312
training step: 43976, total_loss: 4.779500961303711
training step: 43977, total_loss: 0.37815988063812256
training step: 43978, total_loss: 2.5206282138824463
training step: 43979, total_loss: 0.7650247812271118
training step: 43980, total_loss: 0.8313422203063965
training step: 43981, total_loss: 1.2001310586929321
training step: 43982, total_loss: 2.4161629676818848
training step: 43983, total_loss: 0.6597972512245178
training step: 43984, total_loss: 4.037744522094727
training step: 43985, total_loss: 1.5393357276916504
training step: 43986, total_loss: 1.9874677658081055
training step: 43987, total_loss: 4.369787216186523
training step: 43988, total_loss: 3.0025863647460938
training step: 43989, total_loss: 0.42704617977142334
training step: 43990, total_loss: 0.9100269079208374
training step: 43991, total_loss: 4.139348030090332
training step: 43992, total_loss: 1.824753761291504
training step: 43993, total_loss: 2.631307363510132
training step: 43994, total_loss: 1.2190766334533691
training step: 43995, total_loss: 1.580488920211792
training step: 43996, total_loss: 1.7012152671813965
training step: 43997, total_loss: 1.7492413520812988
training step: 43998, total_loss: 2.7396605014801025
training step: 43999, total_loss: 0.2088533341884613
training step: 44000, total_loss: 2.062761068344116
epoch finished! shuffle=False
evaluation: 8000, total_loss: 1.8369704484939575, f1: 55.65560372064389, followup: 18.910695268522744, yesno: 78.50296668188042, heq: 49.87068309752016, dheq: 3.6

Model saved in path test_output//model_44000.ckpt
training step: 44001, total_loss: 4.6566691398620605
training step: 44002, total_loss: 3.005404472351074
training step: 44003, total_loss: 2.3733444213867188
training step: 44004, total_loss: 0.815780520439148
training step: 44005, total_loss: 2.4096531867980957
training step: 44006, total_loss: 2.1121044158935547
training step: 44007, total_loss: 0.7892447710037231
training step: 44008, total_loss: 2.3199832439422607
training step: 44009, total_loss: 1.6088941097259521
training step: 44010, total_loss: 2.047112226486206
training step: 44011, total_loss: 2.528754949569702
training step: 44012, total_loss: 0.41373947262763977
training step: 44013, total_loss: 1.5249124765396118
training step: 44014, total_loss: 1.3211625814437866
training step: 44015, total_loss: 1.6787161827087402
training step: 44016, total_loss: 1.234127163887024
training step: 44017, total_loss: 0.1356354057788849
training step: 44018, total_loss: 1.119227647781372
training step: 44019, total_loss: 1.6927266120910645
training step: 44020, total_loss: 2.216184616088867
training step: 44021, total_loss: 4.534798622131348
training step: 44022, total_loss: 1.4318429231643677
training step: 44023, total_loss: 1.611609697341919
training step: 44024, total_loss: 2.86518931388855
training step: 44025, total_loss: 0.760838508605957
training step: 44026, total_loss: 2.3872671127319336
training step: 44027, total_loss: 1.9696201086044312
training step: 44028, total_loss: 4.477651119232178
training step: 44029, total_loss: 2.616483688354492
training step: 44030, total_loss: 1.2783329486846924
training step: 44031, total_loss: 1.1313188076019287
training step: 44032, total_loss: 0.8895478844642639
training step: 44033, total_loss: 1.8828767538070679
training step: 44034, total_loss: 2.59264874458313
training step: 44035, total_loss: 3.0146775245666504
training step: 44036, total_loss: 0.159933939576149
training step: 44037, total_loss: 1.5273380279541016
training step: 44038, total_loss: 2.4301891326904297
training step: 44039, total_loss: 2.256753444671631
training step: 44040, total_loss: 0.8256065249443054
training step: 44041, total_loss: 1.2953290939331055
training step: 44042, total_loss: 1.2105467319488525
training step: 44043, total_loss: 4.040938377380371
training step: 44044, total_loss: 0.4145396947860718
training step: 44045, total_loss: 0.6797081828117371
training step: 44046, total_loss: 0.45576462149620056
training step: 44047, total_loss: 0.491203248500824
training step: 44048, total_loss: 2.2514138221740723
training step: 44049, total_loss: 2.3294410705566406
training step: 44050, total_loss: 2.120210647583008
training step: 44051, total_loss: 2.725306987762451
training step: 44052, total_loss: 0.6356331706047058
training step: 44053, total_loss: 0.8232813477516174
training step: 44054, total_loss: 0.9382807016372681
training step: 44055, total_loss: 2.3595407009124756
training step: 44056, total_loss: 0.5982058644294739
training step: 44057, total_loss: 2.956120252609253
training step: 44058, total_loss: 2.7296295166015625
training step: 44059, total_loss: 2.222799062728882
training step: 44060, total_loss: 1.7211413383483887
training step: 44061, total_loss: 1.04652738571167
training step: 44062, total_loss: 3.089171886444092
training step: 44063, total_loss: 3.3527510166168213
training step: 44064, total_loss: 2.743164300918579
training step: 44065, total_loss: 0.45861223340034485
training step: 44066, total_loss: 1.5962231159210205
training step: 44067, total_loss: 1.9289557933807373
training step: 44068, total_loss: 2.487494468688965
training step: 44069, total_loss: 1.0928645133972168
training step: 44070, total_loss: 1.4862747192382812
training step: 44071, total_loss: 0.9533709287643433
training step: 44072, total_loss: 1.1349000930786133
training step: 44073, total_loss: 0.5727705359458923
training step: 44074, total_loss: 2.843083620071411
training step: 44075, total_loss: 1.2827446460723877
training step: 44076, total_loss: 2.504054069519043
training step: 44077, total_loss: 3.7837395668029785
training step: 44078, total_loss: 0.6408329010009766
training step: 44079, total_loss: 2.8334596157073975
training step: 44080, total_loss: 3.479429244995117
training step: 44081, total_loss: 1.550484299659729
training step: 44082, total_loss: 0.8860158920288086
training step: 44083, total_loss: 1.7428003549575806
training step: 44084, total_loss: 0.5611748695373535
training step: 44085, total_loss: 1.0720571279525757
training step: 44086, total_loss: 0.6641247272491455
training step: 44087, total_loss: 2.092787027359009
training step: 44088, total_loss: 1.4685063362121582
training step: 44089, total_loss: 0.6657195687294006
training step: 44090, total_loss: 1.995335340499878
training step: 44091, total_loss: 1.4290423393249512
training step: 44092, total_loss: 1.934769630432129
training step: 44093, total_loss: 0.3362092673778534
training step: 44094, total_loss: 0.7037171125411987
training step: 44095, total_loss: 1.1504207849502563
training step: 44096, total_loss: 2.1040523052215576
training step: 44097, total_loss: 2.8345608711242676
training step: 44098, total_loss: 2.1810710430145264
training step: 44099, total_loss: 1.358670711517334
training step: 44100, total_loss: 2.0595810413360596
training step: 44101, total_loss: 2.4508683681488037
training step: 44102, total_loss: 1.1815334558486938
training step: 44103, total_loss: 0.9432518482208252
training step: 44104, total_loss: 3.145319938659668
training step: 44105, total_loss: 2.4947686195373535
training step: 44106, total_loss: 0.6840292811393738
training step: 44107, total_loss: 3.5792174339294434
training step: 44108, total_loss: 0.895615816116333
training step: 44109, total_loss: 0.3507300615310669
training step: 44110, total_loss: 0.31444841623306274
training step: 44111, total_loss: 0.5579141974449158
training step: 44112, total_loss: 2.251469850540161
training step: 44113, total_loss: 1.1525306701660156
training step: 44114, total_loss: 0.2964226007461548
training step: 44115, total_loss: 1.0876765251159668
training step: 44116, total_loss: 2.565037727355957
training step: 44117, total_loss: 1.70652174949646
training step: 44118, total_loss: 3.19038724899292
training step: 44119, total_loss: 2.9519145488739014
training step: 44120, total_loss: 4.576050758361816
training step: 44121, total_loss: 1.6469862461090088
training step: 44122, total_loss: 4.0933332443237305
training step: 44123, total_loss: 2.323133945465088
training step: 44124, total_loss: 0.8792439699172974
training step: 44125, total_loss: 0.22305524349212646
training step: 44126, total_loss: 0.7632335424423218
training step: 44127, total_loss: 2.8682470321655273
training step: 44128, total_loss: 1.287386178970337
training step: 44129, total_loss: 3.066467761993408
training step: 44130, total_loss: 0.25296586751937866
training step: 44131, total_loss: 0.11147597432136536
training step: 44132, total_loss: 0.4072684049606323
training step: 44133, total_loss: 0.5597155094146729
training step: 44134, total_loss: 2.39103102684021
training step: 44135, total_loss: 0.9754258394241333
training step: 44136, total_loss: 1.4204471111297607
training step: 44137, total_loss: 1.0038321018218994
training step: 44138, total_loss: 3.593393325805664
training step: 44139, total_loss: 0.43352290987968445
training step: 44140, total_loss: 2.2001988887786865
training step: 44141, total_loss: 0.12431178987026215
training step: 44142, total_loss: 0.7168102860450745
training step: 44143, total_loss: 2.096421957015991
training step: 44144, total_loss: 2.458848237991333
training step: 44145, total_loss: 4.087355613708496
training step: 44146, total_loss: 2.9689087867736816
training step: 44147, total_loss: 4.665122032165527
training step: 44148, total_loss: 3.027576208114624
training step: 44149, total_loss: 0.4673108458518982
training step: 44150, total_loss: 4.113852024078369
training step: 44151, total_loss: 1.6792232990264893
training step: 44152, total_loss: 1.2264630794525146
training step: 44153, total_loss: 0.3542260527610779
training step: 44154, total_loss: 0.06064284220337868
training step: 44155, total_loss: 0.8991581201553345
training step: 44156, total_loss: 1.7028186321258545
training step: 44157, total_loss: 5.867305755615234
training step: 44158, total_loss: 0.8830741047859192
training step: 44159, total_loss: 1.2882837057113647
training step: 44160, total_loss: 3.4214558601379395
training step: 44161, total_loss: 1.8869582414627075
training step: 44162, total_loss: 3.4012928009033203
training step: 44163, total_loss: 4.396198272705078
training step: 44164, total_loss: 0.25278225541114807
training step: 44165, total_loss: 0.8739194273948669
training step: 44166, total_loss: 1.5218793153762817
training step: 44167, total_loss: 1.8558616638183594
training step: 44168, total_loss: 1.0026884078979492
training step: 44169, total_loss: 0.37498852610588074
training step: 44170, total_loss: 3.019658327102661
training step: 44171, total_loss: 1.926332950592041
training step: 44172, total_loss: 1.4065096378326416
training step: 44173, total_loss: 3.275651693344116
training step: 44174, total_loss: 2.859743595123291
training step: 44175, total_loss: 2.8187859058380127
training step: 44176, total_loss: 0.38695645332336426
training step: 44177, total_loss: 2.6740245819091797
training step: 44178, total_loss: 1.9465404748916626
training step: 44179, total_loss: 0.4978661835193634
training step: 44180, total_loss: 4.326040267944336
training step: 44181, total_loss: 0.8957295417785645
training step: 44182, total_loss: 0.050581078976392746
training step: 44183, total_loss: 1.3181512355804443
training step: 44184, total_loss: 1.536421537399292
training step: 44185, total_loss: 0.21227076649665833
training step: 44186, total_loss: 0.9750268459320068
training step: 44187, total_loss: 1.6689059734344482
training step: 44188, total_loss: 1.677067756652832
training step: 44189, total_loss: 0.7546015977859497
training step: 44190, total_loss: 1.5806856155395508
training step: 44191, total_loss: 2.8180623054504395
training step: 44192, total_loss: 1.9225986003875732
training step: 44193, total_loss: 0.7878546714782715
training step: 44194, total_loss: 1.0880863666534424
training step: 44195, total_loss: 0.6903632879257202
training step: 44196, total_loss: 1.8308801651000977
training step: 44197, total_loss: 2.614973783493042
training step: 44198, total_loss: 4.139542579650879
training step: 44199, total_loss: 1.7314553260803223
training step: 44200, total_loss: 0.1569277048110962
training step: 44201, total_loss: 0.6308897733688354
training step: 44202, total_loss: 0.922682523727417
training step: 44203, total_loss: 1.6206445693969727
training step: 44204, total_loss: 1.4048669338226318
training step: 44205, total_loss: 1.53165602684021
training step: 44206, total_loss: 0.7521688938140869
training step: 44207, total_loss: 2.5123448371887207
training step: 44208, total_loss: 1.2982844114303589
training step: 44209, total_loss: 2.343609094619751
training step: 44210, total_loss: 2.764662027359009
training step: 44211, total_loss: 0.34327852725982666
training step: 44212, total_loss: 1.9090521335601807
training step: 44213, total_loss: 0.0023115212097764015
training step: 44214, total_loss: 0.6838148236274719
training step: 44215, total_loss: 4.011041641235352
training step: 44216, total_loss: 1.4910707473754883
training step: 44217, total_loss: 0.21598395705223083
training step: 44218, total_loss: 1.930361032485962
training step: 44219, total_loss: 3.7196595668792725
training step: 44220, total_loss: 0.8046619892120361
training step: 44221, total_loss: 3.24568510055542
training step: 44222, total_loss: 0.38037335872650146
training step: 44223, total_loss: 0.5524840354919434
training step: 44224, total_loss: 1.5876495838165283
training step: 44225, total_loss: 1.7458744049072266
training step: 44226, total_loss: 0.07366922497749329
training step: 44227, total_loss: 3.0377089977264404
training step: 44228, total_loss: 0.5332411527633667
training step: 44229, total_loss: 0.5447664260864258
training step: 44230, total_loss: 1.1497414112091064
training step: 44231, total_loss: 1.3756980895996094
training step: 44232, total_loss: 1.9861319065093994
training step: 44233, total_loss: 0.09975843876600266
training step: 44234, total_loss: 0.8709980845451355
training step: 44235, total_loss: 4.231781959533691
training step: 44236, total_loss: 1.7411370277404785
training step: 44237, total_loss: 2.2593796253204346
training step: 44238, total_loss: 1.0084911584854126
training step: 44239, total_loss: 0.31216880679130554
training step: 44240, total_loss: 0.39627957344055176
training step: 44241, total_loss: 0.08442471921443939
training step: 44242, total_loss: 0.00831498485058546
training step: 44243, total_loss: 3.8091206550598145
training step: 44244, total_loss: 0.3669759929180145
training step: 44245, total_loss: 1.0338430404663086
training step: 44246, total_loss: 2.131791591644287
training step: 44247, total_loss: 1.6016261577606201
training step: 44248, total_loss: 1.2326693534851074
training step: 44249, total_loss: 0.656458854675293
training step: 44250, total_loss: 1.8732270002365112
training step: 44251, total_loss: 1.3845250606536865
training step: 44252, total_loss: 3.33821964263916
training step: 44253, total_loss: 2.120948553085327
training step: 44254, total_loss: 0.1567486971616745
training step: 44255, total_loss: 3.4879696369171143
training step: 44256, total_loss: 4.899967193603516
training step: 44257, total_loss: 1.4485135078430176
training step: 44258, total_loss: 3.1868906021118164
training step: 44259, total_loss: 1.6122301816940308
training step: 44260, total_loss: 2.4287633895874023
training step: 44261, total_loss: 1.4743564128875732
training step: 44262, total_loss: 2.2656326293945312
training step: 44263, total_loss: 2.0079286098480225
training step: 44264, total_loss: 2.4545016288757324
training step: 44265, total_loss: 3.157707691192627
training step: 44266, total_loss: 4.411656379699707
training step: 44267, total_loss: 0.0010728550842031837
training step: 44268, total_loss: 0.7695972919464111
training step: 44269, total_loss: 2.4981093406677246
training step: 44270, total_loss: 0.01874045841395855
training step: 44271, total_loss: 0.33266037702560425
training step: 44272, total_loss: 1.04997718334198
training step: 44273, total_loss: 0.8429250717163086
training step: 44274, total_loss: 5.513077735900879
training step: 44275, total_loss: 0.5780140161514282
training step: 44276, total_loss: 0.317416250705719
training step: 44277, total_loss: 0.7413821816444397
training step: 44278, total_loss: 1.8780522346496582
training step: 44279, total_loss: 3.047292709350586
training step: 44280, total_loss: 2.5123867988586426
training step: 44281, total_loss: 1.442705750465393
training step: 44282, total_loss: 2.3307361602783203
training step: 44283, total_loss: 1.0798912048339844
training step: 44284, total_loss: 0.5777953267097473
training step: 44285, total_loss: 2.257810115814209
training step: 44286, total_loss: 2.934786796569824
training step: 44287, total_loss: 1.4233473539352417
training step: 44288, total_loss: 0.9258596897125244
training step: 44289, total_loss: 0.7314785718917847
training step: 44290, total_loss: 3.423508644104004
training step: 44291, total_loss: 1.4474462270736694
training step: 44292, total_loss: 3.9058711528778076
training step: 44293, total_loss: 1.2843828201293945
training step: 44294, total_loss: 1.043684720993042
training step: 44295, total_loss: 0.004011760000139475
training step: 44296, total_loss: 4.773625373840332
training step: 44297, total_loss: 2.818708896636963
training step: 44298, total_loss: 1.4800288677215576
training step: 44299, total_loss: 2.957704544067383
training step: 44300, total_loss: 3.060360908508301
training step: 44301, total_loss: 1.4699208736419678
training step: 44302, total_loss: 2.212425708770752
training step: 44303, total_loss: 1.4628435373306274
training step: 44304, total_loss: 1.8477966785430908
training step: 44305, total_loss: 1.9128650426864624
training step: 44306, total_loss: 2.75298810005188
training step: 44307, total_loss: 2.5097103118896484
training step: 44308, total_loss: 2.567282199859619
training step: 44309, total_loss: 1.465444564819336
training step: 44310, total_loss: 0.6110443472862244
training step: 44311, total_loss: 0.14063067734241486
training step: 44312, total_loss: 1.4935194253921509
training step: 44313, total_loss: 3.094921112060547
training step: 44314, total_loss: 4.263752460479736
training step: 44315, total_loss: 3.4638094902038574
training step: 44316, total_loss: 2.1374459266662598
training step: 44317, total_loss: 1.546199083328247
training step: 44318, total_loss: 1.0468478202819824
training step: 44319, total_loss: 3.206986427307129
training step: 44320, total_loss: 1.0343855619430542
training step: 44321, total_loss: 6.303774356842041
training step: 44322, total_loss: 2.529499053955078
training step: 44323, total_loss: 1.0250911712646484
training step: 44324, total_loss: 2.0717968940734863
training step: 44325, total_loss: 1.5415635108947754
training step: 44326, total_loss: 1.6701246500015259
training step: 44327, total_loss: 1.5065433979034424
training step: 44328, total_loss: 2.4134609699249268
training step: 44329, total_loss: 2.5653650760650635
training step: 44330, total_loss: 2.0372474193573
training step: 44331, total_loss: 2.3950562477111816
training step: 44332, total_loss: 3.1809463500976562
training step: 44333, total_loss: 1.4804936647415161
training step: 44334, total_loss: 1.7523815631866455
training step: 44335, total_loss: 2.343873977661133
training step: 44336, total_loss: 0.22780103981494904
training step: 44337, total_loss: 0.3229496479034424
training step: 44338, total_loss: 1.2921133041381836
training step: 44339, total_loss: 1.1801800727844238
training step: 44340, total_loss: 1.3185322284698486
training step: 44341, total_loss: 0.9094480276107788
training step: 44342, total_loss: 2.353968620300293
training step: 44343, total_loss: 2.266018867492676
training step: 44344, total_loss: 1.4349336624145508
training step: 44345, total_loss: 2.6815977096557617
training step: 44346, total_loss: 3.872089385986328
training step: 44347, total_loss: 1.4311652183532715
training step: 44348, total_loss: 0.012321248650550842
training step: 44349, total_loss: 3.858582019805908
training step: 44350, total_loss: 2.7380013465881348
training step: 44351, total_loss: 2.2689809799194336
training step: 44352, total_loss: 2.386774778366089
training step: 44353, total_loss: 1.0615464448928833
training step: 44354, total_loss: 1.112381100654602
training step: 44355, total_loss: 1.3270366191864014
training step: 44356, total_loss: 1.3634315729141235
training step: 44357, total_loss: 6.619781494140625
training step: 44358, total_loss: 1.2342653274536133
training step: 44359, total_loss: 1.7837287187576294
training step: 44360, total_loss: 3.2824487686157227
training step: 44361, total_loss: 1.9996347427368164
training step: 44362, total_loss: 0.5426716208457947
training step: 44363, total_loss: 0.8184040784835815
training step: 44364, total_loss: 1.5626041889190674
training step: 44365, total_loss: 5.722873210906982
training step: 44366, total_loss: 2.598761558532715
training step: 44367, total_loss: 2.349155902862549
training step: 44368, total_loss: 1.3370572328567505
training step: 44369, total_loss: 3.255755662918091
training step: 44370, total_loss: 0.917717456817627
training step: 44371, total_loss: 1.7023621797561646
training step: 44372, total_loss: 2.101438283920288
training step: 44373, total_loss: 0.8592803478240967
training step: 44374, total_loss: 2.1626410484313965
training step: 44375, total_loss: 0.27423638105392456
training step: 44376, total_loss: 0.825000524520874
training step: 44377, total_loss: 2.116773843765259
training step: 44378, total_loss: 2.656693935394287
training step: 44379, total_loss: 0.750475287437439
training step: 44380, total_loss: 0.8554291725158691
training step: 44381, total_loss: 1.5425883531570435
training step: 44382, total_loss: 0.16366268694400787
training step: 44383, total_loss: 0.27360403537750244
training step: 44384, total_loss: 0.07410934567451477
training step: 44385, total_loss: 1.2581866979599
training step: 44386, total_loss: 1.2486655712127686
training step: 44387, total_loss: 3.9473602771759033
training step: 44388, total_loss: 0.6703898906707764
training step: 44389, total_loss: 1.8065707683563232
training step: 44390, total_loss: 1.623523473739624
training step: 44391, total_loss: 0.11307201534509659
training step: 44392, total_loss: 4.515549659729004
training step: 44393, total_loss: 0.6778225898742676
training step: 44394, total_loss: 1.5668189525604248
training step: 44395, total_loss: 2.7061285972595215
training step: 44396, total_loss: 0.7418937087059021
training step: 44397, total_loss: 1.7616393566131592
training step: 44398, total_loss: 2.314060688018799
training step: 44399, total_loss: 1.0585551261901855
training step: 44400, total_loss: 2.102290153503418
training step: 44401, total_loss: 0.2925576865673065
training step: 44402, total_loss: 0.017559390515089035
training step: 44403, total_loss: 0.8290097713470459
training step: 44404, total_loss: 2.7000553607940674
training step: 44405, total_loss: 3.1098761558532715
training step: 44406, total_loss: 2.3673129081726074
training step: 44407, total_loss: 0.2778605818748474
training step: 44408, total_loss: 0.6235532760620117
training step: 44409, total_loss: 1.511791706085205
training step: 44410, total_loss: 1.8346142768859863
training step: 44411, total_loss: 1.396372675895691
training step: 44412, total_loss: 2.2503411769866943
training step: 44413, total_loss: 0.20250698924064636
training step: 44414, total_loss: 1.5837278366088867
training step: 44415, total_loss: 2.2702856063842773
training step: 44416, total_loss: 2.6026408672332764
training step: 44417, total_loss: 0.030713772401213646
training step: 44418, total_loss: 2.1952261924743652
training step: 44419, total_loss: 1.4234455823898315
training step: 44420, total_loss: 0.8113664388656616
training step: 44421, total_loss: 2.7714614868164062
training step: 44422, total_loss: 1.2522554397583008
training step: 44423, total_loss: 5.177053451538086
training step: 44424, total_loss: 2.3073997497558594
training step: 44425, total_loss: 3.8216090202331543
training step: 44426, total_loss: 0.5980558395385742
training step: 44427, total_loss: 1.446986198425293
training step: 44428, total_loss: 1.2900606393814087
training step: 44429, total_loss: 1.3417181968688965
training step: 44430, total_loss: 2.132333993911743
training step: 44431, total_loss: 1.3322086334228516
training step: 44432, total_loss: 0.23285380005836487
training step: 44433, total_loss: 2.1675939559936523
training step: 44434, total_loss: 2.6715705394744873
training step: 44435, total_loss: 2.290142774581909
training step: 44436, total_loss: 2.6153979301452637
training step: 44437, total_loss: 0.5028209686279297
training step: 44438, total_loss: 2.125617265701294
training step: 44439, total_loss: 1.7360317707061768
training step: 44440, total_loss: 2.352869987487793
training step: 44441, total_loss: 0.5151247382164001
training step: 44442, total_loss: 0.7166186571121216
training step: 44443, total_loss: 1.443582534790039
training step: 44444, total_loss: 2.0382513999938965
training step: 44445, total_loss: 0.18891704082489014
training step: 44446, total_loss: 4.1720123291015625
training step: 44447, total_loss: 2.3932151794433594
training step: 44448, total_loss: 3.6924221515655518
training step: 44449, total_loss: 1.6820428371429443
training step: 44450, total_loss: 3.3479042053222656
training step: 44451, total_loss: 0.3402132987976074
training step: 44452, total_loss: 3.006808280944824
training step: 44453, total_loss: 0.062268055975437164
training step: 44454, total_loss: 1.0559446811676025
training step: 44455, total_loss: 1.3646972179412842
training step: 44456, total_loss: 2.095703363418579
training step: 44457, total_loss: 1.1663246154785156
training step: 44458, total_loss: 3.3123881816864014
training step: 44459, total_loss: 0.9957460165023804
training step: 44460, total_loss: 0.5251904726028442
training step: 44461, total_loss: 1.6994829177856445
training step: 44462, total_loss: 0.14465445280075073
training step: 44463, total_loss: 0.4180331826210022
training step: 44464, total_loss: 1.9036927223205566
training step: 44465, total_loss: 1.7678778171539307
training step: 44466, total_loss: 0.6662733554840088
training step: 44467, total_loss: 2.512580394744873
training step: 44468, total_loss: 1.2560999393463135
training step: 44469, total_loss: 3.9050819873809814
training step: 44470, total_loss: 5.489772319793701
training step: 44471, total_loss: 1.2356510162353516
training step: 44472, total_loss: 3.1768975257873535
training step: 44473, total_loss: 1.1595945358276367
training step: 44474, total_loss: 2.966864585876465
training step: 44475, total_loss: 1.4711788892745972
training step: 44476, total_loss: 0.9158499240875244
training step: 44477, total_loss: 0.8889117240905762
training step: 44478, total_loss: 0.3257977366447449
training step: 44479, total_loss: 2.1836564540863037
training step: 44480, total_loss: 2.106119155883789
training step: 44481, total_loss: 0.5001562833786011
training step: 44482, total_loss: 1.1352956295013428
training step: 44483, total_loss: 3.4666361808776855
training step: 44484, total_loss: 2.3419110774993896
training step: 44485, total_loss: 2.54769229888916
training step: 44486, total_loss: 1.1938810348510742
training step: 44487, total_loss: 0.9194542169570923
training step: 44488, total_loss: 1.4074459075927734
training step: 44489, total_loss: 2.2593772411346436
training step: 44490, total_loss: 5.154427528381348
training step: 44491, total_loss: 3.5162088871002197
training step: 44492, total_loss: 0.38378211855888367
training step: 44493, total_loss: 6.788654804229736
training step: 44494, total_loss: 0.8278380036354065
training step: 44495, total_loss: 1.0307987928390503
training step: 44496, total_loss: 2.3018858432769775
training step: 44497, total_loss: 1.165175199508667
training step: 44498, total_loss: 1.1603853702545166
training step: 44499, total_loss: 1.9216285943984985
training step: 44500, total_loss: 2.5205955505371094
training step: 44501, total_loss: 1.3970980644226074
training step: 44502, total_loss: 1.0279239416122437
training step: 44503, total_loss: 2.6758527755737305
training step: 44504, total_loss: 0.8218070268630981
training step: 44505, total_loss: 1.0616559982299805
training step: 44506, total_loss: 0.7632209062576294
training step: 44507, total_loss: 2.0991146564483643
training step: 44508, total_loss: 0.40334898233413696
training step: 44509, total_loss: 2.8644328117370605
training step: 44510, total_loss: 0.4225415289402008
training step: 44511, total_loss: 2.7580082416534424
training step: 44512, total_loss: 1.0442918539047241
training step: 44513, total_loss: 2.563101291656494
training step: 44514, total_loss: 2.5098204612731934
training step: 44515, total_loss: 1.2863469123840332
training step: 44516, total_loss: 1.7563657760620117
training step: 44517, total_loss: 4.296323299407959
training step: 44518, total_loss: 0.1722458153963089
training step: 44519, total_loss: 1.337257981300354
training step: 44520, total_loss: 1.3247432708740234
training step: 44521, total_loss: 2.0858328342437744
training step: 44522, total_loss: 1.9259228706359863
training step: 44523, total_loss: 0.6012347936630249
training step: 44524, total_loss: 2.2179622650146484
training step: 44525, total_loss: 0.3938228487968445
training step: 44526, total_loss: 2.802358388900757
training step: 44527, total_loss: 1.6012756824493408
training step: 44528, total_loss: 2.341454029083252
training step: 44529, total_loss: 2.6466147899627686
training step: 44530, total_loss: 2.5717709064483643
training step: 44531, total_loss: 2.168388843536377
training step: 44532, total_loss: 0.11867217719554901
training step: 44533, total_loss: 0.9950815439224243
training step: 44534, total_loss: 0.15546485781669617
training step: 44535, total_loss: 3.3481521606445312
training step: 44536, total_loss: 0.6266107559204102
training step: 44537, total_loss: 1.2407004833221436
training step: 44538, total_loss: 2.112428665161133
training step: 44539, total_loss: 1.2682726383209229
training step: 44540, total_loss: 1.6735572814941406
training step: 44541, total_loss: 1.8657169342041016
training step: 44542, total_loss: 4.2907562255859375
training step: 44543, total_loss: 2.398599624633789
training step: 44544, total_loss: 2.8163793087005615
training step: 44545, total_loss: 0.038707975298166275
training step: 44546, total_loss: 1.8418711423873901
training step: 44547, total_loss: 2.1063430309295654
training step: 44548, total_loss: 1.7014400959014893
training step: 44549, total_loss: 1.9234838485717773
training step: 44550, total_loss: 1.1247799396514893
training step: 44551, total_loss: 3.27885103225708
training step: 44552, total_loss: 0.9613105058670044
training step: 44553, total_loss: 0.13500596582889557
training step: 44554, total_loss: 3.26704740524292
training step: 44555, total_loss: 2.89316987991333
training step: 44556, total_loss: 2.513023853302002
training step: 44557, total_loss: 1.4031224250793457
training step: 44558, total_loss: 1.7235872745513916
training step: 44559, total_loss: 1.9331046342849731
training step: 44560, total_loss: 2.9161393642425537
training step: 44561, total_loss: 1.1890063285827637
training step: 44562, total_loss: 1.0536454916000366
training step: 44563, total_loss: 2.681241035461426
training step: 44564, total_loss: 2.1536874771118164
training step: 44565, total_loss: 0.8542794585227966
training step: 44566, total_loss: 3.6159048080444336
training step: 44567, total_loss: 0.9557910561561584
training step: 44568, total_loss: 3.2989907264709473
training step: 44569, total_loss: 0.3347090780735016
training step: 44570, total_loss: 0.8290684819221497
training step: 44571, total_loss: 5.418572425842285
training step: 44572, total_loss: 1.5321825742721558
training step: 44573, total_loss: 1.9001890420913696
training step: 44574, total_loss: 0.4786229729652405
training step: 44575, total_loss: 0.0010901582427322865
training step: 44576, total_loss: 4.348659038543701
training step: 44577, total_loss: 2.5195772647857666
training step: 44578, total_loss: 2.9355130195617676
training step: 44579, total_loss: 1.0038005113601685
training step: 44580, total_loss: 4.073291778564453
training step: 44581, total_loss: 1.0667644739151
training step: 44582, total_loss: 0.6983792781829834
training step: 44583, total_loss: 0.05496174842119217
training step: 44584, total_loss: 1.8321924209594727
training step: 44585, total_loss: 2.9445676803588867
training step: 44586, total_loss: 3.9036338329315186
training step: 44587, total_loss: 2.702815294265747
training step: 44588, total_loss: 2.03955078125
training step: 44589, total_loss: 3.49306583404541
training step: 44590, total_loss: 1.5355324745178223
training step: 44591, total_loss: 5.418275833129883
training step: 44592, total_loss: 1.30061674118042
training step: 44593, total_loss: 1.2527848482131958
training step: 44594, total_loss: 2.2889602184295654
training step: 44595, total_loss: 0.631492018699646
training step: 44596, total_loss: 2.321657180786133
training step: 44597, total_loss: 1.6970162391662598
training step: 44598, total_loss: 1.1247156858444214
training step: 44599, total_loss: 2.6869187355041504
training step: 44600, total_loss: 1.7077445983886719
training step: 44601, total_loss: 0.5575790405273438
training step: 44602, total_loss: 3.5922350883483887
training step: 44603, total_loss: 0.7812172174453735
training step: 44604, total_loss: 1.5730465650558472
training step: 44605, total_loss: 1.8525347709655762
training step: 44606, total_loss: 1.0877535343170166
training step: 44607, total_loss: 4.912592887878418
training step: 44608, total_loss: 1.2182084321975708
training step: 44609, total_loss: 1.9684076309204102
training step: 44610, total_loss: 2.6483840942382812
training step: 44611, total_loss: 3.373676300048828
training step: 44612, total_loss: 0.8518112301826477
training step: 44613, total_loss: 0.7503184080123901
training step: 44614, total_loss: 1.2240351438522339
training step: 44615, total_loss: 1.7123913764953613
training step: 44616, total_loss: 2.6992974281311035
training step: 44617, total_loss: 1.3611642122268677
training step: 44618, total_loss: 1.7258024215698242
training step: 44619, total_loss: 2.3117995262145996
training step: 44620, total_loss: 0.4320194721221924
training step: 44621, total_loss: 0.016444619745016098
training step: 44622, total_loss: 0.41441869735717773
training step: 44623, total_loss: 1.7487263679504395
training step: 44624, total_loss: 1.4379360675811768
training step: 44625, total_loss: 2.722520351409912
training step: 44626, total_loss: 1.8069850206375122
training step: 44627, total_loss: 0.9014222621917725
training step: 44628, total_loss: 2.3382697105407715
training step: 44629, total_loss: 1.4579280614852905
training step: 44630, total_loss: 1.2694926261901855
training step: 44631, total_loss: 1.5662504434585571
training step: 44632, total_loss: 3.0176453590393066
training step: 44633, total_loss: 1.9224905967712402
training step: 44634, total_loss: 1.0310297012329102
training step: 44635, total_loss: 3.3589186668395996
training step: 44636, total_loss: 1.207512617111206
training step: 44637, total_loss: 2.938817262649536
training step: 44638, total_loss: 0.00937388651072979
training step: 44639, total_loss: 0.976844310760498
training step: 44640, total_loss: 2.3343427181243896
training step: 44641, total_loss: 0.0087086521089077
training step: 44642, total_loss: 4.570446968078613
training step: 44643, total_loss: 2.6563572883605957
training step: 44644, total_loss: 0.9930571913719177
training step: 44645, total_loss: 2.011094808578491
training step: 44646, total_loss: 4.645204544067383
training step: 44647, total_loss: 0.544434666633606
training step: 44648, total_loss: 1.2568421363830566
training step: 44649, total_loss: 0.5059467554092407
training step: 44650, total_loss: 1.9617477655410767
training step: 44651, total_loss: 2.176969289779663
training step: 44652, total_loss: 0.7026442289352417
training step: 44653, total_loss: 1.7892097234725952
training step: 44654, total_loss: 1.3867186307907104
training step: 44655, total_loss: 0.7643083333969116
training step: 44656, total_loss: 0.8836872577667236
training step: 44657, total_loss: 3.8578591346740723
training step: 44658, total_loss: 0.06729361414909363
training step: 44659, total_loss: 3.720729351043701
training step: 44660, total_loss: 1.5055816173553467
training step: 44661, total_loss: 1.813000202178955
training step: 44662, total_loss: 2.2882604598999023
training step: 44663, total_loss: 1.8529212474822998
training step: 44664, total_loss: 0.5752405524253845
training step: 44665, total_loss: 0.00018481639563106
training step: 44666, total_loss: 2.2131059169769287
training step: 44667, total_loss: 2.351280689239502
training step: 44668, total_loss: 1.8411755561828613
training step: 44669, total_loss: 1.1001224517822266
training step: 44670, total_loss: 0.8447544574737549
training step: 44671, total_loss: 2.290018320083618
training step: 44672, total_loss: 0.38018378615379333
training step: 44673, total_loss: 1.5934518575668335
training step: 44674, total_loss: 1.8117015361785889
training step: 44675, total_loss: 4.491430759429932
training step: 44676, total_loss: 2.0020577907562256
training step: 44677, total_loss: 2.4188623428344727
training step: 44678, total_loss: 0.8638123273849487
training step: 44679, total_loss: 1.002647042274475
training step: 44680, total_loss: 2.0060863494873047
training step: 44681, total_loss: 2.691542625427246
training step: 44682, total_loss: 1.26161789894104
training step: 44683, total_loss: 0.5097915530204773
training step: 44684, total_loss: 2.050673007965088
training step: 44685, total_loss: 0.7049996852874756
training step: 44686, total_loss: 1.6225008964538574
training step: 44687, total_loss: 4.769721508026123
training step: 44688, total_loss: 2.5090341567993164
training step: 44689, total_loss: 0.5780404806137085
training step: 44690, total_loss: 0.6874037981033325
training step: 44691, total_loss: 0.9206578731536865
training step: 44692, total_loss: 1.1861176490783691
training step: 44693, total_loss: 1.9409139156341553
training step: 44694, total_loss: 1.8730146884918213
training step: 44695, total_loss: 3.206737518310547
training step: 44696, total_loss: 0.26407915353775024
training step: 44697, total_loss: 0.747704029083252
training step: 44698, total_loss: 2.8209362030029297
training step: 44699, total_loss: 2.9214258193969727
training step: 44700, total_loss: 2.3951117992401123
training step: 44701, total_loss: 0.2597493529319763
training step: 44702, total_loss: 1.4425690174102783
training step: 44703, total_loss: 0.6934407949447632
training step: 44704, total_loss: 1.1004109382629395
training step: 44705, total_loss: 0.9788618683815002
training step: 44706, total_loss: 0.7087585926055908
training step: 44707, total_loss: 2.752652406692505
training step: 44708, total_loss: 0.14351385831832886
training step: 44709, total_loss: 0.3020343780517578
training step: 44710, total_loss: 0.12881159782409668
training step: 44711, total_loss: 3.9359962940216064
training step: 44712, total_loss: 1.1466566324234009
training step: 44713, total_loss: 2.5932912826538086
training step: 44714, total_loss: 4.8560380935668945
training step: 44715, total_loss: 1.5881891250610352
training step: 44716, total_loss: 3.324613094329834
training step: 44717, total_loss: 1.4337010383605957
training step: 44718, total_loss: 0.1471654623746872
training step: 44719, total_loss: 1.2268884181976318
training step: 44720, total_loss: 1.7078096866607666
training step: 44721, total_loss: 0.9710202217102051
training step: 44722, total_loss: 3.0874476432800293
training step: 44723, total_loss: 1.4758565425872803
training step: 44724, total_loss: 1.5239944458007812
training step: 44725, total_loss: 0.5964156985282898
training step: 44726, total_loss: 4.114570140838623
training step: 44727, total_loss: 4.497317790985107
training step: 44728, total_loss: 1.5806610584259033
training step: 44729, total_loss: 4.868391990661621
training step: 44730, total_loss: 3.2227511405944824
training step: 44731, total_loss: 0.42769742012023926
training step: 44732, total_loss: 3.0613365173339844
training step: 44733, total_loss: 1.3367695808410645
training step: 44734, total_loss: 0.2795039117336273
training step: 44735, total_loss: 2.666569232940674
training step: 44736, total_loss: 3.360445976257324
training step: 44737, total_loss: 1.4585697650909424
training step: 44738, total_loss: 0.34962034225463867
training step: 44739, total_loss: 1.1929563283920288
training step: 44740, total_loss: 3.5131664276123047
training step: 44741, total_loss: 0.4913213551044464
training step: 44742, total_loss: 1.012596607208252
training step: 44743, total_loss: 2.3574721813201904
training step: 44744, total_loss: 1.0133991241455078
training step: 44745, total_loss: 1.928541660308838
training step: 44746, total_loss: 3.9146950244903564
training step: 44747, total_loss: 1.5185105800628662
training step: 44748, total_loss: 0.6231156587600708
training step: 44749, total_loss: 0.5816009044647217
training step: 44750, total_loss: 2.414888381958008
training step: 44751, total_loss: 1.2712156772613525
training step: 44752, total_loss: 0.21293416619300842
training step: 44753, total_loss: 0.13742217421531677
training step: 44754, total_loss: 1.7290306091308594
training step: 44755, total_loss: 2.74365234375
training step: 44756, total_loss: 0.06156657636165619
training step: 44757, total_loss: 0.12135222554206848
training step: 44758, total_loss: 1.2119197845458984
training step: 44759, total_loss: 2.9848828315734863
training step: 44760, total_loss: 1.1400734186172485
training step: 44761, total_loss: 3.348421096801758
training step: 44762, total_loss: 1.1497600078582764
training step: 44763, total_loss: 2.4223105907440186
training step: 44764, total_loss: 1.943393349647522
training step: 44765, total_loss: 2.1407554149627686
training step: 44766, total_loss: 1.541428565979004
training step: 44767, total_loss: 0.3581129312515259
training step: 44768, total_loss: 2.9735665321350098
training step: 44769, total_loss: 3.6307287216186523
training step: 44770, total_loss: 2.031123638153076
training step: 44771, total_loss: 0.8005062341690063
training step: 44772, total_loss: 1.8496484756469727
training step: 44773, total_loss: 0.7731923460960388
training step: 44774, total_loss: 3.253176689147949
training step: 44775, total_loss: 3.5149598121643066
training step: 44776, total_loss: 0.8864063620567322
training step: 44777, total_loss: 2.507708787918091
training step: 44778, total_loss: 1.8513164520263672
training step: 44779, total_loss: 2.6413326263427734
training step: 44780, total_loss: 1.151188611984253
training step: 44781, total_loss: 1.4551303386688232
training step: 44782, total_loss: 2.6381165981292725
training step: 44783, total_loss: 0.1771787703037262
training step: 44784, total_loss: 1.3892388343811035
training step: 44785, total_loss: 2.0536553859710693
training step: 44786, total_loss: 2.6820590496063232
training step: 44787, total_loss: 0.7904374003410339
training step: 44788, total_loss: 0.531464695930481
training step: 44789, total_loss: 3.1992387771606445
training step: 44790, total_loss: 1.745575189590454
training step: 44791, total_loss: 3.391240358352661
training step: 44792, total_loss: 0.04639048874378204
training step: 44793, total_loss: 0.16318067908287048
training step: 44794, total_loss: 2.0448131561279297
training step: 44795, total_loss: 1.7377134561538696
training step: 44796, total_loss: 2.623551368713379
training step: 44797, total_loss: 1.0224206447601318
training step: 44798, total_loss: 1.4782524108886719
training step: 44799, total_loss: 0.5280773043632507
training step: 44800, total_loss: 0.40153032541275024
training step: 44801, total_loss: 0.3894731402397156
training step: 44802, total_loss: 0.1563996970653534
training step: 44803, total_loss: 4.728142738342285
training step: 44804, total_loss: 0.006413005292415619
training step: 44805, total_loss: 0.9166487455368042
training step: 44806, total_loss: 0.9780567288398743
training step: 44807, total_loss: 0.09556841850280762
training step: 44808, total_loss: 1.6295461654663086
training step: 44809, total_loss: 0.9440286755561829
training step: 44810, total_loss: 3.1690993309020996
training step: 44811, total_loss: 1.0349845886230469
training step: 44812, total_loss: 1.9675763845443726
training step: 44813, total_loss: 1.4949350357055664
training step: 44814, total_loss: 3.84773325920105
training step: 44815, total_loss: 0.650274932384491
training step: 44816, total_loss: 4.47811222076416
training step: 44817, total_loss: 3.0118298530578613
training step: 44818, total_loss: 1.7199597358703613
training step: 44819, total_loss: 1.866132140159607
training step: 44820, total_loss: 2.301914691925049
training step: 44821, total_loss: 1.147032618522644
training step: 44822, total_loss: 1.9279930591583252
training step: 44823, total_loss: 0.35915130376815796
training step: 44824, total_loss: 0.22693879902362823
training step: 44825, total_loss: 0.5493872165679932
training step: 44826, total_loss: 0.3044605851173401
training step: 44827, total_loss: 4.707022666931152
training step: 44828, total_loss: 4.266628265380859
training step: 44829, total_loss: 1.094982624053955
training step: 44830, total_loss: 2.2327589988708496
training step: 44831, total_loss: 0.7052289247512817
training step: 44832, total_loss: 2.0033884048461914
training step: 44833, total_loss: 3.3787333965301514
training step: 44834, total_loss: 4.572056293487549
training step: 44835, total_loss: 2.974459171295166
training step: 44836, total_loss: 2.883772373199463
training step: 44837, total_loss: 2.127661943435669
training step: 44838, total_loss: 0.3357998728752136
training step: 44839, total_loss: 1.2350057363510132
training step: 44840, total_loss: 1.4615511894226074
training step: 44841, total_loss: 0.44106268882751465
training step: 44842, total_loss: 0.22990547120571136
training step: 44843, total_loss: 2.8966755867004395
training step: 44844, total_loss: 1.3325434923171997
training step: 44845, total_loss: 0.012900610454380512
training step: 44846, total_loss: 2.1323862075805664
training step: 44847, total_loss: 2.6530227661132812
training step: 44848, total_loss: 0.3584093451499939
training step: 44849, total_loss: 1.5056276321411133
training step: 44850, total_loss: 0.006682930048555136
training step: 44851, total_loss: 2.2333221435546875
training step: 44852, total_loss: 3.119762420654297
training step: 44853, total_loss: 4.233580589294434
training step: 44854, total_loss: 0.2527722716331482
training step: 44855, total_loss: 1.2265167236328125
training step: 44856, total_loss: 2.157682418823242
training step: 44857, total_loss: 0.9288787841796875
training step: 44858, total_loss: 2.151836395263672
training step: 44859, total_loss: 2.235186815261841
training step: 44860, total_loss: 3.645141124725342
training step: 44861, total_loss: 0.8634462952613831
training step: 44862, total_loss: 4.7326436042785645
training step: 44863, total_loss: 1.171027421951294
training step: 44864, total_loss: 3.8922338485717773
training step: 44865, total_loss: 0.01791057363152504
training step: 44866, total_loss: 5.394320487976074
training step: 44867, total_loss: 0.06307417154312134
training step: 44868, total_loss: 1.850929617881775
training step: 44869, total_loss: 0.02832336351275444
training step: 44870, total_loss: 1.1133065223693848
training step: 44871, total_loss: 0.04048867151141167
training step: 44872, total_loss: 4.044455528259277
training step: 44873, total_loss: 3.9256718158721924
training step: 44874, total_loss: 3.744938850402832
training step: 44875, total_loss: 3.002199172973633
training step: 44876, total_loss: 1.0942268371582031
training step: 44877, total_loss: 1.685034990310669
training step: 44878, total_loss: 2.818054437637329
training step: 44879, total_loss: 0.0006693732575513422
training step: 44880, total_loss: 0.4708397686481476
training step: 44881, total_loss: 1.610559344291687
training step: 44882, total_loss: 2.056273937225342
training step: 44883, total_loss: 0.007527880370616913
training step: 44884, total_loss: 1.2645199298858643
training step: 44885, total_loss: 3.991703510284424
training step: 44886, total_loss: 4.829537391662598
training step: 44887, total_loss: 4.0555806159973145
training step: 44888, total_loss: 1.5385897159576416
training step: 44889, total_loss: 0.05732148140668869
training step: 44890, total_loss: 3.310091018676758
training step: 44891, total_loss: 1.2509289979934692
training step: 44892, total_loss: 1.8566797971725464
training step: 44893, total_loss: 1.8741329908370972
training step: 44894, total_loss: 3.123140811920166
training step: 44895, total_loss: 0.510769784450531
training step: 44896, total_loss: 1.8130958080291748
training step: 44897, total_loss: 1.8876564502716064
training step: 44898, total_loss: 1.5488802194595337
training step: 44899, total_loss: 2.161055326461792
training step: 44900, total_loss: 3.5424880981445312
training step: 44901, total_loss: 1.8362051248550415
training step: 44902, total_loss: 0.18578951060771942
training step: 44903, total_loss: 2.655795097351074
training step: 44904, total_loss: 2.0126914978027344
training step: 44905, total_loss: 3.237855911254883
training step: 44906, total_loss: 1.6862287521362305
training step: 44907, total_loss: 2.418917655944824
training step: 44908, total_loss: 0.6196333169937134
training step: 44909, total_loss: 0.8065301775932312
training step: 44910, total_loss: 0.7340912818908691
training step: 44911, total_loss: 3.745981216430664
training step: 44912, total_loss: 3.2569150924682617
training step: 44913, total_loss: 2.0312139987945557
training step: 44914, total_loss: 2.191380023956299
training step: 44915, total_loss: 1.8369452953338623
training step: 44916, total_loss: 2.3914451599121094
training step: 44917, total_loss: 1.9524105787277222
training step: 44918, total_loss: 0.7318665981292725
training step: 44919, total_loss: 2.8009214401245117
training step: 44920, total_loss: 0.22680944204330444
training step: 44921, total_loss: 4.4746551513671875
training step: 44922, total_loss: 1.127990961074829
training step: 44923, total_loss: 1.0209486484527588
training step: 44924, total_loss: 0.04500202462077141
training step: 44925, total_loss: 0.749882161617279
training step: 44926, total_loss: 1.3088438510894775
training step: 44927, total_loss: 0.6738975644111633
training step: 44928, total_loss: 0.9611140489578247
training step: 44929, total_loss: 1.9219781160354614
training step: 44930, total_loss: 3.226187229156494
training step: 44931, total_loss: 4.615135192871094
training step: 44932, total_loss: 1.221866250038147
training step: 44933, total_loss: 0.2617080807685852
training step: 44934, total_loss: 0.23480340838432312
training step: 44935, total_loss: 2.9104509353637695
training step: 44936, total_loss: 3.2838380336761475
training step: 44937, total_loss: 1.928818702697754
training step: 44938, total_loss: 0.9650692343711853
training step: 44939, total_loss: 0.14727619290351868
training step: 44940, total_loss: 2.062213182449341
training step: 44941, total_loss: 1.7885464429855347
training step: 44942, total_loss: 2.916360855102539
training step: 44943, total_loss: 3.1993415355682373
training step: 44944, total_loss: 2.391389846801758
training step: 44945, total_loss: 1.7991039752960205
training step: 44946, total_loss: 3.445777416229248
training step: 44947, total_loss: 0.4876144528388977
training step: 44948, total_loss: 0.013960877433419228
training step: 44949, total_loss: 2.817445993423462
training step: 44950, total_loss: 2.5964064598083496
training step: 44951, total_loss: 0.5841061472892761
training step: 44952, total_loss: 0.8560800552368164
training step: 44953, total_loss: 0.2320464849472046
training step: 44954, total_loss: 1.1205922365188599
training step: 44955, total_loss: 0.5341119766235352INFO:tensorflow:Writing predictions to: test_output/predictions_45000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_45000.json

training step: 44956, total_loss: 1.5382862091064453
training step: 44957, total_loss: 0.4401477575302124
training step: 44958, total_loss: 0.0037157745100557804
training step: 44959, total_loss: 4.2298903465271
training step: 44960, total_loss: 2.9274747371673584
training step: 44961, total_loss: 0.6293477416038513
training step: 44962, total_loss: 0.7893120050430298
training step: 44963, total_loss: 0.014669876545667648
training step: 44964, total_loss: 3.3290162086486816
training step: 44965, total_loss: 2.482891321182251
training step: 44966, total_loss: 3.537315845489502
training step: 44967, total_loss: 0.05499875545501709
training step: 44968, total_loss: 1.3034225702285767
training step: 44969, total_loss: 1.7437447309494019
training step: 44970, total_loss: 1.267601728439331
training step: 44971, total_loss: 1.6320174932479858
training step: 44972, total_loss: 1.0282028913497925
training step: 44973, total_loss: 2.0422685146331787
training step: 44974, total_loss: 2.0224337577819824
training step: 44975, total_loss: 2.9542973041534424
training step: 44976, total_loss: 2.0043587684631348
training step: 44977, total_loss: 2.4907422065734863
training step: 44978, total_loss: 1.6789517402648926
training step: 44979, total_loss: 0.20738133788108826
training step: 44980, total_loss: 3.210510492324829
training step: 44981, total_loss: 0.29969123005867004
training step: 44982, total_loss: 2.2226877212524414
training step: 44983, total_loss: 3.04754638671875
training step: 44984, total_loss: 1.4802203178405762
training step: 44985, total_loss: 3.5477957725524902
training step: 44986, total_loss: 3.20695161819458
training step: 44987, total_loss: 4.916509628295898
training step: 44988, total_loss: 2.410386085510254
training step: 44989, total_loss: 1.327803611755371
training step: 44990, total_loss: 0.7004761695861816
training step: 44991, total_loss: 4.560294151306152
training step: 44992, total_loss: 0.24149096012115479
training step: 44993, total_loss: 0.6066654920578003
training step: 44994, total_loss: 3.1118593215942383
training step: 44995, total_loss: 1.1523323059082031
training step: 44996, total_loss: 1.577469825744629
training step: 44997, total_loss: 0.5058633685112
training step: 44998, total_loss: 1.4729481935501099
training step: 44999, total_loss: 0.7677367925643921
training step: 45000, total_loss: 1.3736531734466553
epoch finished! shuffle=False
evaluation: 9000, total_loss: 1.837197184562683, f1: 55.603448328520706, followup: 24.722349003499165, yesno: 78.98980678533394, heq: 49.018712916476495, dheq: 2.6

Model saved in path test_output//model_45000.ckpt
training step: 45001, total_loss: 1.538223385810852
training step: 45002, total_loss: 3.305614948272705
training step: 45003, total_loss: 5.751216888427734
training step: 45004, total_loss: 1.4748094081878662
training step: 45005, total_loss: 0.9590339660644531
training step: 45006, total_loss: 2.9524588584899902
training step: 45007, total_loss: 2.0062685012817383
training step: 45008, total_loss: 1.372825264930725
training step: 45009, total_loss: 0.698645830154419
training step: 45010, total_loss: 1.4495582580566406
training step: 45011, total_loss: 3.033327102661133
training step: 45012, total_loss: 0.9029775261878967
training step: 45013, total_loss: 0.6815544962882996
training step: 45014, total_loss: 1.1537766456604004
training step: 45015, total_loss: 3.9878487586975098
training step: 45016, total_loss: 0.6579490900039673
training step: 45017, total_loss: 0.962597131729126
training step: 45018, total_loss: 1.098906397819519
training step: 45019, total_loss: 4.504271507263184
training step: 45020, total_loss: 1.4791607856750488
training step: 45021, total_loss: 0.6123941540718079
training step: 45022, total_loss: 0.15384426712989807
training step: 45023, total_loss: 0.28138092160224915
training step: 45024, total_loss: 2.0100326538085938
training step: 45025, total_loss: 0.6046717762947083
training step: 45026, total_loss: 1.9563839435577393
training step: 45027, total_loss: 0.784659743309021
training step: 45028, total_loss: 2.6064553260803223
training step: 45029, total_loss: 1.9623498916625977
training step: 45030, total_loss: 1.592104434967041
training step: 45031, total_loss: 0.4672970771789551
training step: 45032, total_loss: 0.6432397961616516
training step: 45033, total_loss: 3.9048328399658203
training step: 45034, total_loss: 1.1280182600021362
training step: 45035, total_loss: 2.1331098079681396
training step: 45036, total_loss: 1.0294829607009888
training step: 45037, total_loss: 0.4687771499156952
training step: 45038, total_loss: 0.7370287775993347
training step: 45039, total_loss: 1.156184434890747
training step: 45040, total_loss: 0.8968298435211182
training step: 45041, total_loss: 0.978992223739624
training step: 45042, total_loss: 1.4325588941574097
training step: 45043, total_loss: 2.7269623279571533
training step: 45044, total_loss: 5.322577476501465
training step: 45045, total_loss: 2.051828384399414
training step: 45046, total_loss: 0.2410040646791458
training step: 45047, total_loss: 0.8813316226005554
training step: 45048, total_loss: 2.9561281204223633
training step: 45049, total_loss: 4.5504231452941895
training step: 45050, total_loss: 0.5534052848815918
training step: 45051, total_loss: 1.9200448989868164
training step: 45052, total_loss: 3.1807994842529297
training step: 45053, total_loss: 2.201711654663086
training step: 45054, total_loss: 0.008098961785435677
training step: 45055, total_loss: 0.33647841215133667
training step: 45056, total_loss: 2.1014482975006104
training step: 45057, total_loss: 2.354356527328491
training step: 45058, total_loss: 1.8014354705810547
training step: 45059, total_loss: 1.2651097774505615
training step: 45060, total_loss: 1.4317128658294678
training step: 45061, total_loss: 1.9266574382781982
training step: 45062, total_loss: 3.279073715209961
training step: 45063, total_loss: 1.3979010581970215
training step: 45064, total_loss: 3.4249491691589355
training step: 45065, total_loss: 0.7536969780921936
training step: 45066, total_loss: 0.13356506824493408
training step: 45067, total_loss: 0.5766894817352295
training step: 45068, total_loss: 1.0478549003601074
training step: 45069, total_loss: 1.9684966802597046
training step: 45070, total_loss: 4.309330940246582
training step: 45071, total_loss: 1.5749504566192627
training step: 45072, total_loss: 0.4303267300128937
training step: 45073, total_loss: 0.573205292224884
training step: 45074, total_loss: 2.1277806758880615
training step: 45075, total_loss: 2.446852684020996
training step: 45076, total_loss: 3.394395351409912
training step: 45077, total_loss: 3.144721508026123
training step: 45078, total_loss: 1.7186484336853027
training step: 45079, total_loss: 2.5391201972961426
training step: 45080, total_loss: 1.4225165843963623
training step: 45081, total_loss: 1.348698377609253
training step: 45082, total_loss: 2.2136080265045166
training step: 45083, total_loss: 3.5722718238830566
training step: 45084, total_loss: 0.13209107518196106
training step: 45085, total_loss: 2.911234140396118
training step: 45086, total_loss: 2.5692622661590576
training step: 45087, total_loss: 0.08112702518701553
training step: 45088, total_loss: 2.293679714202881
training step: 45089, total_loss: 1.6684672832489014
training step: 45090, total_loss: 0.5518413782119751
training step: 45091, total_loss: 1.053031325340271
training step: 45092, total_loss: 1.3387523889541626
training step: 45093, total_loss: 0.9459478855133057
training step: 45094, total_loss: 2.0191638469696045
training step: 45095, total_loss: 0.3868061900138855
training step: 45096, total_loss: 0.9270526766777039
training step: 45097, total_loss: 2.5424442291259766
training step: 45098, total_loss: 0.9590030908584595
training step: 45099, total_loss: 1.9399458169937134
training step: 45100, total_loss: 2.08613920211792
training step: 45101, total_loss: 1.6247432231903076
training step: 45102, total_loss: 4.846343994140625
training step: 45103, total_loss: 2.078688144683838
training step: 45104, total_loss: 2.058074951171875
training step: 45105, total_loss: 0.8793551921844482
training step: 45106, total_loss: 3.7211029529571533
training step: 45107, total_loss: 2.4926815032958984
training step: 45108, total_loss: 1.443415641784668
training step: 45109, total_loss: 2.9985673427581787
training step: 45110, total_loss: 2.406616449356079
training step: 45111, total_loss: 2.8251984119415283
training step: 45112, total_loss: 2.0375869274139404
training step: 45113, total_loss: 0.8663768172264099
training step: 45114, total_loss: 0.9407634735107422
training step: 45115, total_loss: 2.309305191040039
training step: 45116, total_loss: 1.9173344373703003
training step: 45117, total_loss: 0.20742830634117126
training step: 45118, total_loss: 2.592447519302368
training step: 45119, total_loss: 5.12531042098999
training step: 45120, total_loss: 2.403475284576416
training step: 45121, total_loss: 0.7876560688018799
training step: 45122, total_loss: 1.3708879947662354
training step: 45123, total_loss: 0.14156943559646606
training step: 45124, total_loss: 6.432318687438965
training step: 45125, total_loss: 3.1609983444213867
training step: 45126, total_loss: 2.2347755432128906
training step: 45127, total_loss: 1.2885710000991821
training step: 45128, total_loss: 0.0012960671447217464
training step: 45129, total_loss: 0.033454619348049164
training step: 45130, total_loss: 1.974684715270996
training step: 45131, total_loss: 1.3630266189575195
training step: 45132, total_loss: 2.2867953777313232
training step: 45133, total_loss: 3.537601947784424
training step: 45134, total_loss: 2.7346808910369873
training step: 45135, total_loss: 0.7092964053153992
training step: 45136, total_loss: 2.6693482398986816
training step: 45137, total_loss: 2.3202571868896484
training step: 45138, total_loss: 0.5529048442840576
training step: 45139, total_loss: 2.143742561340332
training step: 45140, total_loss: 1.3907256126403809
training step: 45141, total_loss: 0.7487057447433472
training step: 45142, total_loss: 2.676041603088379
training step: 45143, total_loss: 3.6080029010772705
training step: 45144, total_loss: 0.6737640500068665
training step: 45145, total_loss: 1.428513765335083
training step: 45146, total_loss: 2.4941139221191406
training step: 45147, total_loss: 6.192870140075684
training step: 45148, total_loss: 1.9927562475204468
training step: 45149, total_loss: 4.729676246643066
training step: 45150, total_loss: 2.698190450668335
training step: 45151, total_loss: 1.7653125524520874
training step: 45152, total_loss: 1.3953295946121216
training step: 45153, total_loss: 1.2645025253295898
training step: 45154, total_loss: 0.691601037979126
training step: 45155, total_loss: 4.2950263023376465
training step: 45156, total_loss: 1.1962454319000244
training step: 45157, total_loss: 0.7458494305610657
training step: 45158, total_loss: 1.9823424816131592
training step: 45159, total_loss: 3.1316957473754883
training step: 45160, total_loss: 1.346975564956665
training step: 45161, total_loss: 1.5220344066619873
training step: 45162, total_loss: 1.5045897960662842
training step: 45163, total_loss: 1.9932019710540771
training step: 45164, total_loss: 0.7026860117912292
training step: 45165, total_loss: 2.9562647342681885
training step: 45166, total_loss: 2.0185189247131348
training step: 45167, total_loss: 2.475708246231079
training step: 45168, total_loss: 2.11903715133667
training step: 45169, total_loss: 2.970158576965332
training step: 45170, total_loss: 2.407670497894287
training step: 45171, total_loss: 2.439023017883301
training step: 45172, total_loss: 0.5640279054641724
training step: 45173, total_loss: 1.2718942165374756
training step: 45174, total_loss: 0.18425139784812927
training step: 45175, total_loss: 1.645662546157837
training step: 45176, total_loss: 1.7980304956436157
training step: 45177, total_loss: 0.8582415580749512
training step: 45178, total_loss: 0.12633101642131805
training step: 45179, total_loss: 2.291057586669922
training step: 45180, total_loss: 5.334026336669922
training step: 45181, total_loss: 0.5770726203918457
training step: 45182, total_loss: 2.8696422576904297
training step: 45183, total_loss: 1.7758138179779053
training step: 45184, total_loss: 0.8733121752738953
training step: 45185, total_loss: 1.3204952478408813
training step: 45186, total_loss: 0.3401604890823364
training step: 45187, total_loss: 2.1939501762390137
training step: 45188, total_loss: 1.6210119724273682
training step: 45189, total_loss: 3.516481637954712
training step: 45190, total_loss: 1.4353827238082886
training step: 45191, total_loss: 0.018866445869207382
training step: 45192, total_loss: 3.1526081562042236
training step: 45193, total_loss: 1.3462473154067993
training step: 45194, total_loss: 1.2942839860916138
training step: 45195, total_loss: 3.2841930389404297
training step: 45196, total_loss: 1.7008017301559448
training step: 45197, total_loss: 1.8737865686416626
training step: 45198, total_loss: 4.388333320617676
training step: 45199, total_loss: 3.8472471237182617
training step: 45200, total_loss: 2.403198719024658
training step: 45201, total_loss: 2.058530569076538
training step: 45202, total_loss: 2.9824628829956055
training step: 45203, total_loss: 5.346384525299072
training step: 45204, total_loss: 1.2747881412506104
training step: 45205, total_loss: 2.8949942588806152
training step: 45206, total_loss: 1.604844093322754
training step: 45207, total_loss: 2.0502066612243652
training step: 45208, total_loss: 3.4901537895202637
training step: 45209, total_loss: 2.899033546447754
training step: 45210, total_loss: 1.3298454284667969
training step: 45211, total_loss: 1.8431187868118286
training step: 45212, total_loss: 1.985969066619873
training step: 45213, total_loss: 3.736576795578003
training step: 45214, total_loss: 0.6756234169006348
training step: 45215, total_loss: 1.8943583965301514
training step: 45216, total_loss: 0.8800216317176819
training step: 45217, total_loss: 0.8973528742790222
training step: 45218, total_loss: 1.390669345855713
training step: 45219, total_loss: 3.4239420890808105
training step: 45220, total_loss: 2.663661003112793
training step: 45221, total_loss: 1.5736896991729736
training step: 45222, total_loss: 1.0943442583084106
training step: 45223, total_loss: 1.9686765670776367
training step: 45224, total_loss: 1.7985503673553467
training step: 45225, total_loss: 2.8309073448181152
training step: 45226, total_loss: 1.4265739917755127
training step: 45227, total_loss: 2.5199403762817383
training step: 45228, total_loss: 1.027157187461853
training step: 45229, total_loss: 2.319326877593994
training step: 45230, total_loss: 1.6661896705627441
training step: 45231, total_loss: 2.0371530055999756
training step: 45232, total_loss: 1.3480043411254883
training step: 45233, total_loss: 0.6440857648849487
training step: 45234, total_loss: 1.5333311557769775
training step: 45235, total_loss: 1.1742417812347412
training step: 45236, total_loss: 2.262314796447754
training step: 45237, total_loss: 1.1857154369354248
training step: 45238, total_loss: 2.185962200164795
training step: 45239, total_loss: 3.1597847938537598
training step: 45240, total_loss: 1.4683103561401367
training step: 45241, total_loss: 2.4302773475646973
training step: 45242, total_loss: 1.511566162109375
training step: 45243, total_loss: 2.0367813110351562
training step: 45244, total_loss: 2.9686331748962402
training step: 45245, total_loss: 2.1665849685668945
training step: 45246, total_loss: 1.989732265472412
training step: 45247, total_loss: 2.6132965087890625
training step: 45248, total_loss: 1.1967508792877197
training step: 45249, total_loss: 2.3609535694122314
training step: 45250, total_loss: 2.791043281555176
training step: 45251, total_loss: 2.6190128326416016
training step: 45252, total_loss: 0.960239589214325
training step: 45253, total_loss: 3.180474281311035
training step: 45254, total_loss: 4.418307304382324
training step: 45255, total_loss: 2.5918755531311035
training step: 45256, total_loss: 1.0238033533096313
training step: 45257, total_loss: 2.0835788249969482
training step: 45258, total_loss: 1.438249945640564
training step: 45259, total_loss: 1.234523057937622
training step: 45260, total_loss: 1.1650452613830566
training step: 45261, total_loss: 2.6990365982055664
training step: 45262, total_loss: 4.605985164642334
training step: 45263, total_loss: 1.541281819343567
training step: 45264, total_loss: 2.552631378173828
training step: 45265, total_loss: 1.494848370552063
training step: 45266, total_loss: 0.688472330570221
training step: 45267, total_loss: 1.443302869796753
training step: 45268, total_loss: 1.0049772262573242
training step: 45269, total_loss: 2.5071425437927246
training step: 45270, total_loss: 2.2520620822906494
training step: 45271, total_loss: 3.072601795196533
training step: 45272, total_loss: 0.5810359716415405
training step: 45273, total_loss: 1.0365688800811768
training step: 45274, total_loss: 0.83640456199646
training step: 45275, total_loss: 0.8070501089096069
training step: 45276, total_loss: 1.3672764301300049
training step: 45277, total_loss: 2.213688850402832
training step: 45278, total_loss: 2.1055641174316406
training step: 45279, total_loss: 1.623336672782898
training step: 45280, total_loss: 3.550595283508301
training step: 45281, total_loss: 0.6024662256240845
training step: 45282, total_loss: 1.8565282821655273
training step: 45283, total_loss: 0.9805782437324524
training step: 45284, total_loss: 1.7115058898925781
training step: 45285, total_loss: 1.406072974205017
training step: 45286, total_loss: 2.229346990585327
training step: 45287, total_loss: 1.8493050336837769
training step: 45288, total_loss: 0.220831960439682
training step: 45289, total_loss: 0.9470682740211487
training step: 45290, total_loss: 1.2192240953445435
training step: 45291, total_loss: 0.5862200260162354
training step: 45292, total_loss: 2.475615978240967
training step: 45293, total_loss: 1.4498324394226074
training step: 45294, total_loss: 0.7617882490158081
training step: 45295, total_loss: 0.5885864496231079
training step: 45296, total_loss: 1.627713918685913
training step: 45297, total_loss: 2.689756393432617
training step: 45298, total_loss: 0.8031009435653687
training step: 45299, total_loss: 4.297972202301025
training step: 45300, total_loss: 1.843156337738037
training step: 45301, total_loss: 2.474147319793701
training step: 45302, total_loss: 2.995856761932373
training step: 45303, total_loss: 0.3612896203994751
training step: 45304, total_loss: 0.2880411148071289
training step: 45305, total_loss: 1.275721549987793
training step: 45306, total_loss: 0.4777447581291199
training step: 45307, total_loss: 0.04442586004734039
training step: 45308, total_loss: 1.3805687427520752
training step: 45309, total_loss: 2.6511762142181396
training step: 45310, total_loss: 2.3796210289001465
training step: 45311, total_loss: 0.024014171212911606
training step: 45312, total_loss: 0.805185854434967
training step: 45313, total_loss: 0.8012454509735107
training step: 45314, total_loss: 2.0280025005340576
training step: 45315, total_loss: 3.5811502933502197
training step: 45316, total_loss: 0.7652556896209717
training step: 45317, total_loss: 0.28175634145736694
training step: 45318, total_loss: 0.2696308493614197
training step: 45319, total_loss: 2.572171688079834
training step: 45320, total_loss: 1.3545005321502686
training step: 45321, total_loss: 1.965840458869934
training step: 45322, total_loss: 0.6013242602348328
training step: 45323, total_loss: 2.8167853355407715
training step: 45324, total_loss: 2.6340150833129883
training step: 45325, total_loss: 1.6159676313400269
training step: 45326, total_loss: 1.122330904006958
training step: 45327, total_loss: 0.5195983648300171
training step: 45328, total_loss: 0.054342448711395264
training step: 45329, total_loss: 1.7293727397918701
training step: 45330, total_loss: 3.9018869400024414
training step: 45331, total_loss: 3.867527723312378
training step: 45332, total_loss: 0.5810261368751526
training step: 45333, total_loss: 0.0010683630825951695
training step: 45334, total_loss: 0.11591285467147827
training step: 45335, total_loss: 0.6932312846183777
training step: 45336, total_loss: 1.3229354619979858
training step: 45337, total_loss: 3.141307830810547
training step: 45338, total_loss: 3.9372432231903076
training step: 45339, total_loss: 4.913973808288574
training step: 45340, total_loss: 1.4084852933883667
training step: 45341, total_loss: 3.3629508018493652
training step: 45342, total_loss: 3.0345818996429443
training step: 45343, total_loss: 3.469688892364502
training step: 45344, total_loss: 0.9906545281410217
training step: 45345, total_loss: 2.6121907234191895
training step: 45346, total_loss: 0.21692821383476257
training step: 45347, total_loss: 0.5262691974639893
training step: 45348, total_loss: 1.632348895072937
training step: 45349, total_loss: 2.401230573654175
training step: 45350, total_loss: 0.171770378947258
training step: 45351, total_loss: 3.609487295150757
training step: 45352, total_loss: 1.4008264541625977
training step: 45353, total_loss: 2.47467041015625
training step: 45354, total_loss: 0.44085076451301575
training step: 45355, total_loss: 0.621346116065979
training step: 45356, total_loss: 2.1725516319274902
training step: 45357, total_loss: 1.88022780418396
training step: 45358, total_loss: 2.4811017513275146
training step: 45359, total_loss: 2.568880558013916
training step: 45360, total_loss: 2.264835834503174
training step: 45361, total_loss: 2.235447406768799
training step: 45362, total_loss: 0.6892494559288025
training step: 45363, total_loss: 1.803514003753662
training step: 45364, total_loss: 0.32677581906318665
training step: 45365, total_loss: 2.604417324066162
training step: 45366, total_loss: 2.2908167839050293
training step: 45367, total_loss: 2.1643147468566895
training step: 45368, total_loss: 2.158796787261963
training step: 45369, total_loss: 1.4555639028549194
training step: 45370, total_loss: 1.7860156297683716
training step: 45371, total_loss: 1.5722477436065674
training step: 45372, total_loss: 1.814825415611267
training step: 45373, total_loss: 0.37418389320373535
training step: 45374, total_loss: 1.120643138885498
training step: 45375, total_loss: 1.879549264907837
training step: 45376, total_loss: 1.3452081680297852
training step: 45377, total_loss: 1.7603952884674072
training step: 45378, total_loss: 1.1259515285491943
training step: 45379, total_loss: 0.5974774956703186
training step: 45380, total_loss: 1.264726161956787
training step: 45381, total_loss: 2.3273823261260986
training step: 45382, total_loss: 0.07606916129589081
training step: 45383, total_loss: 1.5885789394378662
training step: 45384, total_loss: 0.8026443719863892
training step: 45385, total_loss: 0.789535641670227
training step: 45386, total_loss: 1.609687328338623
training step: 45387, total_loss: 3.057324171066284
training step: 45388, total_loss: 0.09228770434856415
training step: 45389, total_loss: 1.3346006870269775
training step: 45390, total_loss: 5.357117652893066
training step: 45391, total_loss: 2.195862054824829
training step: 45392, total_loss: 2.6345574855804443
training step: 45393, total_loss: 2.2183585166931152
training step: 45394, total_loss: 0.3161599040031433
training step: 45395, total_loss: 2.7682085037231445
training step: 45396, total_loss: 1.654730200767517
training step: 45397, total_loss: 2.5402050018310547
training step: 45398, total_loss: 0.9075109958648682
training step: 45399, total_loss: 4.519568920135498
training step: 45400, total_loss: 1.608809471130371
training step: 45401, total_loss: 0.9080082178115845
training step: 45402, total_loss: 0.734551191329956
training step: 45403, total_loss: 2.0753705501556396
training step: 45404, total_loss: 1.397606611251831
training step: 45405, total_loss: 1.0821744203567505
training step: 45406, total_loss: 1.819817066192627
training step: 45407, total_loss: 1.3102401494979858
training step: 45408, total_loss: 2.4914448261260986
training step: 45409, total_loss: 2.3128726482391357
training step: 45410, total_loss: 2.5922021865844727
training step: 45411, total_loss: 1.9447486400604248
training step: 45412, total_loss: 0.10322269797325134
training step: 45413, total_loss: 5.795327663421631
training step: 45414, total_loss: 0.07921143621206284
training step: 45415, total_loss: 3.055187225341797
training step: 45416, total_loss: 1.4481370449066162
training step: 45417, total_loss: 3.324451446533203
training step: 45418, total_loss: 0.8080125451087952
training step: 45419, total_loss: 4.591073513031006
training step: 45420, total_loss: 0.497415155172348
training step: 45421, total_loss: 4.060198783874512
training step: 45422, total_loss: 2.520535707473755
training step: 45423, total_loss: 1.7130508422851562
training step: 45424, total_loss: 2.8680408000946045
training step: 45425, total_loss: 1.9498238563537598
training step: 45426, total_loss: 1.833479881286621
training step: 45427, total_loss: 0.2344466894865036
training step: 45428, total_loss: 0.4159787893295288
training step: 45429, total_loss: 2.4250025749206543
training step: 45430, total_loss: 0.7293980121612549
training step: 45431, total_loss: 0.3390539288520813
training step: 45432, total_loss: 0.27103012800216675
training step: 45433, total_loss: 0.05131624639034271
training step: 45434, total_loss: 1.3043835163116455
training step: 45435, total_loss: 0.09697861969470978
training step: 45436, total_loss: 2.200230360031128
training step: 45437, total_loss: 2.9312567710876465
training step: 45438, total_loss: 1.0493718385696411
training step: 45439, total_loss: 0.12996968626976013
training step: 45440, total_loss: 0.5804934501647949
training step: 45441, total_loss: 2.273625135421753
training step: 45442, total_loss: 1.7059381008148193
training step: 45443, total_loss: 0.5964375138282776
training step: 45444, total_loss: 2.2505581378936768
training step: 45445, total_loss: 1.979220986366272
training step: 45446, total_loss: 2.535933256149292
training step: 45447, total_loss: 0.2874550223350525
training step: 45448, total_loss: 0.5415436029434204
training step: 45449, total_loss: 0.8337353467941284
training step: 45450, total_loss: 2.2955615520477295
training step: 45451, total_loss: 1.128164291381836
training step: 45452, total_loss: 1.0803149938583374
training step: 45453, total_loss: 2.541865110397339
training step: 45454, total_loss: 3.3433892726898193
training step: 45455, total_loss: 4.583832740783691
training step: 45456, total_loss: 2.2436461448669434
training step: 45457, total_loss: 2.5072994232177734
training step: 45458, total_loss: 3.4192752838134766
training step: 45459, total_loss: 3.3790600299835205
training step: 45460, total_loss: 0.6336090564727783
training step: 45461, total_loss: 4.285220146179199
training step: 45462, total_loss: 5.349706172943115
training step: 45463, total_loss: 1.875260353088379
training step: 45464, total_loss: 1.7005963325500488
training step: 45465, total_loss: 3.5315980911254883
training step: 45466, total_loss: 0.9544199705123901
training step: 45467, total_loss: 1.905155897140503
training step: 45468, total_loss: 2.170595645904541
training step: 45469, total_loss: 1.818464756011963
training step: 45470, total_loss: 0.5240508317947388
training step: 45471, total_loss: 1.0363690853118896
training step: 45472, total_loss: 3.2863223552703857
training step: 45473, total_loss: 1.9834153652191162
training step: 45474, total_loss: 0.9792632460594177
training step: 45475, total_loss: 0.510058581829071
training step: 45476, total_loss: 0.8663817644119263
training step: 45477, total_loss: 2.756840705871582
training step: 45478, total_loss: 2.9406304359436035
training step: 45479, total_loss: 3.7045485973358154
training step: 45480, total_loss: 1.3401761054992676
training step: 45481, total_loss: 1.4433907270431519
training step: 45482, total_loss: 0.7622520923614502
training step: 45483, total_loss: 1.9486703872680664
training step: 45484, total_loss: 2.436765193939209
training step: 45485, total_loss: 0.027459075674414635
training step: 45486, total_loss: 2.069826126098633
training step: 45487, total_loss: 0.1771198809146881
training step: 45488, total_loss: 1.3849705457687378
training step: 45489, total_loss: 1.7318191528320312
training step: 45490, total_loss: 1.7703027725219727
training step: 45491, total_loss: 1.301086664199829
training step: 45492, total_loss: 2.26539945602417
training step: 45493, total_loss: 0.6377044320106506
training step: 45494, total_loss: 0.9390798211097717
training step: 45495, total_loss: 0.3711085319519043
training step: 45496, total_loss: 1.5835593938827515
training step: 45497, total_loss: 1.0972728729248047
training step: 45498, total_loss: 0.019183047115802765
training step: 45499, total_loss: 4.2201409339904785
training step: 45500, total_loss: 0.25238120555877686
training step: 45501, total_loss: 1.328430414199829
training step: 45502, total_loss: 1.9795891046524048
training step: 45503, total_loss: 0.5877572298049927
training step: 45504, total_loss: 2.9379029273986816
training step: 45505, total_loss: 3.5197620391845703
training step: 45506, total_loss: 1.3009341955184937
training step: 45507, total_loss: 2.150688648223877
training step: 45508, total_loss: 0.729881227016449
training step: 45509, total_loss: 1.4386638402938843
training step: 45510, total_loss: 0.1556946337223053
training step: 45511, total_loss: 1.2266874313354492
training step: 45512, total_loss: 2.665330648422241
training step: 45513, total_loss: 1.480508804321289
training step: 45514, total_loss: 0.8177353143692017
training step: 45515, total_loss: 2.4280197620391846
training step: 45516, total_loss: 0.2845223546028137
training step: 45517, total_loss: 0.7486721277236938
training step: 45518, total_loss: 2.164426326751709
training step: 45519, total_loss: 1.5884263515472412
training step: 45520, total_loss: 0.7756338715553284
training step: 45521, total_loss: 0.8891386985778809
training step: 45522, total_loss: 1.216078281402588
training step: 45523, total_loss: 0.7552957534790039
training step: 45524, total_loss: 7.0930986404418945
training step: 45525, total_loss: 0.12398864328861237
training step: 45526, total_loss: 0.7999299168586731
training step: 45527, total_loss: 3.6226768493652344
training step: 45528, total_loss: 0.5543540716171265
training step: 45529, total_loss: 0.7338845729827881
training step: 45530, total_loss: 0.7008994817733765
training step: 45531, total_loss: 1.3313214778900146
training step: 45532, total_loss: 2.5138485431671143
training step: 45533, total_loss: 2.8979358673095703
training step: 45534, total_loss: 3.44903302192688
training step: 45535, total_loss: 1.6141690015792847
training step: 45536, total_loss: 1.569653868675232
training step: 45537, total_loss: 1.6119279861450195
training step: 45538, total_loss: 4.763016700744629
training step: 45539, total_loss: 2.1949102878570557
training step: 45540, total_loss: 2.0922465324401855
training step: 45541, total_loss: 0.113439179956913
training step: 45542, total_loss: 2.632880687713623
training step: 45543, total_loss: 2.072190999984741
training step: 45544, total_loss: 1.8162624835968018
training step: 45545, total_loss: 1.164543628692627
training step: 45546, total_loss: 0.15740272402763367
training step: 45547, total_loss: 2.506176471710205
training step: 45548, total_loss: 1.0632821321487427
training step: 45549, total_loss: 1.983428716659546
training step: 45550, total_loss: 0.8584884405136108
training step: 45551, total_loss: 2.759833335876465
training step: 45552, total_loss: 0.5604554414749146
training step: 45553, total_loss: 2.251354932785034
training step: 45554, total_loss: 0.4474993348121643
training step: 45555, total_loss: 0.9886844158172607
training step: 45556, total_loss: 1.3490058183670044
training step: 45557, total_loss: 2.3906755447387695
training step: 45558, total_loss: 1.7898082733154297
training step: 45559, total_loss: 1.3746109008789062
training step: 45560, total_loss: 1.5441222190856934
training step: 45561, total_loss: 1.5417572259902954
training step: 45562, total_loss: 2.271409273147583
training step: 45563, total_loss: 1.2944962978363037
training step: 45564, total_loss: 0.7429527640342712
training step: 45565, total_loss: 2.456134796142578
training step: 45566, total_loss: 2.636784553527832
training step: 45567, total_loss: 2.215747833251953
training step: 45568, total_loss: 1.985861897468567
training step: 45569, total_loss: 2.8530328273773193
training step: 45570, total_loss: 1.4710179567337036
training step: 45571, total_loss: 0.7815139293670654
training step: 45572, total_loss: 0.5837001204490662
training step: 45573, total_loss: 1.824709177017212
training step: 45574, total_loss: 2.1675851345062256
training step: 45575, total_loss: 1.644433617591858
training step: 45576, total_loss: 1.601076364517212
training step: 45577, total_loss: 1.5316457748413086
training step: 45578, total_loss: 2.896564483642578
training step: 45579, total_loss: 0.5200725197792053
training step: 45580, total_loss: 1.3789472579956055
training step: 45581, total_loss: 1.9186086654663086
training step: 45582, total_loss: 1.3571581840515137
training step: 45583, total_loss: 0.2368849813938141
training step: 45584, total_loss: 1.6238534450531006
training step: 45585, total_loss: 1.6947712898254395
training step: 45586, total_loss: 1.8189976215362549
training step: 45587, total_loss: 1.8615150451660156
training step: 45588, total_loss: 2.5812859535217285
training step: 45589, total_loss: 2.004546642303467
training step: 45590, total_loss: 0.5164800882339478
training step: 45591, total_loss: 1.8794294595718384
training step: 45592, total_loss: 1.8752477169036865
training step: 45593, total_loss: 1.102906584739685
training step: 45594, total_loss: 0.8916956782341003
training step: 45595, total_loss: 1.1608493328094482
training step: 45596, total_loss: 2.3448691368103027
training step: 45597, total_loss: 1.1346937417984009
training step: 45598, total_loss: 1.7592668533325195
training step: 45599, total_loss: 3.563856601715088
training step: 45600, total_loss: 2.486539125442505
training step: 45601, total_loss: 3.244199752807617
training step: 45602, total_loss: 1.1029924154281616
training step: 45603, total_loss: 0.7797476053237915
training step: 45604, total_loss: 0.8106632232666016
training step: 45605, total_loss: 1.1041784286499023
training step: 45606, total_loss: 0.7437770962715149
training step: 45607, total_loss: 1.8250077962875366
training step: 45608, total_loss: 1.9766416549682617
training step: 45609, total_loss: 3.215674638748169
training step: 45610, total_loss: 0.4058626890182495
training step: 45611, total_loss: 1.9320778846740723
training step: 45612, total_loss: 0.13745351135730743
training step: 45613, total_loss: 0.6743672490119934
training step: 45614, total_loss: 1.414245843887329
training step: 45615, total_loss: 0.12596780061721802
training step: 45616, total_loss: 0.17983821034431458
training step: 45617, total_loss: 0.5390068292617798
training step: 45618, total_loss: 0.6788888573646545
training step: 45619, total_loss: 4.277894973754883
training step: 45620, total_loss: 0.6458625793457031
training step: 45621, total_loss: 1.1563034057617188
training step: 45622, total_loss: 2.5693838596343994
training step: 45623, total_loss: 3.0176117420196533
training step: 45624, total_loss: 1.7471792697906494
training step: 45625, total_loss: 2.442417621612549
training step: 45626, total_loss: 2.7802891731262207
training step: 45627, total_loss: 1.8540302515029907
training step: 45628, total_loss: 1.7641801834106445
training step: 45629, total_loss: 1.8192675113677979
training step: 45630, total_loss: 0.7090526819229126
training step: 45631, total_loss: 1.6080334186553955
training step: 45632, total_loss: 0.6206668019294739
training step: 45633, total_loss: 2.039304256439209
training step: 45634, total_loss: 1.464634656906128
training step: 45635, total_loss: 0.005039874464273453
training step: 45636, total_loss: 0.4308120608329773
training step: 45637, total_loss: 4.236213207244873
training step: 45638, total_loss: 1.1417787075042725
training step: 45639, total_loss: 1.1992363929748535
training step: 45640, total_loss: 0.8036247491836548
training step: 45641, total_loss: 1.881612777709961
training step: 45642, total_loss: 3.7526438236236572
training step: 45643, total_loss: 1.2263596057891846
training step: 45644, total_loss: 0.9979856014251709
training step: 45645, total_loss: 4.082469463348389
training step: 45646, total_loss: 1.4093363285064697
training step: 45647, total_loss: 1.1091848611831665
training step: 45648, total_loss: 0.5241920948028564
training step: 45649, total_loss: 1.9618895053863525
training step: 45650, total_loss: 1.8295217752456665
training step: 45651, total_loss: 1.7883411645889282
training step: 45652, total_loss: 3.385981559753418
training step: 45653, total_loss: 3.8171658515930176
training step: 45654, total_loss: 0.02214236557483673
training step: 45655, total_loss: 1.0356556177139282
training step: 45656, total_loss: 2.6478536128997803
training step: 45657, total_loss: 5.514065742492676
training step: 45658, total_loss: 0.05361783504486084
training step: 45659, total_loss: 3.2038235664367676
training step: 45660, total_loss: 0.06892538070678711
training step: 45661, total_loss: 0.16991031169891357
training step: 45662, total_loss: 3.2035045623779297
training step: 45663, total_loss: 2.8776679039001465
training step: 45664, total_loss: 2.112217903137207
training step: 45665, total_loss: 1.875928521156311
training step: 45666, total_loss: 2.765894889831543
training step: 45667, total_loss: 1.46737539768219
training step: 45668, total_loss: 1.4418365955352783
training step: 45669, total_loss: 0.12715071439743042
training step: 45670, total_loss: 0.8692845106124878
training step: 45671, total_loss: 3.571070909500122
training step: 45672, total_loss: 2.5824267864227295
training step: 45673, total_loss: 4.766622543334961
training step: 45674, total_loss: 0.9149580001831055
training step: 45675, total_loss: 3.975545644760132
training step: 45676, total_loss: 3.645019054412842
training step: 45677, total_loss: 2.086552858352661
training step: 45678, total_loss: 1.2739839553833008
training step: 45679, total_loss: 0.2778664231300354
training step: 45680, total_loss: 4.287837505340576
training step: 45681, total_loss: 0.5996274948120117
training step: 45682, total_loss: 1.0176681280136108
training step: 45683, total_loss: 1.2814029455184937
training step: 45684, total_loss: 1.8383878469467163
training step: 45685, total_loss: 2.406780242919922
training step: 45686, total_loss: 2.0360074043273926
training step: 45687, total_loss: 2.3598337173461914
training step: 45688, total_loss: 0.8749657869338989
training step: 45689, total_loss: 0.7920787334442139
training step: 45690, total_loss: 0.8534660935401917
training step: 45691, total_loss: 2.3824827671051025
training step: 45692, total_loss: 1.1965770721435547
training step: 45693, total_loss: 1.8016647100448608
training step: 45694, total_loss: 1.9769635200500488
training step: 45695, total_loss: 0.2189730405807495
training step: 45696, total_loss: 0.30632612109184265
training step: 45697, total_loss: 2.7257628440856934
training step: 45698, total_loss: 0.60980224609375
training step: 45699, total_loss: 1.3567111492156982
training step: 45700, total_loss: 2.452669382095337
training step: 45701, total_loss: 6.382981300354004
training step: 45702, total_loss: 2.1005070209503174
training step: 45703, total_loss: 1.4926023483276367
training step: 45704, total_loss: 4.152344226837158
training step: 45705, total_loss: 1.0289688110351562
training step: 45706, total_loss: 3.640660524368286
training step: 45707, total_loss: 2.754779577255249
training step: 45708, total_loss: 0.19092240929603577
training step: 45709, total_loss: 1.4260084629058838
training step: 45710, total_loss: 1.5543453693389893
training step: 45711, total_loss: 1.2641671895980835
training step: 45712, total_loss: 0.27730095386505127
training step: 45713, total_loss: 0.747171938419342
training step: 45714, total_loss: 1.433601975440979
training step: 45715, total_loss: 0.7891565561294556
training step: 45716, total_loss: 3.911384105682373
training step: 45717, total_loss: 1.1724236011505127
training step: 45718, total_loss: 1.048352837562561
training step: 45719, total_loss: 2.9206323623657227
training step: 45720, total_loss: 0.4269300699234009
training step: 45721, total_loss: 1.2951408624649048
training step: 45722, total_loss: 1.6585595607757568
training step: 45723, total_loss: 1.2458769083023071
training step: 45724, total_loss: 1.491598129272461
training step: 45725, total_loss: 3.597644567489624
training step: 45726, total_loss: 1.2402286529541016
training step: 45727, total_loss: 2.511599540710449
training step: 45728, total_loss: 0.020085839554667473
training step: 45729, total_loss: 3.657947540283203
training step: 45730, total_loss: 0.8238838911056519
training step: 45731, total_loss: 1.4500998258590698
training step: 45732, total_loss: 0.14078588783740997
training step: 45733, total_loss: 1.2807072401046753
training step: 45734, total_loss: 3.840804100036621
training step: 45735, total_loss: 1.7350562810897827
training step: 45736, total_loss: 5.565019607543945
training step: 45737, total_loss: 1.6283066272735596
training step: 45738, total_loss: 0.6907934546470642
training step: 45739, total_loss: 0.9204855561256409
training step: 45740, total_loss: 0.14898434281349182
training step: 45741, total_loss: 1.195520043373108
training step: 45742, total_loss: 0.6977845430374146
training step: 45743, total_loss: 1.7451837062835693
training step: 45744, total_loss: 3.6183581352233887
training step: 45745, total_loss: 1.2041140794754028
training step: 45746, total_loss: 1.9587981700897217
training step: 45747, total_loss: 1.9341392517089844
training step: 45748, total_loss: 4.414679527282715
training step: 45749, total_loss: 0.7693479061126709
training step: 45750, total_loss: 0.9301357269287109
training step: 45751, total_loss: 0.5987732410430908
training step: 45752, total_loss: 2.3849971294403076
training step: 45753, total_loss: 2.0041871070861816
training step: 45754, total_loss: 0.5500468015670776
training step: 45755, total_loss: 1.9146419763565063
training step: 45756, total_loss: 0.42955493927001953
training step: 45757, total_loss: 0.5072511434555054
training step: 45758, total_loss: 4.741634368896484
training step: 45759, total_loss: 2.3533248901367188
training step: 45760, total_loss: 0.05396031588315964
training step: 45761, total_loss: 2.288005828857422
training step: 45762, total_loss: 5.353780746459961
training step: 45763, total_loss: 2.646878242492676
training step: 45764, total_loss: 0.1478138417005539
training step: 45765, total_loss: 1.8060386180877686
training step: 45766, total_loss: 1.908130407333374
training step: 45767, total_loss: 1.842936635017395
training step: 45768, total_loss: 2.3716678619384766
training step: 45769, total_loss: 0.6960194706916809
training step: 45770, total_loss: 1.912916898727417
training step: 45771, total_loss: 1.2587621212005615
training step: 45772, total_loss: 1.8965613842010498
training step: 45773, total_loss: 0.7945380210876465
training step: 45774, total_loss: 2.3537425994873047
training step: 45775, total_loss: 2.1029715538024902
training step: 45776, total_loss: 7.150432586669922
training step: 45777, total_loss: 0.8244801759719849
training step: 45778, total_loss: 1.3342496156692505
training step: 45779, total_loss: 1.9717084169387817
training step: 45780, total_loss: 0.6078233122825623
training step: 45781, total_loss: 2.387788772583008
training step: 45782, total_loss: 1.5098114013671875
training step: 45783, total_loss: 2.3815808296203613
training step: 45784, total_loss: 4.032782554626465
training step: 45785, total_loss: 2.102473497390747
training step: 45786, total_loss: 0.35700756311416626
training step: 45787, total_loss: 2.471986770629883
training step: 45788, total_loss: 0.33821409940719604
training step: 45789, total_loss: 2.419431209564209
training step: 45790, total_loss: 1.5805410146713257
training step: 45791, total_loss: 1.2531201839447021
training step: 45792, total_loss: 2.6244921684265137
training step: 45793, total_loss: 0.062212880700826645
training step: 45794, total_loss: 2.318237781524658
training step: 45795, total_loss: 2.959958076477051
training step: 45796, total_loss: 0.5102576017379761
training step: 45797, total_loss: 2.674862861633301
training step: 45798, total_loss: 2.4275388717651367
training step: 45799, total_loss: 2.6539764404296875
training step: 45800, total_loss: 0.9162819981575012
training step: 45801, total_loss: 4.573352813720703
training step: 45802, total_loss: 0.417341023683548
training step: 45803, total_loss: 0.8946037292480469
training step: 45804, total_loss: 0.7820684313774109
training step: 45805, total_loss: 3.8139002323150635
training step: 45806, total_loss: 1.590949535369873
training step: 45807, total_loss: 0.21236012876033783
training step: 45808, total_loss: 1.434604525566101
training step: 45809, total_loss: 2.2616705894470215
training step: 45810, total_loss: 4.285099029541016
training step: 45811, total_loss: 1.1963615417480469
training step: 45812, total_loss: 1.9568111896514893
training step: 45813, total_loss: 1.3784099817276
training step: 45814, total_loss: 2.411597967147827
training step: 45815, total_loss: 0.7914786338806152
training step: 45816, total_loss: 5.062910556793213
training step: 45817, total_loss: 0.40649229288101196
training step: 45818, total_loss: 0.9632337093353271
training step: 45819, total_loss: 5.824832439422607
training step: 45820, total_loss: 0.8387038111686707
training step: 45821, total_loss: 0.5110294222831726
training step: 45822, total_loss: 3.171653985977173
training step: 45823, total_loss: 1.7137351036071777
training step: 45824, total_loss: 0.7609275579452515
training step: 45825, total_loss: 2.3490023612976074
training step: 45826, total_loss: 1.4610874652862549
training step: 45827, total_loss: 1.0685447454452515
training step: 45828, total_loss: 1.1887563467025757
training step: 45829, total_loss: 2.51371693611145
training step: 45830, total_loss: 3.5273470878601074
training step: 45831, total_loss: 2.5541656017303467
training step: 45832, total_loss: 3.013329029083252
training step: 45833, total_loss: 2.7052974700927734
training step: 45834, total_loss: 3.1502034664154053
training step: 45835, total_loss: 0.6556878089904785
training step: 45836, total_loss: 3.9776341915130615
training step: 45837, total_loss: 3.473546028137207
training step: 45838, total_loss: 3.2648425102233887
training step: 45839, total_loss: 1.7750217914581299
training step: 45840, total_loss: 1.5141215324401855
training step: 45841, total_loss: 1.430227518081665
training step: 45842, total_loss: 0.49293702840805054
training step: 45843, total_loss: 2.8209307193756104
training step: 45844, total_loss: 1.6750483512878418
training step: 45845, total_loss: 3.751701593399048
training step: 45846, total_loss: 0.28170356154441833
training step: 45847, total_loss: 1.908923864364624
training step: 45848, total_loss: 0.952570915222168
training step: 45849, total_loss: 1.6272600889205933
training step: 45850, total_loss: 2.464326858520508
training step: 45851, total_loss: 0.5686353445053101
training step: 45852, total_loss: 5.511175155639648
training step: 45853, total_loss: 0.6934504508972168
training step: 45854, total_loss: 2.2923583984375
training step: 45855, total_loss: 1.508745789527893
training step: 45856, total_loss: 0.9323866963386536
training step: 45857, total_loss: 2.9029972553253174
training step: 45858, total_loss: 2.5223278999328613
training step: 45859, total_loss: 2.8489322662353516
training step: 45860, total_loss: 1.7852654457092285
training step: 45861, total_loss: 2.0217230319976807
training step: 45862, total_loss: 2.9528017044067383
training step: 45863, total_loss: 0.9971509575843811
training step: 45864, total_loss: 0.7431209087371826
training step: 45865, total_loss: 0.5087888836860657
training step: 45866, total_loss: 1.1010007858276367
training step: 45867, total_loss: 2.2452497482299805
training step: 45868, total_loss: 1.7351531982421875
training step: 45869, total_loss: 3.8329639434814453
training step: 45870, total_loss: 2.974442720413208
training step: 45871, total_loss: 3.0181093215942383
training step: 45872, total_loss: 2.1361708641052246
training step: 45873, total_loss: 1.894086480140686
training step: 45874, total_loss: 2.6003990173339844
training step: 45875, total_loss: 0.6880987882614136
training step: 45876, total_loss: 3.5378036499023438
training step: 45877, total_loss: 0.16419148445129395
training step: 45878, total_loss: 0.1530459225177765
training step: 45879, total_loss: 1.0313401222229004
training step: 45880, total_loss: 2.807544231414795
training step: 45881, total_loss: 3.0947928428649902
training step: 45882, total_loss: 2.787923812866211
training step: 45883, total_loss: 3.690049409866333
training step: 45884, total_loss: 2.8129212856292725
training step: 45885, total_loss: 1.2831491231918335
training step: 45886, total_loss: 2.169308662414551INFO:tensorflow:Writing predictions to: test_output/predictions_46000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_46000.json

training step: 45887, total_loss: 2.637939929962158
training step: 45888, total_loss: 0.24675612151622772
training step: 45889, total_loss: 1.4604136943817139
training step: 45890, total_loss: 3.081737995147705
training step: 45891, total_loss: 1.7035399675369263
training step: 45892, total_loss: 2.126110792160034
training step: 45893, total_loss: 1.6666643619537354
training step: 45894, total_loss: 2.714383125305176
training step: 45895, total_loss: 0.26370760798454285
training step: 45896, total_loss: 3.2241735458374023
training step: 45897, total_loss: 1.391714096069336
training step: 45898, total_loss: 0.8478268384933472
training step: 45899, total_loss: 2.6758415699005127
training step: 45900, total_loss: 2.677016019821167
training step: 45901, total_loss: 2.8696365356445312
training step: 45902, total_loss: 5.871258735656738
training step: 45903, total_loss: 2.2130935192108154
training step: 45904, total_loss: 1.4195356369018555
training step: 45905, total_loss: 0.4564295709133148
training step: 45906, total_loss: 0.1327461302280426
training step: 45907, total_loss: 1.6672333478927612
training step: 45908, total_loss: 5.0623369216918945
training step: 45909, total_loss: 0.8868622779846191
training step: 45910, total_loss: 3.057168960571289
training step: 45911, total_loss: 0.35240647196769714
training step: 45912, total_loss: 1.4783676862716675
training step: 45913, total_loss: 1.8200839757919312
training step: 45914, total_loss: 1.7164547443389893
training step: 45915, total_loss: 0.7284040451049805
training step: 45916, total_loss: 2.736461639404297
training step: 45917, total_loss: 0.32309025526046753
training step: 45918, total_loss: 1.7298927307128906
training step: 45919, total_loss: 3.3074097633361816
training step: 45920, total_loss: 1.4262630939483643
training step: 45921, total_loss: 1.3772374391555786
training step: 45922, total_loss: 1.6207468509674072
training step: 45923, total_loss: 2.2583422660827637
training step: 45924, total_loss: 1.681144118309021
training step: 45925, total_loss: 1.0041208267211914
training step: 45926, total_loss: 0.9089357852935791
training step: 45927, total_loss: 2.745095729827881
training step: 45928, total_loss: 1.1371474266052246
training step: 45929, total_loss: 2.3612799644470215
training step: 45930, total_loss: 0.9404493570327759
training step: 45931, total_loss: 1.5302670001983643
training step: 45932, total_loss: 0.5405954122543335
training step: 45933, total_loss: 0.32048875093460083
training step: 45934, total_loss: 1.740912914276123
training step: 45935, total_loss: 0.6660560965538025
training step: 45936, total_loss: 4.93019962310791
training step: 45937, total_loss: 5.856569290161133
training step: 45938, total_loss: 0.6739543676376343
training step: 45939, total_loss: 1.9960697889328003
training step: 45940, total_loss: 0.30389055609703064
training step: 45941, total_loss: 2.0217676162719727
training step: 45942, total_loss: 1.9701550006866455
training step: 45943, total_loss: 1.723168969154358
training step: 45944, total_loss: 1.6014854907989502
training step: 45945, total_loss: 0.12229356169700623
training step: 45946, total_loss: 0.46554145216941833
training step: 45947, total_loss: 1.5053737163543701
training step: 45948, total_loss: 1.4129135608673096
training step: 45949, total_loss: 3.1906018257141113
training step: 45950, total_loss: 1.2601734399795532
training step: 45951, total_loss: 1.6157939434051514
training step: 45952, total_loss: 1.3904831409454346
training step: 45953, total_loss: 2.430328845977783
training step: 45954, total_loss: 3.0751757621765137
training step: 45955, total_loss: 3.954909563064575
training step: 45956, total_loss: 0.22401484847068787
training step: 45957, total_loss: 2.3972859382629395
training step: 45958, total_loss: 1.1249278783798218
training step: 45959, total_loss: 4.729730606079102
training step: 45960, total_loss: 0.15140187740325928
training step: 45961, total_loss: 0.20402613282203674
training step: 45962, total_loss: 0.21262049674987793
training step: 45963, total_loss: 1.5277072191238403
training step: 45964, total_loss: 3.0274100303649902
training step: 45965, total_loss: 3.711768627166748
training step: 45966, total_loss: 0.6253738403320312
training step: 45967, total_loss: 0.44982051849365234
training step: 45968, total_loss: 0.46372437477111816
training step: 45969, total_loss: 0.38079315423965454
training step: 45970, total_loss: 1.3833324909210205
training step: 45971, total_loss: 0.003654732136055827
training step: 45972, total_loss: 0.3697277307510376
training step: 45973, total_loss: 3.388528823852539
training step: 45974, total_loss: 1.6405969858169556
training step: 45975, total_loss: 4.525512218475342
training step: 45976, total_loss: 2.2034595012664795
training step: 45977, total_loss: 2.6842823028564453
training step: 45978, total_loss: 3.226004123687744
training step: 45979, total_loss: 1.4664480686187744
training step: 45980, total_loss: 0.8951969146728516
training step: 45981, total_loss: 1.294465184211731
training step: 45982, total_loss: 2.161181688308716
training step: 45983, total_loss: 4.36035680770874
training step: 45984, total_loss: 0.4304869771003723
training step: 45985, total_loss: 0.20239663124084473
training step: 45986, total_loss: 2.5739338397979736
training step: 45987, total_loss: 1.4044746160507202
training step: 45988, total_loss: 2.9177041053771973
training step: 45989, total_loss: 1.3023982048034668
training step: 45990, total_loss: 2.852100133895874
training step: 45991, total_loss: 2.8401620388031006
training step: 45992, total_loss: 0.4876686632633209
training step: 45993, total_loss: 5.081941604614258
training step: 45994, total_loss: 1.22416353225708
training step: 45995, total_loss: 3.1848182678222656
training step: 45996, total_loss: 1.1349971294403076
training step: 45997, total_loss: 2.8746495246887207
training step: 45998, total_loss: 0.49744492769241333
training step: 45999, total_loss: 5.093413829803467
training step: 46000, total_loss: 0.4491725564002991
epoch finished! shuffle=False
evaluation: 10000, total_loss: 1.8621206283569336, f1: 54.640233297117035, followup: 19.77787920279933, yesno: 78.94416552563517, heq: 49.003499163243575, dheq: 3.0

Model saved in path test_output//model_46000.ckpt
training step: 46001, total_loss: 2.2165799140930176
training step: 46002, total_loss: 2.180060386657715
training step: 46003, total_loss: 2.4595398902893066
training step: 46004, total_loss: 1.5832161903381348
training step: 46005, total_loss: 3.0904242992401123
training step: 46006, total_loss: 4.682027339935303
training step: 46007, total_loss: 2.5177454948425293
training step: 46008, total_loss: 1.629044532775879
training step: 46009, total_loss: 0.6129069328308105
training step: 46010, total_loss: 3.217440128326416
training step: 46011, total_loss: 0.259470134973526
training step: 46012, total_loss: 2.1869029998779297
training step: 46013, total_loss: 2.2915916442871094
training step: 46014, total_loss: 2.2675395011901855
training step: 46015, total_loss: 2.756948709487915
training step: 46016, total_loss: 0.4317452311515808
training step: 46017, total_loss: 2.3806819915771484
training step: 46018, total_loss: 2.628467559814453
training step: 46019, total_loss: 0.7775218486785889
training step: 46020, total_loss: 3.4696860313415527
training step: 46021, total_loss: 1.0103105306625366
training step: 46022, total_loss: 0.3943246006965637
training step: 46023, total_loss: 0.18428321182727814
training step: 46024, total_loss: 2.029064655303955
training step: 46025, total_loss: 0.40237128734588623
training step: 46026, total_loss: 1.8838202953338623
training step: 46027, total_loss: 2.735628604888916
training step: 46028, total_loss: 2.5066475868225098
training step: 46029, total_loss: 2.6548819541931152
training step: 46030, total_loss: 0.5774238705635071
training step: 46031, total_loss: 1.5165914297103882
training step: 46032, total_loss: 4.481199741363525
training step: 46033, total_loss: 3.9870777130126953
training step: 46034, total_loss: 4.785675048828125
training step: 46035, total_loss: 0.8541253805160522
training step: 46036, total_loss: 3.9664814472198486
training step: 46037, total_loss: 1.8328464031219482
training step: 46038, total_loss: 3.2896881103515625
training step: 46039, total_loss: 0.2877815365791321
training step: 46040, total_loss: 2.0430092811584473
training step: 46041, total_loss: 1.2430410385131836
training step: 46042, total_loss: 2.809567451477051
training step: 46043, total_loss: 4.22123908996582
training step: 46044, total_loss: 4.716102600097656
training step: 46045, total_loss: 0.6326223611831665
training step: 46046, total_loss: 1.137317180633545
training step: 46047, total_loss: 2.350048542022705
training step: 46048, total_loss: 2.49821138381958
training step: 46049, total_loss: 1.1912909746170044
training step: 46050, total_loss: 0.36253559589385986
training step: 46051, total_loss: 1.6357486248016357
training step: 46052, total_loss: 1.3549002408981323
training step: 46053, total_loss: 2.1132466793060303
training step: 46054, total_loss: 0.7211631536483765
training step: 46055, total_loss: 2.6986260414123535
training step: 46056, total_loss: 3.4227921962738037
training step: 46057, total_loss: 1.9974989891052246
training step: 46058, total_loss: 0.37519407272338867
training step: 46059, total_loss: 0.3131697177886963
training step: 46060, total_loss: 1.4882441759109497
training step: 46061, total_loss: 1.684037446975708
training step: 46062, total_loss: 3.4408247470855713
training step: 46063, total_loss: 1.7170361280441284
training step: 46064, total_loss: 2.2723658084869385
training step: 46065, total_loss: 1.2288727760314941
training step: 46066, total_loss: 2.1590070724487305
training step: 46067, total_loss: 3.335359573364258
training step: 46068, total_loss: 2.380918502807617
training step: 46069, total_loss: 2.321786880493164
training step: 46070, total_loss: 1.4565074443817139
training step: 46071, total_loss: 0.7060116529464722
training step: 46072, total_loss: 1.222426176071167
training step: 46073, total_loss: 0.39012524485588074
training step: 46074, total_loss: 1.89845871925354
training step: 46075, total_loss: 3.0705580711364746
training step: 46076, total_loss: 2.4600651264190674
training step: 46077, total_loss: 2.8648791313171387
training step: 46078, total_loss: 0.26474863290786743
training step: 46079, total_loss: 1.3907582759857178
training step: 46080, total_loss: 0.8817939758300781
training step: 46081, total_loss: 2.2117702960968018
training step: 46082, total_loss: 0.6480375528335571
training step: 46083, total_loss: 1.7887327671051025
training step: 46084, total_loss: 1.5131864547729492
training step: 46085, total_loss: 1.578195571899414
training step: 46086, total_loss: 0.4489330053329468
training step: 46087, total_loss: 0.8240989446640015
training step: 46088, total_loss: 1.5060759782791138
training step: 46089, total_loss: 1.3665132522583008
training step: 46090, total_loss: 2.454416275024414
training step: 46091, total_loss: 1.1270700693130493
training step: 46092, total_loss: 1.8424246311187744
training step: 46093, total_loss: 1.749276876449585
training step: 46094, total_loss: 0.36118799448013306
training step: 46095, total_loss: 1.1273505687713623
training step: 46096, total_loss: 0.05563804507255554
training step: 46097, total_loss: 2.526829719543457
training step: 46098, total_loss: 0.3350928723812103
training step: 46099, total_loss: 3.952087163925171
training step: 46100, total_loss: 1.556124210357666
training step: 46101, total_loss: 2.8054213523864746
training step: 46102, total_loss: 0.9109153747558594
training step: 46103, total_loss: 1.5469533205032349
training step: 46104, total_loss: 1.3793959617614746
training step: 46105, total_loss: 1.8321853876113892
training step: 46106, total_loss: 1.0123844146728516
training step: 46107, total_loss: 1.1504039764404297
training step: 46108, total_loss: 0.15078595280647278
training step: 46109, total_loss: 3.032310962677002
training step: 46110, total_loss: 0.3675379455089569
training step: 46111, total_loss: 1.1623069047927856
training step: 46112, total_loss: 3.987292766571045
training step: 46113, total_loss: 0.09443874657154083
training step: 46114, total_loss: 3.0729689598083496
training step: 46115, total_loss: 1.8029488325119019
training step: 46116, total_loss: 1.0180997848510742
training step: 46117, total_loss: 3.4941556453704834
training step: 46118, total_loss: 1.9650299549102783
training step: 46119, total_loss: 3.449395179748535
training step: 46120, total_loss: 1.8509292602539062
training step: 46121, total_loss: 3.113252639770508
training step: 46122, total_loss: 0.8150777220726013
training step: 46123, total_loss: 1.6445386409759521
training step: 46124, total_loss: 1.726837396621704
training step: 46125, total_loss: 1.4466453790664673
training step: 46126, total_loss: 1.228771448135376
training step: 46127, total_loss: 0.5833954811096191
training step: 46128, total_loss: 3.1359808444976807
training step: 46129, total_loss: 1.8525655269622803
training step: 46130, total_loss: 3.4065802097320557
training step: 46131, total_loss: 1.971785306930542
training step: 46132, total_loss: 0.41525986790657043
training step: 46133, total_loss: 1.7541720867156982
training step: 46134, total_loss: 0.3135069012641907
training step: 46135, total_loss: 2.6412277221679688
training step: 46136, total_loss: 1.9135057926177979
training step: 46137, total_loss: 2.7799434661865234
training step: 46138, total_loss: 0.40632662177085876
training step: 46139, total_loss: 1.3312435150146484
training step: 46140, total_loss: 2.484938621520996
training step: 46141, total_loss: 1.2031526565551758
training step: 46142, total_loss: 3.9809093475341797
training step: 46143, total_loss: 1.8760007619857788
training step: 46144, total_loss: 1.1727631092071533
training step: 46145, total_loss: 0.002462784294039011
training step: 46146, total_loss: 0.0015550096286460757
training step: 46147, total_loss: 4.707607269287109
training step: 46148, total_loss: 1.5616942644119263
training step: 46149, total_loss: 2.5524115562438965
training step: 46150, total_loss: 1.272621989250183
training step: 46151, total_loss: 3.1903932094573975
training step: 46152, total_loss: 2.16854190826416
training step: 46153, total_loss: 0.11480450630187988
training step: 46154, total_loss: 2.2748773097991943
training step: 46155, total_loss: 2.2211360931396484
training step: 46156, total_loss: 2.3005211353302
training step: 46157, total_loss: 3.8635270595550537
training step: 46158, total_loss: 2.381666660308838
training step: 46159, total_loss: 2.2979297637939453
training step: 46160, total_loss: 0.717077910900116
training step: 46161, total_loss: 2.401232957839966
training step: 46162, total_loss: 1.2087607383728027
training step: 46163, total_loss: 1.112256407737732
training step: 46164, total_loss: 1.0874083042144775
training step: 46165, total_loss: 3.5844507217407227
training step: 46166, total_loss: 0.7074710130691528
training step: 46167, total_loss: 2.198488712310791
training step: 46168, total_loss: 1.6899149417877197
training step: 46169, total_loss: 0.659491777420044
training step: 46170, total_loss: 2.7124736309051514
training step: 46171, total_loss: 2.2782540321350098
training step: 46172, total_loss: 1.8695385456085205
training step: 46173, total_loss: 0.17667542397975922
training step: 46174, total_loss: 1.2434109449386597
training step: 46175, total_loss: 4.811996936798096
training step: 46176, total_loss: 2.180478096008301
training step: 46177, total_loss: 1.0183420181274414
training step: 46178, total_loss: 0.05133766680955887
training step: 46179, total_loss: 0.4996500313282013
training step: 46180, total_loss: 0.7821342945098877
training step: 46181, total_loss: 0.033674776554107666
training step: 46182, total_loss: 0.020801976323127747
training step: 46183, total_loss: 1.8749887943267822
training step: 46184, total_loss: 4.23232364654541
training step: 46185, total_loss: 1.7426941394805908
training step: 46186, total_loss: 0.7476308345794678
training step: 46187, total_loss: 1.9766790866851807
training step: 46188, total_loss: 2.2809929847717285
training step: 46189, total_loss: 1.2572757005691528
training step: 46190, total_loss: 0.3932914435863495
training step: 46191, total_loss: 0.251070111989975
training step: 46192, total_loss: 5.73861026763916
training step: 46193, total_loss: 0.9407422542572021
training step: 46194, total_loss: 1.4950833320617676
training step: 46195, total_loss: 0.7737964391708374
training step: 46196, total_loss: 2.069267988204956
training step: 46197, total_loss: 0.4256797432899475
training step: 46198, total_loss: 0.2226223200559616
training step: 46199, total_loss: 1.8848464488983154
training step: 46200, total_loss: 4.28493595123291
training step: 46201, total_loss: 0.2713472247123718
training step: 46202, total_loss: 3.1832098960876465
training step: 46203, total_loss: 1.0951972007751465
training step: 46204, total_loss: 0.49967312812805176
training step: 46205, total_loss: 1.2494479417800903
training step: 46206, total_loss: 0.002926237415522337
training step: 46207, total_loss: 1.337113380432129
training step: 46208, total_loss: 1.3073995113372803
training step: 46209, total_loss: 1.9980545043945312
training step: 46210, total_loss: 2.674102783203125
training step: 46211, total_loss: 3.481034755706787
training step: 46212, total_loss: 3.5367836952209473
training step: 46213, total_loss: 1.8836958408355713
training step: 46214, total_loss: 0.6467292308807373
training step: 46215, total_loss: 1.447607398033142
training step: 46216, total_loss: 1.0260392427444458
training step: 46217, total_loss: 0.9488269090652466
training step: 46218, total_loss: 2.040736675262451
training step: 46219, total_loss: 2.3345279693603516
training step: 46220, total_loss: 4.3514204025268555
training step: 46221, total_loss: 1.6197593212127686
training step: 46222, total_loss: 0.35108569264411926
training step: 46223, total_loss: 1.2437200546264648
training step: 46224, total_loss: 4.223522186279297
training step: 46225, total_loss: 2.0348591804504395
training step: 46226, total_loss: 1.30088472366333
training step: 46227, total_loss: 2.8626198768615723
training step: 46228, total_loss: 1.4264872074127197
training step: 46229, total_loss: 4.385662078857422
training step: 46230, total_loss: 0.3560187518596649
training step: 46231, total_loss: 0.9302113056182861
training step: 46232, total_loss: 0.6871935129165649
training step: 46233, total_loss: 1.4683367013931274
training step: 46234, total_loss: 0.20350541174411774
training step: 46235, total_loss: 1.1918331384658813
training step: 46236, total_loss: 2.063333511352539
training step: 46237, total_loss: 1.2616240978240967
training step: 46238, total_loss: 2.263227939605713
training step: 46239, total_loss: 1.5005995035171509
training step: 46240, total_loss: 1.673314094543457
training step: 46241, total_loss: 3.3086485862731934
training step: 46242, total_loss: 0.5461457371711731
training step: 46243, total_loss: 2.5458807945251465
training step: 46244, total_loss: 1.1051945686340332
training step: 46245, total_loss: 0.4751549959182739
training step: 46246, total_loss: 1.5150890350341797
training step: 46247, total_loss: 1.4205900430679321
training step: 46248, total_loss: 0.3619966506958008
training step: 46249, total_loss: 1.0771493911743164
training step: 46250, total_loss: 1.8516604900360107
training step: 46251, total_loss: 0.874662458896637
training step: 46252, total_loss: 1.066898226737976
training step: 46253, total_loss: 1.8632168769836426
training step: 46254, total_loss: 1.0085840225219727
training step: 46255, total_loss: 1.3438432216644287
training step: 46256, total_loss: 1.8680477142333984
training step: 46257, total_loss: 4.009951114654541
training step: 46258, total_loss: 1.6983160972595215
training step: 46259, total_loss: 3.571263313293457
training step: 46260, total_loss: 2.808961868286133
training step: 46261, total_loss: 1.8098832368850708
training step: 46262, total_loss: 0.3777482807636261
training step: 46263, total_loss: 1.6285873651504517
training step: 46264, total_loss: 5.796178817749023
training step: 46265, total_loss: 3.5114736557006836
training step: 46266, total_loss: 2.010833263397217
training step: 46267, total_loss: 1.8646752834320068
training step: 46268, total_loss: 0.003778652520850301
training step: 46269, total_loss: 1.8974429368972778
training step: 46270, total_loss: 3.150797128677368
training step: 46271, total_loss: 3.190241575241089
training step: 46272, total_loss: 4.107281684875488
training step: 46273, total_loss: 1.2090420722961426
training step: 46274, total_loss: 1.3459124565124512
training step: 46275, total_loss: 0.7461562156677246
training step: 46276, total_loss: 1.0067057609558105
training step: 46277, total_loss: 3.920802593231201
training step: 46278, total_loss: 0.6176611185073853
training step: 46279, total_loss: 2.548341751098633
training step: 46280, total_loss: 4.169921875
training step: 46281, total_loss: 2.0512828826904297
training step: 46282, total_loss: 4.922858238220215
training step: 46283, total_loss: 1.3010236024856567
training step: 46284, total_loss: 1.2748427391052246
training step: 46285, total_loss: 0.6865531206130981
training step: 46286, total_loss: 1.2583247423171997
training step: 46287, total_loss: 2.126373767852783
training step: 46288, total_loss: 1.7051563262939453
training step: 46289, total_loss: 1.4698249101638794
training step: 46290, total_loss: 2.3638315200805664
training step: 46291, total_loss: 1.820541262626648
training step: 46292, total_loss: 3.711912155151367
training step: 46293, total_loss: 0.9581907391548157
training step: 46294, total_loss: 1.7342443466186523
training step: 46295, total_loss: 1.9340448379516602
training step: 46296, total_loss: 0.0002740212366916239
training step: 46297, total_loss: 1.0322253704071045
training step: 46298, total_loss: 2.697267532348633
training step: 46299, total_loss: 1.4862515926361084
training step: 46300, total_loss: 0.6811578869819641
training step: 46301, total_loss: 2.0143942832946777
training step: 46302, total_loss: 1.062874436378479
training step: 46303, total_loss: 0.8780186176300049
training step: 46304, total_loss: 1.5413590669631958
training step: 46305, total_loss: 0.9936124086380005
training step: 46306, total_loss: 1.9962303638458252
training step: 46307, total_loss: 0.9426670074462891
training step: 46308, total_loss: 2.949204444885254
training step: 46309, total_loss: 4.8080644607543945
training step: 46310, total_loss: 1.9980051517486572
training step: 46311, total_loss: 2.6866252422332764
training step: 46312, total_loss: 0.5274177193641663
training step: 46313, total_loss: 1.6632325649261475
training step: 46314, total_loss: 1.5745855569839478
training step: 46315, total_loss: 0.6112957000732422
training step: 46316, total_loss: 1.675694227218628
training step: 46317, total_loss: 0.9504299163818359
training step: 46318, total_loss: 1.2526416778564453
training step: 46319, total_loss: 0.765689492225647
training step: 46320, total_loss: 6.003898620605469
training step: 46321, total_loss: 1.8984060287475586
training step: 46322, total_loss: 1.3194687366485596
training step: 46323, total_loss: 1.432835340499878
training step: 46324, total_loss: 1.1272876262664795
training step: 46325, total_loss: 1.6105315685272217
training step: 46326, total_loss: 0.05278635397553444
training step: 46327, total_loss: 3.5882656574249268
training step: 46328, total_loss: 1.000839352607727
training step: 46329, total_loss: 1.0925722122192383
training step: 46330, total_loss: 4.5268354415893555
training step: 46331, total_loss: 0.8019626140594482
training step: 46332, total_loss: 0.13153158128261566
training step: 46333, total_loss: 0.5334389209747314
training step: 46334, total_loss: 0.4065060317516327
training step: 46335, total_loss: 2.868443727493286
training step: 46336, total_loss: 4.5824503898620605
training step: 46337, total_loss: 1.380002737045288
training step: 46338, total_loss: 0.9165306091308594
training step: 46339, total_loss: 1.5241923332214355
training step: 46340, total_loss: 1.0489016771316528
training step: 46341, total_loss: 0.9741815328598022
training step: 46342, total_loss: 0.828999936580658
training step: 46343, total_loss: 0.7808318734169006
training step: 46344, total_loss: 1.398283839225769
training step: 46345, total_loss: 0.024801060557365417
training step: 46346, total_loss: 0.7605190277099609
training step: 46347, total_loss: 1.183242678642273
training step: 46348, total_loss: 4.659835338592529
training step: 46349, total_loss: 4.460337162017822
training step: 46350, total_loss: 3.0162301063537598
training step: 46351, total_loss: 0.0019227380398660898
training step: 46352, total_loss: 0.0025494929868727922
training step: 46353, total_loss: 1.8737733364105225
training step: 46354, total_loss: 1.1534481048583984
training step: 46355, total_loss: 3.878664970397949
training step: 46356, total_loss: 2.0215108394622803
training step: 46357, total_loss: 0.4401925802230835
training step: 46358, total_loss: 0.2661207914352417
training step: 46359, total_loss: 4.503081321716309
training step: 46360, total_loss: 1.7640970945358276
training step: 46361, total_loss: 2.2483696937561035
training step: 46362, total_loss: 2.2817602157592773
training step: 46363, total_loss: 1.4035634994506836
training step: 46364, total_loss: 4.155050277709961
training step: 46365, total_loss: 1.034650206565857
training step: 46366, total_loss: 0.5855363607406616
training step: 46367, total_loss: 4.189695358276367
training step: 46368, total_loss: 0.8960826396942139
training step: 46369, total_loss: 5.673701286315918
training step: 46370, total_loss: 0.7668768763542175
training step: 46371, total_loss: 1.0629727840423584
training step: 46372, total_loss: 4.075359344482422
training step: 46373, total_loss: 1.2774258852005005
training step: 46374, total_loss: 1.1362054347991943
training step: 46375, total_loss: 1.6901872158050537
training step: 46376, total_loss: 3.7956159114837646
training step: 46377, total_loss: 2.201650381088257
training step: 46378, total_loss: 1.951587200164795
training step: 46379, total_loss: 3.423351287841797
training step: 46380, total_loss: 3.5046651363372803
training step: 46381, total_loss: 1.5829472541809082
training step: 46382, total_loss: 1.7181109189987183
training step: 46383, total_loss: 1.3994262218475342
training step: 46384, total_loss: 4.077868461608887
training step: 46385, total_loss: 1.080640196800232
training step: 46386, total_loss: 3.710239887237549
training step: 46387, total_loss: 1.124804973602295
training step: 46388, total_loss: 2.5728797912597656
training step: 46389, total_loss: 0.6138085722923279
training step: 46390, total_loss: 3.4686226844787598
training step: 46391, total_loss: 3.114736557006836
training step: 46392, total_loss: 0.5525639653205872
training step: 46393, total_loss: 0.7445743083953857
training step: 46394, total_loss: 4.524511814117432
training step: 46395, total_loss: 5.2156524658203125
training step: 46396, total_loss: 2.323594331741333
training step: 46397, total_loss: 2.4087345600128174
training step: 46398, total_loss: 2.344329595565796
training step: 46399, total_loss: 5.613954067230225
training step: 46400, total_loss: 3.675652027130127
training step: 46401, total_loss: 1.7053661346435547
training step: 46402, total_loss: 1.1521999835968018
training step: 46403, total_loss: 2.923513412475586
training step: 46404, total_loss: 1.4373196363449097
training step: 46405, total_loss: 0.5228379964828491
training step: 46406, total_loss: 0.32916000485420227
training step: 46407, total_loss: 2.6515848636627197
training step: 46408, total_loss: 1.9307880401611328
training step: 46409, total_loss: 0.4806181490421295
training step: 46410, total_loss: 0.7131166458129883
training step: 46411, total_loss: 2.921612501144409
training step: 46412, total_loss: 1.4129645824432373
training step: 46413, total_loss: 1.4407707452774048
training step: 46414, total_loss: 1.8680566549301147
training step: 46415, total_loss: 1.0986881256103516
training step: 46416, total_loss: 3.0502772331237793
training step: 46417, total_loss: 2.45835542678833
training step: 46418, total_loss: 0.7165732383728027
training step: 46419, total_loss: 3.630669116973877
training step: 46420, total_loss: 0.5824684500694275
training step: 46421, total_loss: 1.7456551790237427
training step: 46422, total_loss: 0.5223584175109863
training step: 46423, total_loss: 1.0270545482635498
training step: 46424, total_loss: 1.333673119544983
training step: 46425, total_loss: 1.935378074645996
training step: 46426, total_loss: 3.3960845470428467
training step: 46427, total_loss: 1.9223246574401855
training step: 46428, total_loss: 0.20256054401397705
training step: 46429, total_loss: 1.2066006660461426
training step: 46430, total_loss: 0.6016615629196167
training step: 46431, total_loss: 1.1638182401657104
training step: 46432, total_loss: 2.3440327644348145
training step: 46433, total_loss: 3.83432674407959
training step: 46434, total_loss: 2.778533458709717
training step: 46435, total_loss: 2.1693170070648193
training step: 46436, total_loss: 2.1636478900909424
training step: 46437, total_loss: 2.4330055713653564
training step: 46438, total_loss: 0.7746808528900146
training step: 46439, total_loss: 2.088662624359131
training step: 46440, total_loss: 0.19921615719795227
training step: 46441, total_loss: 2.6379876136779785
training step: 46442, total_loss: 1.8943142890930176
training step: 46443, total_loss: 0.3755554258823395
training step: 46444, total_loss: 2.105875015258789
training step: 46445, total_loss: 1.2494919300079346
training step: 46446, total_loss: 2.1204895973205566
training step: 46447, total_loss: 2.201601982116699
training step: 46448, total_loss: 2.0765981674194336
training step: 46449, total_loss: 1.0776786804199219
training step: 46450, total_loss: 0.5216244459152222
training step: 46451, total_loss: 1.0183846950531006
training step: 46452, total_loss: 1.4029189348220825
training step: 46453, total_loss: 3.2707901000976562
training step: 46454, total_loss: 2.280357837677002
training step: 46455, total_loss: 1.2393171787261963
training step: 46456, total_loss: 2.2476918697357178
training step: 46457, total_loss: 1.7337119579315186
training step: 46458, total_loss: 0.2106335610151291
training step: 46459, total_loss: 2.656418800354004
training step: 46460, total_loss: 0.016718516126275063
training step: 46461, total_loss: 2.873843193054199
training step: 46462, total_loss: 2.56369948387146
training step: 46463, total_loss: 1.4314888715744019
training step: 46464, total_loss: 2.9807682037353516
training step: 46465, total_loss: 1.4846110343933105
training step: 46466, total_loss: 1.1656935214996338
training step: 46467, total_loss: 1.2287360429763794
training step: 46468, total_loss: 2.3086557388305664
training step: 46469, total_loss: 0.05695512145757675
training step: 46470, total_loss: 3.378652572631836
training step: 46471, total_loss: 0.33610495924949646
training step: 46472, total_loss: 4.909183502197266
training step: 46473, total_loss: 2.7207117080688477
training step: 46474, total_loss: 2.5714006423950195
training step: 46475, total_loss: 1.012523889541626
training step: 46476, total_loss: 0.5131091475486755
training step: 46477, total_loss: 1.2709749937057495
training step: 46478, total_loss: 2.1096527576446533
training step: 46479, total_loss: 2.868110179901123
training step: 46480, total_loss: 2.341254234313965
training step: 46481, total_loss: 0.3423447608947754
training step: 46482, total_loss: 1.6225173473358154
training step: 46483, total_loss: 1.582615852355957
training step: 46484, total_loss: 0.6477174162864685
training step: 46485, total_loss: 1.6474072933197021
training step: 46486, total_loss: 1.429824948310852
training step: 46487, total_loss: 2.2278988361358643
training step: 46488, total_loss: 2.108166217803955
training step: 46489, total_loss: 0.15210747718811035
training step: 46490, total_loss: 4.624239921569824
training step: 46491, total_loss: 0.44902104139328003
training step: 46492, total_loss: 4.675078392028809
training step: 46493, total_loss: 2.896329879760742
training step: 46494, total_loss: 4.121585369110107
training step: 46495, total_loss: 1.295251727104187
training step: 46496, total_loss: 1.3860244750976562
training step: 46497, total_loss: 2.350531578063965
training step: 46498, total_loss: 3.309718608856201
training step: 46499, total_loss: 1.5943697690963745
training step: 46500, total_loss: 2.644186019897461
training step: 46501, total_loss: 1.9859130382537842
training step: 46502, total_loss: 2.496554374694824
training step: 46503, total_loss: 1.068444848060608
training step: 46504, total_loss: 1.0813851356506348
training step: 46505, total_loss: 1.6397576332092285
training step: 46506, total_loss: 1.7631914615631104
training step: 46507, total_loss: 0.15939298272132874
training step: 46508, total_loss: 2.7549281120300293
training step: 46509, total_loss: 0.18774640560150146
training step: 46510, total_loss: 2.1815223693847656
training step: 46511, total_loss: 2.171637535095215
training step: 46512, total_loss: 2.9854421615600586
training step: 46513, total_loss: 4.935901165008545
training step: 46514, total_loss: 0.3445666432380676
training step: 46515, total_loss: 0.3358955383300781
training step: 46516, total_loss: 1.280085563659668
training step: 46517, total_loss: 1.1206539869308472
training step: 46518, total_loss: 3.301229953765869
training step: 46519, total_loss: 1.6612834930419922
training step: 46520, total_loss: 1.384798288345337
training step: 46521, total_loss: 2.2395734786987305
training step: 46522, total_loss: 0.9740647077560425
training step: 46523, total_loss: 0.6938101053237915
training step: 46524, total_loss: 1.6843072175979614
training step: 46525, total_loss: 0.5607359409332275
training step: 46526, total_loss: 0.4844933748245239
training step: 46527, total_loss: 0.2206331193447113
training step: 46528, total_loss: 0.8352739214897156
training step: 46529, total_loss: 0.09196759760379791
training step: 46530, total_loss: 1.406486988067627
training step: 46531, total_loss: 0.08220519870519638
training step: 46532, total_loss: 0.6015976071357727
training step: 46533, total_loss: 0.7621217966079712
training step: 46534, total_loss: 6.232017993927002
training step: 46535, total_loss: 1.3834058046340942
training step: 46536, total_loss: 0.62330162525177
training step: 46537, total_loss: 2.0795066356658936
training step: 46538, total_loss: 3.1480650901794434
training step: 46539, total_loss: 1.21022367477417
training step: 46540, total_loss: 0.9570885896682739
training step: 46541, total_loss: 0.128423273563385
training step: 46542, total_loss: 2.3352582454681396
training step: 46543, total_loss: 3.360260009765625
training step: 46544, total_loss: 0.49231472611427307
training step: 46545, total_loss: 3.2665860652923584
training step: 46546, total_loss: 2.647200345993042
training step: 46547, total_loss: 2.3355348110198975
training step: 46548, total_loss: 1.8481571674346924
training step: 46549, total_loss: 0.8970851302146912
training step: 46550, total_loss: 0.7560689449310303
training step: 46551, total_loss: 3.43400239944458
training step: 46552, total_loss: 1.9561994075775146
training step: 46553, total_loss: 1.1817768812179565
training step: 46554, total_loss: 2.2151336669921875
training step: 46555, total_loss: 0.4076247811317444
training step: 46556, total_loss: 0.5019888877868652
training step: 46557, total_loss: 1.71842622756958
training step: 46558, total_loss: 1.417718529701233
training step: 46559, total_loss: 1.4249374866485596
training step: 46560, total_loss: 1.0058372020721436
training step: 46561, total_loss: 2.781599521636963
training step: 46562, total_loss: 0.25864946842193604
training step: 46563, total_loss: 1.9952163696289062
training step: 46564, total_loss: 0.715325653553009
training step: 46565, total_loss: 2.2571732997894287
training step: 46566, total_loss: 2.6210200786590576
training step: 46567, total_loss: 2.778243064880371
training step: 46568, total_loss: 0.6144297122955322
training step: 46569, total_loss: 0.39988142251968384
training step: 46570, total_loss: 0.603001058101654
training step: 46571, total_loss: 2.9412927627563477
training step: 46572, total_loss: 0.09573698043823242
training step: 46573, total_loss: 0.7660118937492371
training step: 46574, total_loss: 0.863806962966919
training step: 46575, total_loss: 5.865747451782227
training step: 46576, total_loss: 0.19313666224479675
training step: 46577, total_loss: 2.101398468017578
training step: 46578, total_loss: 3.370739221572876
training step: 46579, total_loss: 4.652248859405518
training step: 46580, total_loss: 5.21649169921875
training step: 46581, total_loss: 2.098796844482422
training step: 46582, total_loss: 0.9225555062294006
training step: 46583, total_loss: 2.197563409805298
training step: 46584, total_loss: 0.036019954830408096
training step: 46585, total_loss: 0.27043887972831726
training step: 46586, total_loss: 1.0601081848144531
training step: 46587, total_loss: 2.382026195526123
training step: 46588, total_loss: 0.4422798156738281
training step: 46589, total_loss: 0.8275662660598755
training step: 46590, total_loss: 1.1322941780090332
training step: 46591, total_loss: 5.092423439025879
training step: 46592, total_loss: 1.617170810699463
training step: 46593, total_loss: 3.5665855407714844
training step: 46594, total_loss: 0.7877981662750244
training step: 46595, total_loss: 0.8045315146446228
training step: 46596, total_loss: 2.372962713241577
training step: 46597, total_loss: 2.9768893718719482
training step: 46598, total_loss: 0.9750575423240662
training step: 46599, total_loss: 1.529050350189209
training step: 46600, total_loss: 1.013183355331421
training step: 46601, total_loss: 3.518059730529785
training step: 46602, total_loss: 1.2380473613739014
training step: 46603, total_loss: 1.4649574756622314
training step: 46604, total_loss: 1.0008413791656494
training step: 46605, total_loss: 2.049952745437622
training step: 46606, total_loss: 1.3175688982009888
training step: 46607, total_loss: 1.721893310546875
training step: 46608, total_loss: 1.6174169778823853
training step: 46609, total_loss: 2.676602840423584
training step: 46610, total_loss: 0.9912336468696594
training step: 46611, total_loss: 1.265554428100586
training step: 46612, total_loss: 1.2660539150238037
training step: 46613, total_loss: 0.3210395574569702
training step: 46614, total_loss: 0.9891775846481323
training step: 46615, total_loss: 2.1743016242980957
training step: 46616, total_loss: 3.4630508422851562
training step: 46617, total_loss: 0.7692729234695435
training step: 46618, total_loss: 0.12346011400222778
training step: 46619, total_loss: 1.508380651473999
training step: 46620, total_loss: 0.014362907037138939
training step: 46621, total_loss: 0.6014543771743774
training step: 46622, total_loss: 0.5642849802970886
training step: 46623, total_loss: 0.5485098958015442
training step: 46624, total_loss: 0.27125999331474304
training step: 46625, total_loss: 0.0024918573908507824
training step: 46626, total_loss: 3.183075428009033
training step: 46627, total_loss: 3.1029791831970215
training step: 46628, total_loss: 3.2975540161132812
training step: 46629, total_loss: 2.457841396331787
training step: 46630, total_loss: 3.252817153930664
training step: 46631, total_loss: 1.985198736190796
training step: 46632, total_loss: 0.4048554003238678
training step: 46633, total_loss: 0.3077014684677124
training step: 46634, total_loss: 0.7586345672607422
training step: 46635, total_loss: 0.089928537607193
training step: 46636, total_loss: 0.16624613106250763
training step: 46637, total_loss: 0.8196839690208435
training step: 46638, total_loss: 0.040261052548885345
training step: 46639, total_loss: 4.026234149932861
training step: 46640, total_loss: 0.05457870662212372
training step: 46641, total_loss: 1.2650431394577026
training step: 46642, total_loss: 0.2890607714653015
training step: 46643, total_loss: 1.2167757749557495
training step: 46644, total_loss: 2.8038313388824463
training step: 46645, total_loss: 3.106806755065918
training step: 46646, total_loss: 1.8090229034423828
training step: 46647, total_loss: 3.143049716949463
training step: 46648, total_loss: 0.26226940751075745
training step: 46649, total_loss: 0.993712306022644
training step: 46650, total_loss: 2.6294827461242676
training step: 46651, total_loss: 0.20283016562461853
training step: 46652, total_loss: 2.7736287117004395
training step: 46653, total_loss: 0.3971697688102722
training step: 46654, total_loss: 3.129830837249756
training step: 46655, total_loss: 2.122460126876831
training step: 46656, total_loss: 2.318716526031494
training step: 46657, total_loss: 0.11627335846424103
training step: 46658, total_loss: 2.272429943084717
training step: 46659, total_loss: 1.6435552835464478
training step: 46660, total_loss: 0.6182271838188171
training step: 46661, total_loss: 5.278682708740234
training step: 46662, total_loss: 0.9482263922691345
training step: 46663, total_loss: 0.5275306701660156
training step: 46664, total_loss: 2.0532665252685547
training step: 46665, total_loss: 1.3411959409713745
training step: 46666, total_loss: 0.45271021127700806
training step: 46667, total_loss: 2.55672550201416
training step: 46668, total_loss: 0.6303184628486633
training step: 46669, total_loss: 2.0799074172973633
training step: 46670, total_loss: 0.7285619974136353
training step: 46671, total_loss: 1.1799888610839844
training step: 46672, total_loss: 0.6798283457756042
training step: 46673, total_loss: 2.3116416931152344
training step: 46674, total_loss: 1.525283932685852
training step: 46675, total_loss: 2.4900245666503906
training step: 46676, total_loss: 2.1861350536346436
training step: 46677, total_loss: 0.07050454616546631
training step: 46678, total_loss: 0.9942598342895508
training step: 46679, total_loss: 2.330556631088257
training step: 46680, total_loss: 0.9265937209129333
training step: 46681, total_loss: 0.9782447218894958
training step: 46682, total_loss: 1.3437392711639404
training step: 46683, total_loss: 0.47749289870262146
training step: 46684, total_loss: 0.8481077551841736
training step: 46685, total_loss: 4.740671157836914
training step: 46686, total_loss: 3.9855659008026123
training step: 46687, total_loss: 1.0311790704727173
training step: 46688, total_loss: 0.29419562220573425
training step: 46689, total_loss: 2.3826026916503906
training step: 46690, total_loss: 1.1440868377685547
training step: 46691, total_loss: 2.0150179862976074
training step: 46692, total_loss: 1.309992790222168
training step: 46693, total_loss: 0.41125813126564026
training step: 46694, total_loss: 1.910933494567871
training step: 46695, total_loss: 2.4491512775421143
training step: 46696, total_loss: 1.7841131687164307
training step: 46697, total_loss: 0.9810735583305359
training step: 46698, total_loss: 1.3237755298614502
training step: 46699, total_loss: 2.7302987575531006
training step: 46700, total_loss: 1.6933280229568481
training step: 46701, total_loss: 0.14080728590488434
training step: 46702, total_loss: 1.0833215713500977
training step: 46703, total_loss: 1.0610544681549072
training step: 46704, total_loss: 3.00778865814209
training step: 46705, total_loss: 0.6224318742752075
training step: 46706, total_loss: 0.7531951665878296
training step: 46707, total_loss: 1.3448904752731323
training step: 46708, total_loss: 0.722645103931427
training step: 46709, total_loss: 0.834111213684082
training step: 46710, total_loss: 2.1212244033813477
training step: 46711, total_loss: 5.5175089836120605
training step: 46712, total_loss: 1.1824870109558105
training step: 46713, total_loss: 0.12218903005123138
training step: 46714, total_loss: 4.639521598815918
training step: 46715, total_loss: 1.3290575742721558
training step: 46716, total_loss: 0.006195115856826305
training step: 46717, total_loss: 1.2712116241455078
training step: 46718, total_loss: 0.6156550049781799
training step: 46719, total_loss: 2.002906084060669
training step: 46720, total_loss: 1.6459784507751465
training step: 46721, total_loss: 5.25593376159668
training step: 46722, total_loss: 3.787672519683838
training step: 46723, total_loss: 0.19098153710365295
training step: 46724, total_loss: 1.9100122451782227
training step: 46725, total_loss: 2.6655006408691406
training step: 46726, total_loss: 2.1305952072143555
training step: 46727, total_loss: 2.80509614944458
training step: 46728, total_loss: 3.442939281463623
training step: 46729, total_loss: 1.015676736831665
training step: 46730, total_loss: 0.262830525636673
training step: 46731, total_loss: 1.8254491090774536
training step: 46732, total_loss: 1.0955767631530762
training step: 46733, total_loss: 2.1322364807128906
training step: 46734, total_loss: 3.4107728004455566
training step: 46735, total_loss: 3.3969810009002686
training step: 46736, total_loss: 1.2914533615112305
training step: 46737, total_loss: 0.0009566735243424773
training step: 46738, total_loss: 0.0606442466378212
training step: 46739, total_loss: 2.6710550785064697
training step: 46740, total_loss: 3.017139434814453
training step: 46741, total_loss: 2.8891124725341797
training step: 46742, total_loss: 4.105057716369629
training step: 46743, total_loss: 2.531283140182495
training step: 46744, total_loss: 0.36806756258010864
training step: 46745, total_loss: 1.577641487121582
training step: 46746, total_loss: 1.313886046409607
training step: 46747, total_loss: 3.9781551361083984
training step: 46748, total_loss: 0.5278252363204956
training step: 46749, total_loss: 0.4671004116535187
training step: 46750, total_loss: 1.6306958198547363
training step: 46751, total_loss: 1.0607316493988037
training step: 46752, total_loss: 1.3695158958435059
training step: 46753, total_loss: 2.6409077644348145
training step: 46754, total_loss: 2.502366542816162
training step: 46755, total_loss: 1.9227266311645508
training step: 46756, total_loss: 1.4891695976257324
training step: 46757, total_loss: 0.6918722987174988
training step: 46758, total_loss: 1.3768367767333984
training step: 46759, total_loss: 0.9715696573257446
training step: 46760, total_loss: 2.7590372562408447
training step: 46761, total_loss: 0.07718075066804886
training step: 46762, total_loss: 2.947072982788086
training step: 46763, total_loss: 1.2018377780914307
training step: 46764, total_loss: 0.4238739609718323
training step: 46765, total_loss: 1.0970468521118164
training step: 46766, total_loss: 0.15704599022865295
training step: 46767, total_loss: 4.943990230560303
training step: 46768, total_loss: 1.659513235092163
training step: 46769, total_loss: 7.95181941986084
training step: 46770, total_loss: 1.390270709991455
training step: 46771, total_loss: 2.570509195327759
training step: 46772, total_loss: 0.6625663042068481
training step: 46773, total_loss: 2.9927773475646973
training step: 46774, total_loss: 1.0462579727172852
training step: 46775, total_loss: 1.8995966911315918
training step: 46776, total_loss: 1.698148488998413
training step: 46777, total_loss: 1.236956000328064
training step: 46778, total_loss: 0.004777973517775536
training step: 46779, total_loss: 1.7234711647033691
training step: 46780, total_loss: 1.7584609985351562
training step: 46781, total_loss: 0.7693496346473694
training step: 46782, total_loss: 3.32395601272583
training step: 46783, total_loss: 1.186772346496582
training step: 46784, total_loss: 1.7177388668060303
training step: 46785, total_loss: 1.1042399406433105
training step: 46786, total_loss: 1.2918181419372559
training step: 46787, total_loss: 0.5808584690093994
training step: 46788, total_loss: 2.668923854827881
training step: 46789, total_loss: 4.4865922927856445
training step: 46790, total_loss: 1.460758090019226
training step: 46791, total_loss: 2.15511155128479
training step: 46792, total_loss: 3.650088310241699
training step: 46793, total_loss: 0.9943956732749939
training step: 46794, total_loss: 1.9156346321105957
training step: 46795, total_loss: 4.880884170532227
training step: 46796, total_loss: 0.9424213767051697
training step: 46797, total_loss: 1.46116304397583
training step: 46798, total_loss: 1.6060420274734497
training step: 46799, total_loss: 2.0033621788024902
training step: 46800, total_loss: 1.9279900789260864
training step: 46801, total_loss: 1.7221133708953857
training step: 46802, total_loss: 2.561544179916382
training step: 46803, total_loss: 0.9218976497650146
training step: 46804, total_loss: 0.8982017040252686
training step: 46805, total_loss: 3.874397039413452
training step: 46806, total_loss: 0.34705060720443726
training step: 46807, total_loss: 0.7381638884544373
training step: 46808, total_loss: 2.356813430786133
training step: 46809, total_loss: 2.729792594909668
training step: 46810, total_loss: 2.204263687133789
training step: 46811, total_loss: 0.004965805448591709
training step: 46812, total_loss: 0.5682953000068665
training step: 46813, total_loss: 3.6482789516448975
training step: 46814, total_loss: 0.17179223895072937
training step: 46815, total_loss: 1.4876720905303955
training step: 46816, total_loss: 2.040976047515869
training step: 46817, total_loss: 2.3783681392669678
training step: 46818, total_loss: 4.9655022621154785
training step: 46819, total_loss: 0.4876425564289093
training step: 46820, total_loss: 1.255812168121338
training step: 46821, total_loss: 1.7804007530212402
training step: 46822, total_loss: 1.1008834838867188
training step: 46823, total_loss: 0.925969660282135
training step: 46824, total_loss: 2.4447503089904785
training step: 46825, total_loss: 1.5331692695617676
training step: 46826, total_loss: 3.8611936569213867
training step: 46827, total_loss: 2.9852447509765625
training step: 46828, total_loss: 1.9512230157852173
training step: 46829, total_loss: 2.3795204162597656
training step: 46830, total_loss: 1.3049081563949585
training step: 46831, total_loss: 1.205402135848999
training step: 46832, total_loss: 2.297175884246826
training step: 46833, total_loss: 1.8716557025909424
training step: 46834, total_loss: 1.05881929397583
training step: 46835, total_loss: 0.06399403512477875
training step: 46836, total_loss: 1.6384209394454956
training step: 46837, total_loss: 2.5663137435913086
training step: 46838, total_loss: 1.1705318689346313
training step: 46839, total_loss: 1.0239112377166748
training step: 46840, total_loss: 0.49889248609542847
training step: 46841, total_loss: 0.5936698913574219
training step: 46842, total_loss: 0.739582359790802
training step: 46843, total_loss: 2.244689464569092
training step: 46844, total_loss: 0.8120931386947632
training step: 46845, total_loss: 1.2912348508834839
training step: 46846, total_loss: 0.4105767607688904
training step: 46847, total_loss: 1.3416543006896973
training step: 46848, total_loss: 0.4585859179496765
training step: 46849, total_loss: 1.2766728401184082
training step: 46850, total_loss: 1.3117589950561523
training step: 46851, total_loss: 2.6071486473083496
training step: 46852, total_loss: 1.335669755935669
training step: 46853, total_loss: 1.9802772998809814
training step: 46854, total_loss: 1.845468282699585
training step: 46855, total_loss: 5.450765132904053
training step: 46856, total_loss: 1.970320463180542
training step: 46857, total_loss: 4.580953598022461
training step: 46858, total_loss: 2.705984592437744
training step: 46859, total_loss: 2.0954339504241943
training step: 46860, total_loss: 0.6240636110305786
training step: 46861, total_loss: 4.979887008666992
training step: 46862, total_loss: 3.392338991165161
training step: 46863, total_loss: 0.8550185561180115
training step: 46864, total_loss: 0.09303049743175507
training step: 46865, total_loss: 0.27995824813842773
training step: 46866, total_loss: 0.801783561706543
training step: 46867, total_loss: 2.43450665473938
training step: 46868, total_loss: 3.5232176780700684
training step: 46869, total_loss: 2.277290105819702
training step: 46870, total_loss: 0.4362141489982605
training step: 46871, total_loss: 1.474321961402893
training step: 46872, total_loss: 0.009653459303081036
training step: 46873, total_loss: 2.253305673599243
training step: 46874, total_loss: 0.7415595054626465
training step: 46875, total_loss: 1.9169228076934814
training step: 46876, total_loss: 3.1013808250427246
training step: 46877, total_loss: 2.8582563400268555
training step: 46878, total_loss: 2.542998790740967
training step: 46879, total_loss: 0.07315259426832199
training step: 46880, total_loss: 0.01007260661572218
training step: 46881, total_loss: 0.33328938484191895
training step: 46882, total_loss: 0.7644272446632385
training step: 46883, total_loss: 0.32794106006622314
training step: 46884, total_loss: 1.336578130722046
training step: 46885, total_loss: 2.94303560256958
training step: 46886, total_loss: 3.0182089805603027
training step: 46887, total_loss: 0.5518389344215393
training step: 46888, total_loss: 0.2832443118095398
training step: 46889, total_loss: 0.26489436626434326
training step: 46890, total_loss: 0.6861959099769592
training step: 46891, total_loss: 0.7527742981910706
training step: 46892, total_loss: 2.6042227745056152
training step: 46893, total_loss: 1.0627505779266357
training step: 46894, total_loss: 0.7148571014404297
training step: 46895, total_loss: 1.344293236732483
training step: 46896, total_loss: 1.3118605613708496
training step: 46897, total_loss: 0.9318352341651917
training step: 46898, total_loss: 0.06758879125118256
training step: 46899, total_loss: 1.6517890691757202
training step: 46900, total_loss: 0.763910174369812
training step: 46901, total_loss: 0.002166556427255273
training step: 46902, total_loss: 2.089109420776367
training step: 46903, total_loss: 1.7632520198822021
training step: 46904, total_loss: 1.3563984632492065
training step: 46905, total_loss: 1.6960877180099487
training step: 46906, total_loss: 1.1068609952926636
training step: 46907, total_loss: 0.46559232473373413
training step: 46908, total_loss: 1.5773472785949707
training step: 46909, total_loss: 4.4255194664001465
training step: 46910, total_loss: 1.1725847721099854
training step: 46911, total_loss: 1.2482686042785645
training step: 46912, total_loss: 1.7259577512741089
training step: 46913, total_loss: 1.855452537536621
training step: 46914, total_loss: 1.8227143287658691
training step: 46915, total_loss: 0.6583499312400818
training step: 46916, total_loss: 0.7188388109207153
training step: 46917, total_loss: 1.0704548358917236
training step: 46918, total_loss: 0.9907092452049255
training step: 46919, total_loss: 2.8909502029418945
training step: 46920, total_loss: 0.6690802574157715
training step: 46921, total_loss: 1.542622447013855
training step: 46922, total_loss: 2.3199307918548584
training step: 46923, total_loss: 1.9920234680175781
training step: 46924, total_loss: 3.519141435623169
training step: 46925, total_loss: 2.7969791889190674
training step: 46926, total_loss: 2.0462822914123535
training step: 46927, total_loss: 0.3976360559463501
training step: 46928, total_loss: 0.27515843510627747
training step: 46929, total_loss: 3.980489730834961
training step: 46930, total_loss: 0.1596384197473526
training step: 46931, total_loss: 7.644626617431641
training step: 46932, total_loss: 1.3153241872787476
training step: 46933, total_loss: 0.5047443509101868
training step: 46934, total_loss: 2.6639556884765625
training step: 46935, total_loss: 2.8439528942108154
training step: 46936, total_loss: 2.2320616245269775
training step: 46937, total_loss: 1.3241119384765625
training step: 46938, total_loss: 0.9534083008766174
training step: 46939, total_loss: 0.6321445107460022
training step: 46940, total_loss: 1.519716739654541
training step: 46941, total_loss: 1.6534976959228516
training step: 46942, total_loss: 4.107933044433594
training step: 46943, total_loss: 0.9775786399841309
training step: 46944, total_loss: 1.1503713130950928
training step: 46945, total_loss: 4.794796943664551
training step: 46946, total_loss: 0.5636337995529175
training step: 46947, total_loss: 4.138518810272217
training step: 46948, total_loss: 2.0100045204162598
training step: 46949, total_loss: 1.7281397581100464
training step: 46950, total_loss: 1.0376288890838623
training step: 46951, total_loss: 2.7275099754333496
training step: 46952, total_loss: 2.2005341053009033
training step: 46953, total_loss: 2.571124315261841
training step: 46954, total_loss: 1.4672644138336182
training step: 46955, total_loss: 1.651291847229004
training step: 46956, total_loss: 0.9060732126235962
training step: 46957, total_loss: 1.2050870656967163
training step: 46958, total_loss: 0.2085549682378769
training step: 46959, total_loss: 5.526115894317627
training step: 46960, total_loss: 1.1912360191345215
training step: 46961, total_loss: 0.5452312231063843
training step: 46962, total_loss: 1.526017189025879
training step: 46963, total_loss: 3.980942726135254
training step: 46964, total_loss: 0.463784784078598
training step: 46965, total_loss: 1.517676830291748
training step: 46966, total_loss: 0.09471471607685089
training step: 46967, total_loss: 1.7531031370162964
training step: 46968, total_loss: 1.054945707321167
training step: 46969, total_loss: 1.9670205116271973
training step: 46970, total_loss: 1.0944759845733643
training step: 46971, total_loss: 1.9582087993621826
training step: 46972, total_loss: 0.41437724232673645
training step: 46973, total_loss: 0.887383759021759INFO:tensorflow:Writing predictions to: test_output/predictions_47000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_47000.json

training step: 46974, total_loss: 2.1764707565307617
training step: 46975, total_loss: 2.3072640895843506
training step: 46976, total_loss: 2.353471279144287
training step: 46977, total_loss: 1.9990770816802979
training step: 46978, total_loss: 1.7765412330627441
training step: 46979, total_loss: 2.457782745361328
training step: 46980, total_loss: 0.7188450694084167
training step: 46981, total_loss: 1.6768126487731934
training step: 46982, total_loss: 0.890190601348877
training step: 46983, total_loss: 0.16978083550930023
training step: 46984, total_loss: 2.3705384731292725
training step: 46985, total_loss: 1.7289016246795654
training step: 46986, total_loss: 0.3228245973587036
training step: 46987, total_loss: 2.5815320014953613
training step: 46988, total_loss: 0.8482898473739624
training step: 46989, total_loss: 2.5738298892974854
training step: 46990, total_loss: 3.0037927627563477
training step: 46991, total_loss: 3.4190971851348877
training step: 46992, total_loss: 1.2261815071105957
training step: 46993, total_loss: 3.6779608726501465
training step: 46994, total_loss: 3.009268045425415
training step: 46995, total_loss: 0.5622166991233826
training step: 46996, total_loss: 1.5984928607940674
training step: 46997, total_loss: 0.34575754404067993
training step: 46998, total_loss: 2.19706654548645
training step: 46999, total_loss: 0.6144243478775024
training step: 47000, total_loss: 1.3064521551132202
epoch finished! shuffle=False
evaluation: 11000, total_loss: 1.8147310018539429, f1: 56.06410385415695, followup: 18.621633957097217, yesno: 78.98980678533394, heq: 50.007606876616464, dheq: 3.1

Model saved in path test_output//model_47000.ckpt
training step: 47001, total_loss: 1.6987261772155762
training step: 47002, total_loss: 2.1782171726226807
training step: 47003, total_loss: 1.2790215015411377
training step: 47004, total_loss: 1.786611795425415
training step: 47005, total_loss: 3.309359312057495
training step: 47006, total_loss: 2.003415107727051
training step: 47007, total_loss: 0.26840147376060486
training step: 47008, total_loss: 1.4588128328323364
training step: 47009, total_loss: 0.5694712996482849
training step: 47010, total_loss: 1.808459758758545
training step: 47011, total_loss: 0.9234486818313599
training step: 47012, total_loss: 1.4427051544189453
training step: 47013, total_loss: 1.1981806755065918
training step: 47014, total_loss: 2.9342167377471924
training step: 47015, total_loss: 1.8033592700958252
training step: 47016, total_loss: 0.37462979555130005
training step: 47017, total_loss: 0.6013868451118469
training step: 47018, total_loss: 1.661632776260376
training step: 47019, total_loss: 2.5899875164031982
training step: 47020, total_loss: 1.1838504076004028
training step: 47021, total_loss: 2.7411108016967773
training step: 47022, total_loss: 1.5079320669174194
training step: 47023, total_loss: 1.1608638763427734
training step: 47024, total_loss: 1.4549336433410645
training step: 47025, total_loss: 1.460605502128601
training step: 47026, total_loss: 2.087714672088623
training step: 47027, total_loss: 5.540780067443848
training step: 47028, total_loss: 0.31775593757629395
training step: 47029, total_loss: 1.6170064210891724
training step: 47030, total_loss: 0.5872235298156738
training step: 47031, total_loss: 1.455857276916504
training step: 47032, total_loss: 1.3647184371948242
training step: 47033, total_loss: 1.0921087265014648
training step: 47034, total_loss: 0.9793847799301147
training step: 47035, total_loss: 4.051272869110107
training step: 47036, total_loss: 2.549682855606079
training step: 47037, total_loss: 1.5445045232772827
training step: 47038, total_loss: 2.902008533477783
training step: 47039, total_loss: 2.0839669704437256
training step: 47040, total_loss: 0.8653637170791626
training step: 47041, total_loss: 0.991692841053009
training step: 47042, total_loss: 2.8022990226745605
training step: 47043, total_loss: 0.010937322862446308
training step: 47044, total_loss: 2.221527099609375
training step: 47045, total_loss: 2.2526845932006836
training step: 47046, total_loss: 1.2892435789108276
training step: 47047, total_loss: 3.729175567626953
training step: 47048, total_loss: 2.1322708129882812
training step: 47049, total_loss: 1.3142340183258057
training step: 47050, total_loss: 1.612940788269043
training step: 47051, total_loss: 1.6842272281646729
training step: 47052, total_loss: 2.745664596557617
training step: 47053, total_loss: 2.600980281829834
training step: 47054, total_loss: 2.6436855792999268
training step: 47055, total_loss: 1.250551462173462
training step: 47056, total_loss: 0.7615251541137695
training step: 47057, total_loss: 3.3882224559783936
training step: 47058, total_loss: 0.8902708292007446
training step: 47059, total_loss: 2.3589792251586914
training step: 47060, total_loss: 1.3441076278686523
training step: 47061, total_loss: 2.243006944656372
training step: 47062, total_loss: 3.3652210235595703
training step: 47063, total_loss: 0.9820621609687805
training step: 47064, total_loss: 1.4324079751968384
training step: 47065, total_loss: 4.983262062072754
training step: 47066, total_loss: 3.421298027038574
training step: 47067, total_loss: 0.5928350687026978
training step: 47068, total_loss: 3.8230488300323486
training step: 47069, total_loss: 2.2669906616210938
training step: 47070, total_loss: 2.9741883277893066
training step: 47071, total_loss: 3.999199151992798
training step: 47072, total_loss: 1.2442944049835205
training step: 47073, total_loss: 2.055788278579712
training step: 47074, total_loss: 1.5005395412445068
training step: 47075, total_loss: 2.069605827331543
training step: 47076, total_loss: 0.012843044474720955
training step: 47077, total_loss: 1.8475332260131836
training step: 47078, total_loss: 1.96087646484375
training step: 47079, total_loss: 2.7777507305145264
training step: 47080, total_loss: 1.5385701656341553
training step: 47081, total_loss: 0.288848876953125
training step: 47082, total_loss: 1.1846164464950562
training step: 47083, total_loss: 3.57133150100708
training step: 47084, total_loss: 2.2051351070404053
training step: 47085, total_loss: 3.0546610355377197
training step: 47086, total_loss: 1.5891740322113037
training step: 47087, total_loss: 2.9769821166992188
training step: 47088, total_loss: 3.38930082321167
training step: 47089, total_loss: 1.2589836120605469
training step: 47090, total_loss: 2.5468907356262207
training step: 47091, total_loss: 1.2003979682922363
training step: 47092, total_loss: 1.290236473083496
training step: 47093, total_loss: 0.3634517192840576
training step: 47094, total_loss: 2.2912397384643555
training step: 47095, total_loss: 2.9598217010498047
training step: 47096, total_loss: 1.231884241104126
training step: 47097, total_loss: 2.065523147583008
training step: 47098, total_loss: 1.3199427127838135
training step: 47099, total_loss: 0.9393419623374939
training step: 47100, total_loss: 2.1060657501220703
training step: 47101, total_loss: 1.4476778507232666
training step: 47102, total_loss: 2.617523670196533
training step: 47103, total_loss: 3.6880688667297363
training step: 47104, total_loss: 0.4511851966381073
training step: 47105, total_loss: 2.2704617977142334
training step: 47106, total_loss: 4.782154083251953
training step: 47107, total_loss: 0.22392064332962036
training step: 47108, total_loss: 0.3419033885002136
training step: 47109, total_loss: 1.6286671161651611
training step: 47110, total_loss: 0.5785790085792542
training step: 47111, total_loss: 0.21323862671852112
training step: 47112, total_loss: 2.3853774070739746
training step: 47113, total_loss: 0.7189104557037354
training step: 47114, total_loss: 1.7375329732894897
training step: 47115, total_loss: 1.9366755485534668
training step: 47116, total_loss: 2.2896387577056885
training step: 47117, total_loss: 1.533003330230713
training step: 47118, total_loss: 0.2612392008304596
training step: 47119, total_loss: 0.9352335333824158
training step: 47120, total_loss: 1.5677030086517334
training step: 47121, total_loss: 1.5390151739120483
training step: 47122, total_loss: 1.847724199295044
training step: 47123, total_loss: 1.77162766456604
training step: 47124, total_loss: 1.500126838684082
training step: 47125, total_loss: 1.479710340499878
training step: 47126, total_loss: 2.7480528354644775
training step: 47127, total_loss: 1.1883021593093872
training step: 47128, total_loss: 0.6256610155105591
training step: 47129, total_loss: 2.002092123031616
training step: 47130, total_loss: 2.712352991104126
training step: 47131, total_loss: 3.5662970542907715
training step: 47132, total_loss: 4.280497074127197
training step: 47133, total_loss: 2.6610822677612305
training step: 47134, total_loss: 0.8910795450210571
training step: 47135, total_loss: 1.2196065187454224
training step: 47136, total_loss: 1.5431442260742188
training step: 47137, total_loss: 2.850245952606201
training step: 47138, total_loss: 3.1624250411987305
training step: 47139, total_loss: 2.65926194190979
training step: 47140, total_loss: 2.155418872833252
training step: 47141, total_loss: 2.941056251525879
training step: 47142, total_loss: 1.8207378387451172
training step: 47143, total_loss: 2.2968926429748535
training step: 47144, total_loss: 1.887110710144043
training step: 47145, total_loss: 2.0144357681274414
training step: 47146, total_loss: 0.6304212808609009
training step: 47147, total_loss: 1.2259385585784912
training step: 47148, total_loss: 1.2585442066192627
training step: 47149, total_loss: 1.6387838125228882
training step: 47150, total_loss: 3.7277345657348633
training step: 47151, total_loss: 4.8380889892578125
training step: 47152, total_loss: 1.0929466485977173
training step: 47153, total_loss: 3.074557304382324
training step: 47154, total_loss: 1.0005629062652588
training step: 47155, total_loss: 1.0615904331207275
training step: 47156, total_loss: 1.8496966361999512
training step: 47157, total_loss: 1.3395897150039673
training step: 47158, total_loss: 0.7439398765563965
training step: 47159, total_loss: 4.624256610870361
training step: 47160, total_loss: 0.9260398149490356
training step: 47161, total_loss: 0.11987413465976715
training step: 47162, total_loss: 2.318861484527588
training step: 47163, total_loss: 2.143653631210327
training step: 47164, total_loss: 1.975616693496704
training step: 47165, total_loss: 1.8559820652008057
training step: 47166, total_loss: 1.7492365837097168
training step: 47167, total_loss: 1.8710829019546509
training step: 47168, total_loss: 1.5602284669876099
training step: 47169, total_loss: 1.2721309661865234
training step: 47170, total_loss: 1.6455508470535278
training step: 47171, total_loss: 0.6289479732513428
training step: 47172, total_loss: 3.013732433319092
training step: 47173, total_loss: 1.6810094118118286
training step: 47174, total_loss: 1.1210627555847168
training step: 47175, total_loss: 0.7016919851303101
training step: 47176, total_loss: 0.00018171800184063613
training step: 47177, total_loss: 0.6689339876174927
training step: 47178, total_loss: 1.081241488456726
training step: 47179, total_loss: 1.2940518856048584
training step: 47180, total_loss: 0.1679382622241974
training step: 47181, total_loss: 2.146167039871216
training step: 47182, total_loss: 1.5377984046936035
training step: 47183, total_loss: 1.39505934715271
training step: 47184, total_loss: 1.8381257057189941
training step: 47185, total_loss: 2.0992228984832764
training step: 47186, total_loss: 3.2384471893310547
training step: 47187, total_loss: 1.811124563217163
training step: 47188, total_loss: 1.5034205913543701
training step: 47189, total_loss: 1.0747849941253662
training step: 47190, total_loss: 2.055596113204956
training step: 47191, total_loss: 4.328566551208496
training step: 47192, total_loss: 2.11838698387146
training step: 47193, total_loss: 1.2422429323196411
training step: 47194, total_loss: 2.7448136806488037
training step: 47195, total_loss: 1.8199644088745117
training step: 47196, total_loss: 8.959636688232422
training step: 47197, total_loss: 3.6124274730682373
training step: 47198, total_loss: 2.003916025161743
training step: 47199, total_loss: 3.280888080596924
training step: 47200, total_loss: 2.025569438934326
training step: 47201, total_loss: 3.0161197185516357
training step: 47202, total_loss: 1.811127781867981
training step: 47203, total_loss: 2.2910852432250977
training step: 47204, total_loss: 1.1837135553359985
training step: 47205, total_loss: 0.5992627143859863
training step: 47206, total_loss: 0.23817522823810577
training step: 47207, total_loss: 0.2796635329723358
training step: 47208, total_loss: 0.6975273489952087
training step: 47209, total_loss: 0.7674015760421753
training step: 47210, total_loss: 2.5245566368103027
training step: 47211, total_loss: 0.8392231464385986
training step: 47212, total_loss: 0.019586369395256042
training step: 47213, total_loss: 4.14199161529541
training step: 47214, total_loss: 1.2619578838348389
training step: 47215, total_loss: 0.557467520236969
training step: 47216, total_loss: 0.40316352248191833
training step: 47217, total_loss: 1.011212944984436
training step: 47218, total_loss: 2.2327165603637695
training step: 47219, total_loss: 1.2479631900787354
training step: 47220, total_loss: 0.5051377415657043
training step: 47221, total_loss: 0.09218224883079529
training step: 47222, total_loss: 0.8015280961990356
training step: 47223, total_loss: 0.5948859453201294
training step: 47224, total_loss: 1.9330768585205078
training step: 47225, total_loss: 0.6546034216880798
training step: 47226, total_loss: 1.7396495342254639
training step: 47227, total_loss: 2.2319586277008057
training step: 47228, total_loss: 1.2818448543548584
training step: 47229, total_loss: 4.000416278839111
training step: 47230, total_loss: 3.6169216632843018
training step: 47231, total_loss: 1.4419034719467163
training step: 47232, total_loss: 1.848388671875
training step: 47233, total_loss: 1.4284818172454834
training step: 47234, total_loss: 1.3897701501846313
training step: 47235, total_loss: 2.112027168273926
training step: 47236, total_loss: 2.316694498062134
training step: 47237, total_loss: 2.3047611713409424
training step: 47238, total_loss: 1.3954238891601562
training step: 47239, total_loss: 0.5279939770698547
training step: 47240, total_loss: 0.8919679522514343
training step: 47241, total_loss: 1.8821287155151367
training step: 47242, total_loss: 0.7127938270568848
training step: 47243, total_loss: 5.236808776855469
training step: 47244, total_loss: 0.8997202515602112
training step: 47245, total_loss: 3.4878077507019043
training step: 47246, total_loss: 0.6539860963821411
training step: 47247, total_loss: 0.40087491273880005
training step: 47248, total_loss: 0.7942671179771423
training step: 47249, total_loss: 0.6065768599510193
training step: 47250, total_loss: 3.0510289669036865
training step: 47251, total_loss: 0.002672310220077634
training step: 47252, total_loss: 0.42150402069091797
training step: 47253, total_loss: 6.229037284851074
training step: 47254, total_loss: 1.6849596500396729
training step: 47255, total_loss: 2.6366145610809326
training step: 47256, total_loss: 1.7832696437835693
training step: 47257, total_loss: 2.250946044921875
training step: 47258, total_loss: 2.8086864948272705
training step: 47259, total_loss: 0.6186404824256897
training step: 47260, total_loss: 1.7221438884735107
training step: 47261, total_loss: 1.3517519235610962
training step: 47262, total_loss: 2.050281047821045
training step: 47263, total_loss: 0.7667044401168823
training step: 47264, total_loss: 1.254425287246704
training step: 47265, total_loss: 2.447057008743286
training step: 47266, total_loss: 2.512296676635742
training step: 47267, total_loss: 3.0496902465820312
training step: 47268, total_loss: 1.977846622467041
training step: 47269, total_loss: 1.1901705265045166
training step: 47270, total_loss: 0.07684631645679474
training step: 47271, total_loss: 1.3941431045532227
training step: 47272, total_loss: 0.046526066958904266
training step: 47273, total_loss: 0.6362814903259277
training step: 47274, total_loss: 1.05390202999115
training step: 47275, total_loss: 2.813530206680298
training step: 47276, total_loss: 1.0205504894256592
training step: 47277, total_loss: 1.2337214946746826
training step: 47278, total_loss: 2.1918563842773438
training step: 47279, total_loss: 5.044507026672363
training step: 47280, total_loss: 2.1274733543395996
training step: 47281, total_loss: 2.1597518920898438
training step: 47282, total_loss: 1.188905119895935
training step: 47283, total_loss: 2.1400558948516846
training step: 47284, total_loss: 1.9953856468200684
training step: 47285, total_loss: 1.4623241424560547
training step: 47286, total_loss: 3.321025848388672
training step: 47287, total_loss: 3.2428197860717773
training step: 47288, total_loss: 1.2378644943237305
training step: 47289, total_loss: 0.2570418119430542
training step: 47290, total_loss: 0.11992087960243225
training step: 47291, total_loss: 1.8833742141723633
training step: 47292, total_loss: 3.459770917892456
training step: 47293, total_loss: 2.8769102096557617
training step: 47294, total_loss: 2.1004812717437744
training step: 47295, total_loss: 7.540897369384766
training step: 47296, total_loss: 1.457798957824707
training step: 47297, total_loss: 1.9600646495819092
training step: 47298, total_loss: 1.7363547086715698
training step: 47299, total_loss: 0.2575390636920929
training step: 47300, total_loss: 0.9784704446792603
training step: 47301, total_loss: 1.183048129081726
training step: 47302, total_loss: 1.5001407861709595
training step: 47303, total_loss: 1.8956624269485474
training step: 47304, total_loss: 1.3239116668701172
training step: 47305, total_loss: 9.220411448040977e-05
training step: 47306, total_loss: 1.8092752695083618
training step: 47307, total_loss: 0.8438502550125122
training step: 47308, total_loss: 1.5194348096847534
training step: 47309, total_loss: 1.1347681283950806
training step: 47310, total_loss: 2.203425407409668
training step: 47311, total_loss: 2.7864768505096436
training step: 47312, total_loss: 0.597261905670166
training step: 47313, total_loss: 1.336572289466858
training step: 47314, total_loss: 1.60427987575531
training step: 47315, total_loss: 2.865936040878296
training step: 47316, total_loss: 0.05280333757400513
training step: 47317, total_loss: 0.9895584583282471
training step: 47318, total_loss: 0.5127052664756775
training step: 47319, total_loss: 3.88836669921875
training step: 47320, total_loss: 1.01679265499115
training step: 47321, total_loss: 0.6718201637268066
training step: 47322, total_loss: 1.174464225769043
training step: 47323, total_loss: 0.7528867125511169
training step: 47324, total_loss: 1.869286060333252
training step: 47325, total_loss: 0.6045687198638916
training step: 47326, total_loss: 2.748659133911133
training step: 47327, total_loss: 0.16417381167411804
training step: 47328, total_loss: 3.7179126739501953
training step: 47329, total_loss: 3.4365997314453125
training step: 47330, total_loss: 0.17348091304302216
training step: 47331, total_loss: 1.3604577779769897
training step: 47332, total_loss: 1.600923776626587
training step: 47333, total_loss: 0.5352106094360352
training step: 47334, total_loss: 0.5317404270172119
training step: 47335, total_loss: 1.834165096282959
training step: 47336, total_loss: 0.9366227388381958
training step: 47337, total_loss: 0.11998490989208221
training step: 47338, total_loss: 1.0073940753936768
training step: 47339, total_loss: 2.8313231468200684
training step: 47340, total_loss: 3.914335250854492
training step: 47341, total_loss: 2.5441641807556152
training step: 47342, total_loss: 0.6346312761306763
training step: 47343, total_loss: 2.2917070388793945
training step: 47344, total_loss: 1.3368850946426392
training step: 47345, total_loss: 1.793025255203247
training step: 47346, total_loss: 5.406948566436768
training step: 47347, total_loss: 1.646765112876892
training step: 47348, total_loss: 1.684432029724121
training step: 47349, total_loss: 1.3588778972625732
training step: 47350, total_loss: 1.72332763671875
training step: 47351, total_loss: 2.0045714378356934
training step: 47352, total_loss: 2.7669832706451416
training step: 47353, total_loss: 1.0568058490753174
training step: 47354, total_loss: 2.38566255569458
training step: 47355, total_loss: 1.4716007709503174
training step: 47356, total_loss: 1.630878210067749
training step: 47357, total_loss: 2.0218324661254883
training step: 47358, total_loss: 0.8251468539237976
training step: 47359, total_loss: 2.4201464653015137
training step: 47360, total_loss: 3.894869565963745
training step: 47361, total_loss: 3.074850559234619
training step: 47362, total_loss: 0.6849799752235413
training step: 47363, total_loss: 1.4722576141357422
training step: 47364, total_loss: 1.2863168716430664
training step: 47365, total_loss: 0.4628395140171051
training step: 47366, total_loss: 0.10479091852903366
training step: 47367, total_loss: 3.4608020782470703
training step: 47368, total_loss: 1.4219470024108887
training step: 47369, total_loss: 0.5000261068344116
training step: 47370, total_loss: 5.479374408721924
training step: 47371, total_loss: 2.6716885566711426
training step: 47372, total_loss: 0.3070269823074341
training step: 47373, total_loss: 1.9355604648590088
training step: 47374, total_loss: 0.9695210456848145
training step: 47375, total_loss: 2.16458797454834
training step: 47376, total_loss: 2.3956775665283203
training step: 47377, total_loss: 3.1891255378723145
training step: 47378, total_loss: 4.330448150634766
training step: 47379, total_loss: 2.9536678791046143
training step: 47380, total_loss: 2.5804595947265625
training step: 47381, total_loss: 0.3104647099971771
training step: 47382, total_loss: 0.26326513290405273
training step: 47383, total_loss: 1.2887983322143555
training step: 47384, total_loss: 1.6924077272415161
training step: 47385, total_loss: 2.239443063735962
training step: 47386, total_loss: 1.7275176048278809
training step: 47387, total_loss: 0.6373518705368042
training step: 47388, total_loss: 1.5590492486953735
training step: 47389, total_loss: 1.878865122795105
training step: 47390, total_loss: 3.200516700744629
training step: 47391, total_loss: 0.4470318555831909
training step: 47392, total_loss: 0.24613037705421448
training step: 47393, total_loss: 1.5060946941375732
training step: 47394, total_loss: 0.9456126689910889
training step: 47395, total_loss: 0.8263330459594727
training step: 47396, total_loss: 1.5991142988204956
training step: 47397, total_loss: 2.204524040222168
training step: 47398, total_loss: 1.6605596542358398
training step: 47399, total_loss: 0.0678120106458664
training step: 47400, total_loss: 1.5689729452133179
training step: 47401, total_loss: 3.1374804973602295
training step: 47402, total_loss: 3.5591881275177
training step: 47403, total_loss: 1.6217024326324463
training step: 47404, total_loss: 0.8905424475669861
training step: 47405, total_loss: 0.3322516083717346
training step: 47406, total_loss: 0.4138593077659607
training step: 47407, total_loss: 1.6843656301498413
training step: 47408, total_loss: 2.717259168624878
training step: 47409, total_loss: 2.0087990760803223
training step: 47410, total_loss: 0.1844066083431244
training step: 47411, total_loss: 1.3537342548370361
training step: 47412, total_loss: 2.227956771850586
training step: 47413, total_loss: 0.4474949538707733
training step: 47414, total_loss: 0.521348774433136
training step: 47415, total_loss: 1.899338722229004
training step: 47416, total_loss: 1.7381565570831299
training step: 47417, total_loss: 0.8877683877944946
training step: 47418, total_loss: 0.11227727681398392
training step: 47419, total_loss: 3.858302116394043
training step: 47420, total_loss: 0.28903311491012573
training step: 47421, total_loss: 0.5578988790512085
training step: 47422, total_loss: 2.7052392959594727
training step: 47423, total_loss: 1.4238941669464111
training step: 47424, total_loss: 0.949446439743042
training step: 47425, total_loss: 2.0973103046417236
training step: 47426, total_loss: 0.4832893908023834
training step: 47427, total_loss: 4.640308380126953
training step: 47428, total_loss: 3.7166967391967773
training step: 47429, total_loss: 1.1944351196289062
training step: 47430, total_loss: 0.6424520611763
training step: 47431, total_loss: 3.455852746963501
training step: 47432, total_loss: 3.0217018127441406
training step: 47433, total_loss: 3.1715927124023438
training step: 47434, total_loss: 5.38882303237915
training step: 47435, total_loss: 0.5076125860214233
training step: 47436, total_loss: 1.7163710594177246
training step: 47437, total_loss: 0.18559294939041138
training step: 47438, total_loss: 2.2324740886688232
training step: 47439, total_loss: 1.704464077949524
training step: 47440, total_loss: 1.1972328424453735
training step: 47441, total_loss: 0.3689002990722656
training step: 47442, total_loss: 2.978217601776123
training step: 47443, total_loss: 0.5502921342849731
training step: 47444, total_loss: 0.33833277225494385
training step: 47445, total_loss: 3.122093677520752
training step: 47446, total_loss: 2.5051679611206055
training step: 47447, total_loss: 1.8703632354736328
training step: 47448, total_loss: 2.307129144668579
training step: 47449, total_loss: 4.319439888000488
training step: 47450, total_loss: 0.0871322900056839
training step: 47451, total_loss: 6.655022144317627
training step: 47452, total_loss: 0.41774362325668335
training step: 47453, total_loss: 0.8492081165313721
training step: 47454, total_loss: 0.395552396774292
training step: 47455, total_loss: 0.1776270717382431
training step: 47456, total_loss: 3.6223387718200684
training step: 47457, total_loss: 0.5894346237182617
training step: 47458, total_loss: 1.1715805530548096
training step: 47459, total_loss: 1.3169426918029785
training step: 47460, total_loss: 0.0698423981666565
training step: 47461, total_loss: 2.263340473175049
training step: 47462, total_loss: 1.3467485904693604
training step: 47463, total_loss: 0.05238961800932884
training step: 47464, total_loss: 3.6124465465545654
training step: 47465, total_loss: 1.9053735733032227
training step: 47466, total_loss: 0.6022319197654724
training step: 47467, total_loss: 0.50113844871521
training step: 47468, total_loss: 6.448984146118164
training step: 47469, total_loss: 1.2192641496658325
training step: 47470, total_loss: 3.272765874862671
training step: 47471, total_loss: 4.316804885864258
training step: 47472, total_loss: 1.4777182340621948
training step: 47473, total_loss: 1.8171560764312744
training step: 47474, total_loss: 4.249907493591309
training step: 47475, total_loss: 1.2949044704437256
training step: 47476, total_loss: 2.874019145965576
training step: 47477, total_loss: 1.184725284576416
training step: 47478, total_loss: 4.144048690795898
training step: 47479, total_loss: 0.6212819814682007
training step: 47480, total_loss: 1.4952068328857422
training step: 47481, total_loss: 0.005776938982307911
training step: 47482, total_loss: 3.5251283645629883
training step: 47483, total_loss: 0.3056502640247345
training step: 47484, total_loss: 4.325863838195801
training step: 47485, total_loss: 0.9438916444778442
training step: 47486, total_loss: 2.138775110244751
training step: 47487, total_loss: 0.045070432126522064
training step: 47488, total_loss: 0.9815077781677246
training step: 47489, total_loss: 1.27872633934021
training step: 47490, total_loss: 0.4081774353981018
training step: 47491, total_loss: 0.6473008990287781
training step: 47492, total_loss: 0.1916433870792389
training step: 47493, total_loss: 0.6580480337142944
training step: 47494, total_loss: 4.266077041625977
training step: 47495, total_loss: 3.430449962615967
training step: 47496, total_loss: 0.931601881980896
training step: 47497, total_loss: 2.910305976867676
training step: 47498, total_loss: 1.0501295328140259
training step: 47499, total_loss: 4.466095924377441
training step: 47500, total_loss: 0.10716818273067474
training step: 47501, total_loss: 3.6201257705688477
training step: 47502, total_loss: 0.11132602393627167
training step: 47503, total_loss: 2.0875797271728516
training step: 47504, total_loss: 1.94266939163208
training step: 47505, total_loss: 1.0299103260040283
training step: 47506, total_loss: 2.0855212211608887
training step: 47507, total_loss: 2.01530122756958
training step: 47508, total_loss: 0.05968206375837326
training step: 47509, total_loss: 1.7249679565429688
training step: 47510, total_loss: 1.539567470550537
training step: 47511, total_loss: 0.34508654475212097
training step: 47512, total_loss: 1.1189334392547607
training step: 47513, total_loss: 1.3702969551086426
training step: 47514, total_loss: 0.783052921295166
training step: 47515, total_loss: 1.7728002071380615
training step: 47516, total_loss: 1.4351388216018677
training step: 47517, total_loss: 1.5889911651611328
training step: 47518, total_loss: 4.578037738800049
training step: 47519, total_loss: 1.3292903900146484
training step: 47520, total_loss: 1.7740709781646729
training step: 47521, total_loss: 2.4041452407836914
training step: 47522, total_loss: 1.6494953632354736
training step: 47523, total_loss: 0.4783528447151184
training step: 47524, total_loss: 1.5170509815216064
training step: 47525, total_loss: 1.1181178092956543
training step: 47526, total_loss: 0.29550185799598694
training step: 47527, total_loss: 2.170595407485962
training step: 47528, total_loss: 1.3074181079864502
training step: 47529, total_loss: 0.7114537954330444
training step: 47530, total_loss: 1.8772611618041992
training step: 47531, total_loss: 1.5508195161819458
training step: 47532, total_loss: 0.7097914218902588
training step: 47533, total_loss: 0.8061254620552063
training step: 47534, total_loss: 0.00015537720173597336
training step: 47535, total_loss: 1.5902321338653564
training step: 47536, total_loss: 1.5426621437072754
training step: 47537, total_loss: 1.0693988800048828
training step: 47538, total_loss: 1.8792290687561035
training step: 47539, total_loss: 0.037730585783720016
training step: 47540, total_loss: 2.5979013442993164
training step: 47541, total_loss: 2.0589280128479004
training step: 47542, total_loss: 0.7281658053398132
training step: 47543, total_loss: 1.0095802545547485
training step: 47544, total_loss: 0.4529147148132324
training step: 47545, total_loss: 1.500514030456543
training step: 47546, total_loss: 2.2730579376220703
training step: 47547, total_loss: 2.004892587661743
training step: 47548, total_loss: 2.8591909408569336
training step: 47549, total_loss: 3.7295923233032227
training step: 47550, total_loss: 2.1285901069641113
training step: 47551, total_loss: 0.08061729371547699
training step: 47552, total_loss: 1.2551820278167725
training step: 47553, total_loss: 3.0826468467712402
training step: 47554, total_loss: 1.1590791940689087
training step: 47555, total_loss: 2.2983760833740234
training step: 47556, total_loss: 3.6376209259033203
training step: 47557, total_loss: 0.8771662712097168
training step: 47558, total_loss: 2.735293388366699
training step: 47559, total_loss: 0.2317219078540802
training step: 47560, total_loss: 1.1332231760025024
training step: 47561, total_loss: 4.2554426193237305
training step: 47562, total_loss: 1.1347672939300537
training step: 47563, total_loss: 1.7500218152999878
training step: 47564, total_loss: 1.656877875328064
training step: 47565, total_loss: 3.956059455871582
training step: 47566, total_loss: 1.140723466873169
training step: 47567, total_loss: 1.3979287147521973
training step: 47568, total_loss: 1.0527281761169434
training step: 47569, total_loss: 1.6523269414901733
training step: 47570, total_loss: 2.185885190963745
training step: 47571, total_loss: 2.8239383697509766
training step: 47572, total_loss: 0.6917445659637451
training step: 47573, total_loss: 3.229769706726074
training step: 47574, total_loss: 0.9983607530593872
training step: 47575, total_loss: 2.7700912952423096
training step: 47576, total_loss: 0.38045525550842285
training step: 47577, total_loss: 1.4039758443832397
training step: 47578, total_loss: 1.4257643222808838
training step: 47579, total_loss: 3.15470814704895
training step: 47580, total_loss: 1.3331518173217773
training step: 47581, total_loss: 0.5486835241317749
training step: 47582, total_loss: 1.2588424682617188
training step: 47583, total_loss: 0.21960501372814178
training step: 47584, total_loss: 0.477952778339386
training step: 47585, total_loss: 3.4524052143096924
training step: 47586, total_loss: 2.4719958305358887
training step: 47587, total_loss: 1.6500605344772339
training step: 47588, total_loss: 2.0718705654144287
training step: 47589, total_loss: 2.06060791015625
training step: 47590, total_loss: 6.216659069061279
training step: 47591, total_loss: 1.8530268669128418
training step: 47592, total_loss: 0.3586835265159607
training step: 47593, total_loss: 1.520086646080017
training step: 47594, total_loss: 4.577426433563232
training step: 47595, total_loss: 1.8873916864395142
training step: 47596, total_loss: 1.5366990566253662
training step: 47597, total_loss: 0.978752613067627
training step: 47598, total_loss: 2.923067092895508
training step: 47599, total_loss: 2.390497922897339
training step: 47600, total_loss: 1.4476735591888428
training step: 47601, total_loss: 2.4004664421081543
training step: 47602, total_loss: 0.8647630214691162
training step: 47603, total_loss: 1.8763434886932373
training step: 47604, total_loss: 3.4839553833007812
training step: 47605, total_loss: 3.254539966583252
training step: 47606, total_loss: 2.260862112045288
training step: 47607, total_loss: 0.03055071085691452
training step: 47608, total_loss: 0.020434603095054626
training step: 47609, total_loss: 1.2317801713943481
training step: 47610, total_loss: 1.9743832349777222
training step: 47611, total_loss: 1.7519116401672363
training step: 47612, total_loss: 2.0645976066589355
training step: 47613, total_loss: 1.0668519735336304
training step: 47614, total_loss: 3.5630087852478027
training step: 47615, total_loss: 0.0277436263859272
training step: 47616, total_loss: 1.9235501289367676
training step: 47617, total_loss: 0.6326163411140442
training step: 47618, total_loss: 0.4638543426990509
training step: 47619, total_loss: 3.40376877784729
training step: 47620, total_loss: 0.9614487886428833
training step: 47621, total_loss: 1.9714783430099487
training step: 47622, total_loss: 1.3408799171447754
training step: 47623, total_loss: 0.06680072098970413
training step: 47624, total_loss: 1.3446505069732666
training step: 47625, total_loss: 1.2340668439865112
training step: 47626, total_loss: 0.009350836277008057
training step: 47627, total_loss: 0.09447652101516724
training step: 47628, total_loss: 0.168998122215271
training step: 47629, total_loss: 1.371835470199585
training step: 47630, total_loss: 1.3849048614501953
training step: 47631, total_loss: 3.1971144676208496
training step: 47632, total_loss: 1.2762892246246338
training step: 47633, total_loss: 0.22930167615413666
training step: 47634, total_loss: 1.9375908374786377
training step: 47635, total_loss: 3.3739371299743652
training step: 47636, total_loss: 2.880173683166504
training step: 47637, total_loss: 0.6935765147209167
training step: 47638, total_loss: 3.259852409362793
training step: 47639, total_loss: 4.849465370178223
training step: 47640, total_loss: 1.0669190883636475
training step: 47641, total_loss: 0.11022239923477173
training step: 47642, total_loss: 2.250307321548462
training step: 47643, total_loss: 0.9629611968994141
training step: 47644, total_loss: 3.4337987899780273
training step: 47645, total_loss: 0.8494324088096619
training step: 47646, total_loss: 4.966799736022949
training step: 47647, total_loss: 2.2412607669830322
training step: 47648, total_loss: 0.057868581265211105
training step: 47649, total_loss: 1.706352710723877
training step: 47650, total_loss: 0.9477167725563049
training step: 47651, total_loss: 0.4150964319705963
training step: 47652, total_loss: 2.113934278488159
training step: 47653, total_loss: 1.5875920057296753
training step: 47654, total_loss: 0.1488640308380127
training step: 47655, total_loss: 2.5951223373413086
training step: 47656, total_loss: 1.4409229755401611
training step: 47657, total_loss: 0.1610899269580841
training step: 47658, total_loss: 0.9841226935386658
training step: 47659, total_loss: 0.4040983319282532
training step: 47660, total_loss: 2.106996536254883
training step: 47661, total_loss: 0.8537879586219788
training step: 47662, total_loss: 1.5807693004608154
training step: 47663, total_loss: 0.5343157052993774
training step: 47664, total_loss: 0.07980900257825851
training step: 47665, total_loss: 2.321086883544922
training step: 47666, total_loss: 3.9385807514190674
training step: 47667, total_loss: 3.549008369445801
training step: 47668, total_loss: 2.2843379974365234
training step: 47669, total_loss: 0.7104859948158264
training step: 47670, total_loss: 0.9974126219749451
training step: 47671, total_loss: 0.7913322448730469
training step: 47672, total_loss: 2.161489486694336
training step: 47673, total_loss: 2.7616052627563477
training step: 47674, total_loss: 4.824602127075195
training step: 47675, total_loss: 0.9721114039421082
training step: 47676, total_loss: 3.4377901554107666
training step: 47677, total_loss: 2.61012601852417
training step: 47678, total_loss: 1.0292669534683228
training step: 47679, total_loss: 1.5147185325622559
training step: 47680, total_loss: 0.09906140714883804
training step: 47681, total_loss: 1.9197498559951782
training step: 47682, total_loss: 0.9360169172286987
training step: 47683, total_loss: 1.559645652770996
training step: 47684, total_loss: 0.23884311318397522
training step: 47685, total_loss: 2.9250824451446533
training step: 47686, total_loss: 0.9289017915725708
training step: 47687, total_loss: 0.1898101568222046
training step: 47688, total_loss: 2.298922538757324
training step: 47689, total_loss: 0.6726760864257812
training step: 47690, total_loss: 1.2244534492492676
training step: 47691, total_loss: 0.6109783053398132
training step: 47692, total_loss: 4.054940223693848
training step: 47693, total_loss: 1.533205270767212
training step: 47694, total_loss: 1.5407274961471558
training step: 47695, total_loss: 5.2230377197265625
training step: 47696, total_loss: 3.130005359649658
training step: 47697, total_loss: 0.4333685636520386
training step: 47698, total_loss: 1.4975614547729492
training step: 47699, total_loss: 0.5261359214782715
training step: 47700, total_loss: 0.7782306671142578
training step: 47701, total_loss: 0.033403001725673676
training step: 47702, total_loss: 6.752977060386911e-05
training step: 47703, total_loss: 0.9679021835327148
training step: 47704, total_loss: 0.6670501232147217
training step: 47705, total_loss: 0.7422802448272705
training step: 47706, total_loss: 4.08579683303833
training step: 47707, total_loss: 1.9952702522277832
training step: 47708, total_loss: 2.999692916870117
training step: 47709, total_loss: 0.6515210866928101
training step: 47710, total_loss: 1.4843389987945557
training step: 47711, total_loss: 0.9218809604644775
training step: 47712, total_loss: 2.1472067832946777
training step: 47713, total_loss: 0.0003019112045876682
training step: 47714, total_loss: 0.5328515768051147
training step: 47715, total_loss: 2.6817803382873535
training step: 47716, total_loss: 1.3723835945129395
training step: 47717, total_loss: 1.2323307991027832
training step: 47718, total_loss: 1.6006543636322021
training step: 47719, total_loss: 3.9424474239349365
training step: 47720, total_loss: 1.6418530941009521
training step: 47721, total_loss: 1.7162480354309082
training step: 47722, total_loss: 0.5680151581764221
training step: 47723, total_loss: 0.9783530831336975
training step: 47724, total_loss: 2.8398776054382324
training step: 47725, total_loss: 1.6375027894973755
training step: 47726, total_loss: 2.5514001846313477
training step: 47727, total_loss: 0.31771737337112427
training step: 47728, total_loss: 2.027198076248169
training step: 47729, total_loss: 0.4334254264831543
training step: 47730, total_loss: 2.1559269428253174
training step: 47731, total_loss: 0.6380866765975952
training step: 47732, total_loss: 3.5598323345184326
training step: 47733, total_loss: 0.3358481824398041
training step: 47734, total_loss: 0.27008089423179626
training step: 47735, total_loss: 1.10141921043396
training step: 47736, total_loss: 3.006381034851074
training step: 47737, total_loss: 2.3288819789886475
training step: 47738, total_loss: 1.3428921699523926
training step: 47739, total_loss: 1.5374512672424316
training step: 47740, total_loss: 0.1920589804649353
training step: 47741, total_loss: 0.9677316546440125
training step: 47742, total_loss: 1.6146767139434814
training step: 47743, total_loss: 2.1121344566345215
training step: 47744, total_loss: 1.2056207656860352
training step: 47745, total_loss: 2.121245861053467
training step: 47746, total_loss: 2.243962287902832
training step: 47747, total_loss: 3.9081459045410156
training step: 47748, total_loss: 3.3292062282562256
training step: 47749, total_loss: 5.055962562561035
training step: 47750, total_loss: 0.031315598636865616
training step: 47751, total_loss: 1.2257716655731201
training step: 47752, total_loss: 1.0748335123062134
training step: 47753, total_loss: 0.5197787284851074
training step: 47754, total_loss: 2.9564318656921387
training step: 47755, total_loss: 1.2278416156768799
training step: 47756, total_loss: 0.7792052030563354
training step: 47757, total_loss: 2.9094579219818115
training step: 47758, total_loss: 2.0253682136535645
training step: 47759, total_loss: 1.987816572189331
training step: 47760, total_loss: 3.3242149353027344
training step: 47761, total_loss: 4.924411773681641
training step: 47762, total_loss: 0.5117509365081787
training step: 47763, total_loss: 2.5193662643432617
training step: 47764, total_loss: 1.1075571775436401
training step: 47765, total_loss: 3.4482553005218506
training step: 47766, total_loss: 1.1377086639404297
training step: 47767, total_loss: 2.8711206912994385
training step: 47768, total_loss: 3.3329451084136963
training step: 47769, total_loss: 1.510809302330017
training step: 47770, total_loss: 1.9943207502365112
training step: 47771, total_loss: 3.3973779678344727
training step: 47772, total_loss: 0.939302921295166
training step: 47773, total_loss: 1.3111741542816162
training step: 47774, total_loss: 1.4845118522644043
training step: 47775, total_loss: 1.532167911529541
training step: 47776, total_loss: 1.8521959781646729
training step: 47777, total_loss: 1.0857138633728027
training step: 47778, total_loss: 0.608583390712738
training step: 47779, total_loss: 2.6849820613861084
training step: 47780, total_loss: 0.017940638586878777
training step: 47781, total_loss: 0.6032280921936035
training step: 47782, total_loss: 0.9556079506874084
training step: 47783, total_loss: 2.048637866973877
training step: 47784, total_loss: 0.4019516110420227
training step: 47785, total_loss: 0.5986313819885254
training step: 47786, total_loss: 1.632824420928955
training step: 47787, total_loss: 2.572969436645508
training step: 47788, total_loss: 2.4814600944519043
training step: 47789, total_loss: 0.2996565103530884
training step: 47790, total_loss: 4.175837516784668
training step: 47791, total_loss: 0.8736811876296997
training step: 47792, total_loss: 0.9044763445854187
training step: 47793, total_loss: 0.1592782884836197
training step: 47794, total_loss: 3.135770797729492
training step: 47795, total_loss: 2.4415342807769775
training step: 47796, total_loss: 2.884402275085449
training step: 47797, total_loss: 1.572049617767334
training step: 47798, total_loss: 6.210862159729004
training step: 47799, total_loss: 2.8890442848205566
training step: 47800, total_loss: 0.2666022479534149
training step: 47801, total_loss: 0.7982094287872314
training step: 47802, total_loss: 1.9437410831451416
training step: 47803, total_loss: 5.562787055969238
training step: 47804, total_loss: 0.07963903993368149
training step: 47805, total_loss: 0.2503803074359894
training step: 47806, total_loss: 0.0705135241150856
training step: 47807, total_loss: 1.107437014579773
training step: 47808, total_loss: 2.043196201324463
training step: 47809, total_loss: 1.6656739711761475
training step: 47810, total_loss: 2.1688361167907715
training step: 47811, total_loss: 2.8611011505126953
training step: 47812, total_loss: 0.004014886450022459
training step: 47813, total_loss: 1.4346234798431396
training step: 47814, total_loss: 0.2310141623020172
training step: 47815, total_loss: 1.2316792011260986
training step: 47816, total_loss: 0.37443655729293823
training step: 47817, total_loss: 2.423475980758667
training step: 47818, total_loss: 3.9077258110046387
training step: 47819, total_loss: 0.011694463901221752
training step: 47820, total_loss: 2.4450149536132812
training step: 47821, total_loss: 2.603271245956421
training step: 47822, total_loss: 2.3895106315612793
training step: 47823, total_loss: 1.3457921743392944
training step: 47824, total_loss: 3.1795740127563477
training step: 47825, total_loss: 1.2389459609985352
training step: 47826, total_loss: 2.2387123107910156
training step: 47827, total_loss: 1.538783073425293
training step: 47828, total_loss: 1.0585713386535645
training step: 47829, total_loss: 1.9785493612289429
training step: 47830, total_loss: 1.300761342048645
training step: 47831, total_loss: 0.8543484807014465
training step: 47832, total_loss: 0.2786484956741333
training step: 47833, total_loss: 2.2746291160583496
training step: 47834, total_loss: 0.5211448669433594
training step: 47835, total_loss: 1.0898551940917969
training step: 47836, total_loss: 2.0376243591308594
training step: 47837, total_loss: 0.9692103862762451
training step: 47838, total_loss: 2.9604439735412598
training step: 47839, total_loss: 1.9843087196350098
training step: 47840, total_loss: 6.444916248321533
training step: 47841, total_loss: 5.545385837554932
training step: 47842, total_loss: 0.7642797231674194
training step: 47843, total_loss: 0.865695595741272
training step: 47844, total_loss: 1.1644493341445923
training step: 47845, total_loss: 1.7011901140213013
training step: 47846, total_loss: 1.0325840711593628
training step: 47847, total_loss: 0.6455693244934082
training step: 47848, total_loss: 3.0449109077453613
training step: 47849, total_loss: 0.743135929107666
training step: 47850, total_loss: 0.18157944083213806
training step: 47851, total_loss: 1.6626862287521362
training step: 47852, total_loss: 1.7433751821517944
training step: 47853, total_loss: 0.9115685224533081
training step: 47854, total_loss: 0.596656084060669
training step: 47855, total_loss: 0.9424734115600586
training step: 47856, total_loss: 0.9289048910140991
training step: 47857, total_loss: 1.489431619644165
training step: 47858, total_loss: 9.107174992095679e-05
training step: 47859, total_loss: 0.3578411340713501
training step: 47860, total_loss: 0.3871096670627594
training step: 47861, total_loss: 1.7241915464401245
training step: 47862, total_loss: 3.209648847579956
training step: 47863, total_loss: 2.291862964630127
training step: 47864, total_loss: 2.4562268257141113
training step: 47865, total_loss: 2.058765172958374
training step: 47866, total_loss: 1.9440268278121948
training step: 47867, total_loss: 0.6717671155929565
training step: 47868, total_loss: 1.1204158067703247
training step: 47869, total_loss: 3.7330539226531982
training step: 47870, total_loss: 0.8469909429550171
training step: 47871, total_loss: 1.2865409851074219
training step: 47872, total_loss: 1.4530298709869385
training step: 47873, total_loss: 1.0470566749572754
training step: 47874, total_loss: 0.6649894118309021
training step: 47875, total_loss: 4.6766862869262695
training step: 47876, total_loss: 1.411871075630188
training step: 47877, total_loss: 3.747103452682495
training step: 47878, total_loss: 2.391970634460449
training step: 47879, total_loss: 3.5169217586517334
training step: 47880, total_loss: 1.1008411645889282
training step: 47881, total_loss: 4.0230913162231445
training step: 47882, total_loss: 0.4277200996875763
training step: 47883, total_loss: 0.7009841203689575
training step: 47884, total_loss: 2.5593323707580566
training step: 47885, total_loss: 1.3065252304077148
training step: 47886, total_loss: 3.1367485523223877
training step: 47887, total_loss: 0.7231756448745728
training step: 47888, total_loss: 0.6121195554733276
training step: 47889, total_loss: 0.9341846108436584
training step: 47890, total_loss: 1.0127183198928833
training step: 47891, total_loss: 1.2513225078582764
training step: 47892, total_loss: 1.1816474199295044
training step: 47893, total_loss: 2.600431442260742
training step: 47894, total_loss: 0.9897259473800659
training step: 47895, total_loss: 2.8964931964874268
training step: 47896, total_loss: 2.82600736618042
training step: 47897, total_loss: 1.4501694440841675
training step: 47898, total_loss: 1.09356689453125
training step: 47899, total_loss: 0.9864634871482849
training step: 47900, total_loss: 1.41265869140625
training step: 47901, total_loss: 0.9045783281326294
training step: 47902, total_loss: 0.31821149587631226
training step: 47903, total_loss: 1.0483050346374512INFO:tensorflow:Writing predictions to: test_output/predictions_48000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_48000.json

training step: 47904, total_loss: 2.924654006958008
training step: 47905, total_loss: 3.666501522064209
training step: 47906, total_loss: 2.3920390605926514
training step: 47907, total_loss: 2.405001640319824
training step: 47908, total_loss: 2.4346508979797363
training step: 47909, total_loss: 0.8664316534996033
training step: 47910, total_loss: 1.7287507057189941
training step: 47911, total_loss: 4.040768623352051
training step: 47912, total_loss: 3.7803783416748047
training step: 47913, total_loss: 0.9979910254478455
training step: 47914, total_loss: 2.509312152862549
training step: 47915, total_loss: 2.4731218814849854
training step: 47916, total_loss: 0.28196731209754944
training step: 47917, total_loss: 0.6116690039634705
training step: 47918, total_loss: 2.327911853790283
training step: 47919, total_loss: 5.648630619049072
training step: 47920, total_loss: 3.0321621894836426
training step: 47921, total_loss: 1.3352141380310059
training step: 47922, total_loss: 2.8238115310668945
training step: 47923, total_loss: 1.444757342338562
training step: 47924, total_loss: 1.3620507717132568
training step: 47925, total_loss: 0.7583568096160889
training step: 47926, total_loss: 1.3927571773529053
training step: 47927, total_loss: 1.3037437200546265
training step: 47928, total_loss: 0.44348442554473877
training step: 47929, total_loss: 1.4191974401474
training step: 47930, total_loss: 0.00016300423885695636
training step: 47931, total_loss: 1.5225653648376465
training step: 47932, total_loss: 2.6403536796569824
training step: 47933, total_loss: 0.9202638864517212
training step: 47934, total_loss: 1.272157073020935
training step: 47935, total_loss: 1.75791597366333
training step: 47936, total_loss: 1.5852895975112915
training step: 47937, total_loss: 2.380828380584717
training step: 47938, total_loss: 5.888919830322266
training step: 47939, total_loss: 2.74178409576416
training step: 47940, total_loss: 1.5048911571502686
training step: 47941, total_loss: 1.9244301319122314
training step: 47942, total_loss: 1.172305941581726
training step: 47943, total_loss: 0.976091742515564
training step: 47944, total_loss: 0.5825722217559814
training step: 47945, total_loss: 0.5198858976364136
training step: 47946, total_loss: 1.5626797676086426
training step: 47947, total_loss: 0.3170807659626007
training step: 47948, total_loss: 0.6120865941047668
training step: 47949, total_loss: 1.0459569692611694
training step: 47950, total_loss: 0.15341906249523163
training step: 47951, total_loss: 2.018820285797119
training step: 47952, total_loss: 1.6861478090286255
training step: 47953, total_loss: 2.5702714920043945
training step: 47954, total_loss: 5.496138572692871
training step: 47955, total_loss: 0.08520419150590897
training step: 47956, total_loss: 0.5689706206321716
training step: 47957, total_loss: 1.1347079277038574
training step: 47958, total_loss: 1.3320276737213135
training step: 47959, total_loss: 0.8218688368797302
training step: 47960, total_loss: 0.6012168526649475
training step: 47961, total_loss: 0.7519081234931946
training step: 47962, total_loss: 2.0415730476379395
training step: 47963, total_loss: 1.3258084058761597
training step: 47964, total_loss: 1.0031499862670898
training step: 47965, total_loss: 2.0505785942077637
training step: 47966, total_loss: 4.168900489807129
training step: 47967, total_loss: 1.8374067544937134
training step: 47968, total_loss: 0.13099730014801025
training step: 47969, total_loss: 2.808609962463379
training step: 47970, total_loss: 1.7361197471618652
training step: 47971, total_loss: 1.8635573387145996
training step: 47972, total_loss: 1.8881217241287231
training step: 47973, total_loss: 3.2477989196777344
training step: 47974, total_loss: 0.9016844034194946
training step: 47975, total_loss: 1.24613356590271
training step: 47976, total_loss: 1.9848694801330566
training step: 47977, total_loss: 2.264849901199341
training step: 47978, total_loss: 1.5554547309875488
training step: 47979, total_loss: 2.1474595069885254
training step: 47980, total_loss: 0.7945271730422974
training step: 47981, total_loss: 1.6644777059555054
training step: 47982, total_loss: 1.3598648309707642
training step: 47983, total_loss: 2.0127906799316406
training step: 47984, total_loss: 2.0568106174468994
training step: 47985, total_loss: 0.12719973921775818
training step: 47986, total_loss: 3.36271071434021
training step: 47987, total_loss: 0.24962572753429413
training step: 47988, total_loss: 1.3417987823486328
training step: 47989, total_loss: 0.9741570353507996
training step: 47990, total_loss: 2.835056781768799
training step: 47991, total_loss: 0.6058821082115173
training step: 47992, total_loss: 1.3731977939605713
training step: 47993, total_loss: 1.581379771232605
training step: 47994, total_loss: 2.4585649967193604
training step: 47995, total_loss: 1.0755506753921509
training step: 47996, total_loss: 1.1896462440490723
training step: 47997, total_loss: 1.7298939228057861
training step: 47998, total_loss: 4.426249980926514
training step: 47999, total_loss: 0.16472695767879486
training step: 48000, total_loss: 1.8659024238586426
epoch finished! shuffle=False
evaluation: 12000, total_loss: 1.9007704257965088, f1: 55.20138582854673, followup: 19.154115320249506, yesno: 77.43800395557584, heq: 49.44469800699833, dheq: 3.3

Model saved in path test_output//model_48000.ckpt
training step: 48001, total_loss: 0.9092292785644531
training step: 48002, total_loss: 0.25167298316955566
training step: 48003, total_loss: 1.8676146268844604
training step: 48004, total_loss: 1.7750319242477417
training step: 48005, total_loss: 2.184022903442383
training step: 48006, total_loss: 1.2806026935577393
training step: 48007, total_loss: 2.027839183807373
training step: 48008, total_loss: 0.47151362895965576
training step: 48009, total_loss: 2.2236452102661133
training step: 48010, total_loss: 0.8903347253799438
training step: 48011, total_loss: 3.810554027557373
training step: 48012, total_loss: 0.8562920689582825
training step: 48013, total_loss: 0.3335343599319458
training step: 48014, total_loss: 3.257967472076416
training step: 48015, total_loss: 2.2437689304351807
training step: 48016, total_loss: 1.6609292030334473
training step: 48017, total_loss: 0.4405478537082672
training step: 48018, total_loss: 2.3783321380615234
training step: 48019, total_loss: 1.378741979598999
training step: 48020, total_loss: 1.1234683990478516
training step: 48021, total_loss: 3.1152117252349854
training step: 48022, total_loss: 1.8609395027160645
training step: 48023, total_loss: 1.8605457544326782
training step: 48024, total_loss: 0.7446242570877075
training step: 48025, total_loss: 1.7786579132080078
training step: 48026, total_loss: 2.2607123851776123
training step: 48027, total_loss: 2.4740471839904785
training step: 48028, total_loss: 0.6159517168998718
training step: 48029, total_loss: 2.424342632293701
training step: 48030, total_loss: 1.4691836833953857
training step: 48031, total_loss: 0.7408028244972229
training step: 48032, total_loss: 1.137966513633728
training step: 48033, total_loss: 3.4328958988189697
training step: 48034, total_loss: 0.8068419694900513
training step: 48035, total_loss: 0.30850154161453247
training step: 48036, total_loss: 1.3563852310180664
training step: 48037, total_loss: 2.0340664386749268
training step: 48038, total_loss: 0.9044615030288696
training step: 48039, total_loss: 1.3904577493667603
training step: 48040, total_loss: 0.9985734224319458
training step: 48041, total_loss: 0.852400541305542
training step: 48042, total_loss: 1.8115899562835693
training step: 48043, total_loss: 0.23496440052986145
training step: 48044, total_loss: 4.827410697937012
training step: 48045, total_loss: 1.0119585990905762
training step: 48046, total_loss: 3.4352474212646484
training step: 48047, total_loss: 0.8006620407104492
training step: 48048, total_loss: 1.4292324781417847
training step: 48049, total_loss: 0.013463438488543034
training step: 48050, total_loss: 2.262514114379883
training step: 48051, total_loss: 2.3911991119384766
training step: 48052, total_loss: 0.5380685329437256
training step: 48053, total_loss: 0.1614455133676529
training step: 48054, total_loss: 4.232763290405273
training step: 48055, total_loss: 2.7976632118225098
training step: 48056, total_loss: 0.10927466303110123
training step: 48057, total_loss: 0.2512706518173218
training step: 48058, total_loss: 0.6058834791183472
training step: 48059, total_loss: 0.5127694606781006
training step: 48060, total_loss: 3.9560906887054443
training step: 48061, total_loss: 2.025022029876709
training step: 48062, total_loss: 0.007032036781311035
training step: 48063, total_loss: 1.5630552768707275
training step: 48064, total_loss: 4.424193859100342
training step: 48065, total_loss: 1.8836051225662231
training step: 48066, total_loss: 2.497234344482422
training step: 48067, total_loss: 4.358959197998047
training step: 48068, total_loss: 5.944263458251953
training step: 48069, total_loss: 0.8812867403030396
training step: 48070, total_loss: 3.129666328430176
training step: 48071, total_loss: 1.9062525033950806
training step: 48072, total_loss: 4.135434627532959
training step: 48073, total_loss: 3.0976905822753906
training step: 48074, total_loss: 1.0168262720108032
training step: 48075, total_loss: 2.7564220428466797
training step: 48076, total_loss: 4.452483177185059
training step: 48077, total_loss: 0.5377932190895081
training step: 48078, total_loss: 1.760110855102539
training step: 48079, total_loss: 0.41638869047164917
training step: 48080, total_loss: 2.526318073272705
training step: 48081, total_loss: 0.6975300312042236
training step: 48082, total_loss: 1.1228578090667725
training step: 48083, total_loss: 1.5106642246246338
training step: 48084, total_loss: 3.8822896480560303
training step: 48085, total_loss: 2.2990636825561523
training step: 48086, total_loss: 2.519444465637207
training step: 48087, total_loss: 2.5025808811187744
training step: 48088, total_loss: 4.765222072601318
training step: 48089, total_loss: 1.5200724601745605
training step: 48090, total_loss: 3.024472713470459
training step: 48091, total_loss: 2.296924114227295
training step: 48092, total_loss: 1.1237308979034424
training step: 48093, total_loss: 0.8210625052452087
training step: 48094, total_loss: 0.8140567541122437
training step: 48095, total_loss: 1.0264427661895752
training step: 48096, total_loss: 1.0657355785369873
training step: 48097, total_loss: 3.718128204345703
training step: 48098, total_loss: 0.8205116987228394
training step: 48099, total_loss: 1.9331198930740356
training step: 48100, total_loss: 2.4018850326538086
training step: 48101, total_loss: 1.1411913633346558
training step: 48102, total_loss: 0.362819105386734
training step: 48103, total_loss: 0.16726623475551605
training step: 48104, total_loss: 0.7447932958602905
training step: 48105, total_loss: 2.8576416969299316
training step: 48106, total_loss: 2.2657437324523926
training step: 48107, total_loss: 0.6396805644035339
training step: 48108, total_loss: 3.5858283042907715
training step: 48109, total_loss: 1.503065586090088
training step: 48110, total_loss: 2.1928207874298096
training step: 48111, total_loss: 0.33920255303382874
training step: 48112, total_loss: 1.799515724182129
training step: 48113, total_loss: 3.3979830741882324
training step: 48114, total_loss: 2.753441095352173
training step: 48115, total_loss: 1.2075142860412598
training step: 48116, total_loss: 1.0768826007843018
training step: 48117, total_loss: 0.8036511540412903
training step: 48118, total_loss: 0.5519687533378601
training step: 48119, total_loss: 1.6356912851333618
training step: 48120, total_loss: 2.250124454498291
training step: 48121, total_loss: 0.8897535800933838
training step: 48122, total_loss: 4.203280448913574
training step: 48123, total_loss: 0.4562338888645172
training step: 48124, total_loss: 2.0887622833251953
training step: 48125, total_loss: 0.32497328519821167
training step: 48126, total_loss: 3.1521010398864746
training step: 48127, total_loss: 1.1874260902404785
training step: 48128, total_loss: 1.0293989181518555
training step: 48129, total_loss: 0.41083043813705444
training step: 48130, total_loss: 0.9510838985443115
training step: 48131, total_loss: 0.6703622341156006
training step: 48132, total_loss: 1.3018450736999512
training step: 48133, total_loss: 1.4195363521575928
training step: 48134, total_loss: 1.3941881656646729
training step: 48135, total_loss: 1.369777798652649
training step: 48136, total_loss: 2.433455467224121
training step: 48137, total_loss: 2.2091588973999023
training step: 48138, total_loss: 1.4471282958984375
training step: 48139, total_loss: 1.9987611770629883
training step: 48140, total_loss: 1.1890009641647339
training step: 48141, total_loss: 2.7837376594543457
training step: 48142, total_loss: 1.3577044010162354
training step: 48143, total_loss: 1.0575231313705444
training step: 48144, total_loss: 0.7622736692428589
training step: 48145, total_loss: 0.340958833694458
training step: 48146, total_loss: 2.911116123199463
training step: 48147, total_loss: 2.713449478149414
training step: 48148, total_loss: 2.4913363456726074
training step: 48149, total_loss: 0.1713189333677292
training step: 48150, total_loss: 1.2049461603164673
training step: 48151, total_loss: 0.0005579774733632803
training step: 48152, total_loss: 0.7567315101623535
training step: 48153, total_loss: 0.7130763530731201
training step: 48154, total_loss: 2.4331016540527344
training step: 48155, total_loss: 2.4188802242279053
training step: 48156, total_loss: 3.0154647827148438
training step: 48157, total_loss: 1.7844367027282715
training step: 48158, total_loss: 0.45223867893218994
training step: 48159, total_loss: 4.95796012878418
training step: 48160, total_loss: 2.821115493774414
training step: 48161, total_loss: 2.183424949645996
training step: 48162, total_loss: 0.804475724697113
training step: 48163, total_loss: 1.6565583944320679
training step: 48164, total_loss: 2.306408643722534
training step: 48165, total_loss: 2.4773571491241455
training step: 48166, total_loss: 3.488015651702881
training step: 48167, total_loss: 1.6336560249328613
training step: 48168, total_loss: 2.288066864013672
training step: 48169, total_loss: 1.1364246606826782
training step: 48170, total_loss: 0.07212354242801666
training step: 48171, total_loss: 1.0376547574996948
training step: 48172, total_loss: 0.2857150435447693
training step: 48173, total_loss: 2.129866123199463
training step: 48174, total_loss: 2.5034573078155518
training step: 48175, total_loss: 2.9275765419006348
training step: 48176, total_loss: 1.5127089023590088
training step: 48177, total_loss: 3.0638322830200195
training step: 48178, total_loss: 1.966357707977295
training step: 48179, total_loss: 3.3734750747680664
training step: 48180, total_loss: 1.6469109058380127
training step: 48181, total_loss: 2.0089612007141113
training step: 48182, total_loss: 2.697047472000122
training step: 48183, total_loss: 0.03337618708610535
training step: 48184, total_loss: 0.9838076829910278
training step: 48185, total_loss: 4.6986260414123535
training step: 48186, total_loss: 1.3411874771118164
training step: 48187, total_loss: 0.4735492467880249
training step: 48188, total_loss: 3.195093870162964
training step: 48189, total_loss: 1.0925298929214478
training step: 48190, total_loss: 1.6260969638824463
training step: 48191, total_loss: 2.803903102874756
training step: 48192, total_loss: 0.5476833581924438
training step: 48193, total_loss: 2.843445301055908
training step: 48194, total_loss: 0.32304689288139343
training step: 48195, total_loss: 2.220266342163086
training step: 48196, total_loss: 0.9894610643386841
training step: 48197, total_loss: 1.7953524589538574
training step: 48198, total_loss: 1.418622374534607
training step: 48199, total_loss: 0.9294484853744507
training step: 48200, total_loss: 1.0333516597747803
training step: 48201, total_loss: 0.5630156993865967
training step: 48202, total_loss: 1.9713722467422485
training step: 48203, total_loss: 1.3437232971191406
training step: 48204, total_loss: 0.7468113303184509
training step: 48205, total_loss: 0.21124334633350372
training step: 48206, total_loss: 2.2947120666503906
training step: 48207, total_loss: 1.7974724769592285
training step: 48208, total_loss: 1.8458539247512817
training step: 48209, total_loss: 1.656886339187622
training step: 48210, total_loss: 3.198608160018921
training step: 48211, total_loss: 1.1939671039581299
training step: 48212, total_loss: 1.4408395290374756
training step: 48213, total_loss: 0.03331420570611954
training step: 48214, total_loss: 0.07577669620513916
training step: 48215, total_loss: 0.17795592546463013
training step: 48216, total_loss: 0.44785135984420776
training step: 48217, total_loss: 5.0326128005981445
training step: 48218, total_loss: 1.22248375415802
training step: 48219, total_loss: 0.2535884976387024
training step: 48220, total_loss: 1.2934811115264893
training step: 48221, total_loss: 2.685716390609741
training step: 48222, total_loss: 1.198291301727295
training step: 48223, total_loss: 6.511422157287598
training step: 48224, total_loss: 0.0006550755351781845
training step: 48225, total_loss: 1.648703694343567
training step: 48226, total_loss: 0.5657656192779541
training step: 48227, total_loss: 2.6593265533447266
training step: 48228, total_loss: 0.289150208234787
training step: 48229, total_loss: 2.2714614868164062
training step: 48230, total_loss: 2.8666627407073975
training step: 48231, total_loss: 0.9247028827667236
training step: 48232, total_loss: 0.8378601670265198
training step: 48233, total_loss: 0.8046300411224365
training step: 48234, total_loss: 0.010555252432823181
training step: 48235, total_loss: 0.1943274736404419
training step: 48236, total_loss: 1.645638346672058
training step: 48237, total_loss: 1.6350257396697998
training step: 48238, total_loss: 0.5586413741111755
training step: 48239, total_loss: 1.2415271997451782
training step: 48240, total_loss: 1.7870361804962158
training step: 48241, total_loss: 1.3567413091659546
training step: 48242, total_loss: 0.6730055212974548
training step: 48243, total_loss: 1.2065577507019043
training step: 48244, total_loss: 3.883894920349121
training step: 48245, total_loss: 1.0583198070526123
training step: 48246, total_loss: 1.1515774726867676
training step: 48247, total_loss: 0.26131105422973633
training step: 48248, total_loss: 2.333341121673584
training step: 48249, total_loss: 0.30563539266586304
training step: 48250, total_loss: 2.430497407913208
training step: 48251, total_loss: 2.7604267597198486
training step: 48252, total_loss: 0.7604506015777588
training step: 48253, total_loss: 0.5185990333557129
training step: 48254, total_loss: 1.6354267597198486
training step: 48255, total_loss: 0.48006507754325867
training step: 48256, total_loss: 2.616140604019165
training step: 48257, total_loss: 1.7370681762695312
training step: 48258, total_loss: 0.0005457031074911356
training step: 48259, total_loss: 4.098133087158203
training step: 48260, total_loss: 0.6784389019012451
training step: 48261, total_loss: 0.0010243381839245558
training step: 48262, total_loss: 1.1916497945785522
training step: 48263, total_loss: 2.9424009323120117
training step: 48264, total_loss: 2.886173725128174
training step: 48265, total_loss: 0.2547057867050171
training step: 48266, total_loss: 2.935396671295166
training step: 48267, total_loss: 1.0648417472839355
training step: 48268, total_loss: 2.3514132499694824
training step: 48269, total_loss: 0.09630778431892395
training step: 48270, total_loss: 2.160554885864258
training step: 48271, total_loss: 4.218260765075684
training step: 48272, total_loss: 2.4400932788848877
training step: 48273, total_loss: 0.2103789746761322
training step: 48274, total_loss: 1.2093734741210938
training step: 48275, total_loss: 4.045176029205322
training step: 48276, total_loss: 0.6439305543899536
training step: 48277, total_loss: 1.1811461448669434
training step: 48278, total_loss: 2.1555237770080566
training step: 48279, total_loss: 2.85968279838562
training step: 48280, total_loss: 1.8442931175231934
training step: 48281, total_loss: 0.14954617619514465
training step: 48282, total_loss: 1.1252318620681763
training step: 48283, total_loss: 1.6829748153686523
training step: 48284, total_loss: 1.3604304790496826
training step: 48285, total_loss: 0.11362359672784805
training step: 48286, total_loss: 2.2357399463653564
training step: 48287, total_loss: 0.18806904554367065
training step: 48288, total_loss: 1.9499454498291016
training step: 48289, total_loss: 2.9488179683685303
training step: 48290, total_loss: 1.6729481220245361
training step: 48291, total_loss: 0.8303455114364624
training step: 48292, total_loss: 3.1151485443115234
training step: 48293, total_loss: 3.167613983154297
training step: 48294, total_loss: 3.826261043548584
training step: 48295, total_loss: 0.6507899761199951
training step: 48296, total_loss: 1.358182668685913
training step: 48297, total_loss: 2.1639575958251953
training step: 48298, total_loss: 1.1210167407989502
training step: 48299, total_loss: 4.253890514373779
training step: 48300, total_loss: 0.1070706769824028
training step: 48301, total_loss: 0.7830787897109985
training step: 48302, total_loss: 0.39274901151657104
training step: 48303, total_loss: 1.093575358390808
training step: 48304, total_loss: 1.2525720596313477
training step: 48305, total_loss: 1.6843702793121338
training step: 48306, total_loss: 1.1488267183303833
training step: 48307, total_loss: 2.5409321784973145
training step: 48308, total_loss: 1.543271541595459
training step: 48309, total_loss: 1.7050504684448242
training step: 48310, total_loss: 4.390389442443848
training step: 48311, total_loss: 0.9924572706222534
training step: 48312, total_loss: 3.0099246501922607
training step: 48313, total_loss: 1.0504170656204224
training step: 48314, total_loss: 2.6077613830566406
training step: 48315, total_loss: 2.690610647201538
training step: 48316, total_loss: 1.486562728881836
training step: 48317, total_loss: 0.3104294538497925
training step: 48318, total_loss: 3.419945240020752
training step: 48319, total_loss: 1.7033976316452026
training step: 48320, total_loss: 0.04601896554231644
training step: 48321, total_loss: 0.9550516605377197
training step: 48322, total_loss: 4.244561195373535
training step: 48323, total_loss: 1.6092314720153809
training step: 48324, total_loss: 0.43630924820899963
training step: 48325, total_loss: 0.9865299463272095
training step: 48326, total_loss: 3.8023056983947754
training step: 48327, total_loss: 1.5706523656845093
training step: 48328, total_loss: 1.1067538261413574
training step: 48329, total_loss: 0.49075567722320557
training step: 48330, total_loss: 3.1384096145629883
training step: 48331, total_loss: 1.5823688507080078
training step: 48332, total_loss: 1.0427438020706177
training step: 48333, total_loss: 0.622692883014679
training step: 48334, total_loss: 0.9010699987411499
training step: 48335, total_loss: 1.2773655652999878
training step: 48336, total_loss: 4.901815891265869
training step: 48337, total_loss: 1.5522838830947876
training step: 48338, total_loss: 1.8033490180969238
training step: 48339, total_loss: 4.735780239105225
training step: 48340, total_loss: 1.4131168127059937
training step: 48341, total_loss: 0.13908672332763672
training step: 48342, total_loss: 0.33594733476638794
training step: 48343, total_loss: 3.384530544281006
training step: 48344, total_loss: 4.893685817718506
training step: 48345, total_loss: 3.3003950119018555
training step: 48346, total_loss: 1.7343405485153198
training step: 48347, total_loss: 1.785413146018982
training step: 48348, total_loss: 2.174114465713501
training step: 48349, total_loss: 2.6590123176574707
training step: 48350, total_loss: 1.8879399299621582
training step: 48351, total_loss: 1.9915636777877808
training step: 48352, total_loss: 0.9446604251861572
training step: 48353, total_loss: 2.3060855865478516
training step: 48354, total_loss: 1.3104405403137207
training step: 48355, total_loss: 1.040908932685852
training step: 48356, total_loss: 2.5763227939605713
training step: 48357, total_loss: 1.8846750259399414
training step: 48358, total_loss: 0.5735520720481873
training step: 48359, total_loss: 0.2968106269836426
training step: 48360, total_loss: 1.2624030113220215
training step: 48361, total_loss: 0.47854962944984436
training step: 48362, total_loss: 0.7046713829040527
training step: 48363, total_loss: 1.8243787288665771
training step: 48364, total_loss: 2.7922213077545166
training step: 48365, total_loss: 0.87404865026474
training step: 48366, total_loss: 1.0451793670654297
training step: 48367, total_loss: 1.075416088104248
training step: 48368, total_loss: 1.2386448383331299
training step: 48369, total_loss: 1.7658936977386475
training step: 48370, total_loss: 1.50246000289917
training step: 48371, total_loss: 0.9988715648651123
training step: 48372, total_loss: 0.06971617788076401
training step: 48373, total_loss: 1.670214295387268
training step: 48374, total_loss: 1.26035737991333
training step: 48375, total_loss: 4.013976573944092
training step: 48376, total_loss: 0.7456268072128296
training step: 48377, total_loss: 1.9928661584854126
training step: 48378, total_loss: 1.2713596820831299
training step: 48379, total_loss: 1.9813790321350098
training step: 48380, total_loss: 1.914889931678772
training step: 48381, total_loss: 1.0504627227783203
training step: 48382, total_loss: 2.5268630981445312
training step: 48383, total_loss: 0.27401435375213623
training step: 48384, total_loss: 0.20164301991462708
training step: 48385, total_loss: 0.28383108973503113
training step: 48386, total_loss: 2.221733570098877
training step: 48387, total_loss: 0.8816338777542114
training step: 48388, total_loss: 2.589663028717041
training step: 48389, total_loss: 0.28674542903900146
training step: 48390, total_loss: 3.1099929809570312
training step: 48391, total_loss: 2.46929669380188
training step: 48392, total_loss: 0.4031810164451599
training step: 48393, total_loss: 1.883249282836914
training step: 48394, total_loss: 1.1859455108642578
training step: 48395, total_loss: 0.6165333986282349
training step: 48396, total_loss: 1.7341022491455078
training step: 48397, total_loss: 3.5642261505126953
training step: 48398, total_loss: 2.3631749153137207
training step: 48399, total_loss: 0.989883542060852
training step: 48400, total_loss: 2.0803606510162354
training step: 48401, total_loss: 3.0041329860687256
training step: 48402, total_loss: 1.3157391548156738
training step: 48403, total_loss: 1.3958317041397095
training step: 48404, total_loss: 1.9022685289382935
training step: 48405, total_loss: 1.8095331192016602
training step: 48406, total_loss: 6.886317253112793
training step: 48407, total_loss: 2.4152824878692627
training step: 48408, total_loss: 0.9259302616119385
training step: 48409, total_loss: 2.241044044494629
training step: 48410, total_loss: 1.4125598669052124
training step: 48411, total_loss: 1.6956408023834229
training step: 48412, total_loss: 1.6429864168167114
training step: 48413, total_loss: 1.9887504577636719
training step: 48414, total_loss: 2.909302234649658
training step: 48415, total_loss: 0.04758438095450401
training step: 48416, total_loss: 1.045606017112732
training step: 48417, total_loss: 0.42667827010154724
training step: 48418, total_loss: 1.1649222373962402
training step: 48419, total_loss: 0.34563156962394714
training step: 48420, total_loss: 0.6110305190086365
training step: 48421, total_loss: 1.6537060737609863
training step: 48422, total_loss: 0.05426344275474548
training step: 48423, total_loss: 2.0166447162628174
training step: 48424, total_loss: 2.1443960666656494
training step: 48425, total_loss: 3.557612895965576
training step: 48426, total_loss: 0.7933345437049866
training step: 48427, total_loss: 2.0339791774749756
training step: 48428, total_loss: 3.2930688858032227
training step: 48429, total_loss: 2.2589101791381836
training step: 48430, total_loss: 1.1373932361602783
training step: 48431, total_loss: 0.9445047378540039
training step: 48432, total_loss: 3.4846811294555664
training step: 48433, total_loss: 1.5083703994750977
training step: 48434, total_loss: 1.8425889015197754
training step: 48435, total_loss: 1.5294556617736816
training step: 48436, total_loss: 0.7738234400749207
training step: 48437, total_loss: 0.8827027082443237
training step: 48438, total_loss: 0.2391478270292282
training step: 48439, total_loss: 0.643101155757904
training step: 48440, total_loss: 0.6320104002952576
training step: 48441, total_loss: 0.7291650772094727
training step: 48442, total_loss: 0.5925950407981873
training step: 48443, total_loss: 5.068089485168457
training step: 48444, total_loss: 0.8005988597869873
training step: 48445, total_loss: 0.40434950590133667
training step: 48446, total_loss: 0.6837484836578369
training step: 48447, total_loss: 1.804957628250122
training step: 48448, total_loss: 1.1731483936309814
training step: 48449, total_loss: 0.49822133779525757
training step: 48450, total_loss: 1.3202784061431885
training step: 48451, total_loss: 3.318648099899292
training step: 48452, total_loss: 0.6988485455513
training step: 48453, total_loss: 0.0116810267791152
training step: 48454, total_loss: 3.048797845840454
training step: 48455, total_loss: 0.2947327196598053
training step: 48456, total_loss: 0.4888609051704407
training step: 48457, total_loss: 2.6542863845825195
training step: 48458, total_loss: 1.813584804534912
training step: 48459, total_loss: 3.0139496326446533
training step: 48460, total_loss: 0.8428475260734558
training step: 48461, total_loss: 1.396265983581543
training step: 48462, total_loss: 1.9299416542053223
training step: 48463, total_loss: 2.294748544692993
training step: 48464, total_loss: 1.6050986051559448
training step: 48465, total_loss: 3.41646409034729
training step: 48466, total_loss: 0.41709309816360474
training step: 48467, total_loss: 0.9500908851623535
training step: 48468, total_loss: 2.785400390625
training step: 48469, total_loss: 0.19132553040981293
training step: 48470, total_loss: 2.788081169128418
training step: 48471, total_loss: 0.19673685729503632
training step: 48472, total_loss: 2.514296531677246
training step: 48473, total_loss: 2.5964627265930176
training step: 48474, total_loss: 1.3645082712173462
training step: 48475, total_loss: 1.896559715270996
training step: 48476, total_loss: 1.7550852298736572
training step: 48477, total_loss: 2.6626884937286377
training step: 48478, total_loss: 1.1698312759399414
training step: 48479, total_loss: 1.5840635299682617
training step: 48480, total_loss: 4.241634845733643
training step: 48481, total_loss: 0.0702371597290039
training step: 48482, total_loss: 1.5631844997406006
training step: 48483, total_loss: 0.17999950051307678
training step: 48484, total_loss: 1.5011266469955444
training step: 48485, total_loss: 3.8600246906280518
training step: 48486, total_loss: 4.537692546844482
training step: 48487, total_loss: 1.5562152862548828
training step: 48488, total_loss: 1.1443625688552856
training step: 48489, total_loss: 1.686841607093811
training step: 48490, total_loss: 3.3100085258483887
training step: 48491, total_loss: 3.448636293411255
training step: 48492, total_loss: 3.2209529876708984
training step: 48493, total_loss: 1.4282774925231934
training step: 48494, total_loss: 3.2404794692993164
training step: 48495, total_loss: 2.386767625808716
training step: 48496, total_loss: 3.19209885597229
training step: 48497, total_loss: 0.7402912378311157
training step: 48498, total_loss: 1.3379385471343994
training step: 48499, total_loss: 0.724469780921936
training step: 48500, total_loss: 1.2929942607879639
training step: 48501, total_loss: 3.6771509647369385
training step: 48502, total_loss: 3.5053346157073975
training step: 48503, total_loss: 1.020308017730713
training step: 48504, total_loss: 0.9670840501785278
training step: 48505, total_loss: 0.8145462870597839
training step: 48506, total_loss: 0.36815345287323
training step: 48507, total_loss: 1.1231365203857422
training step: 48508, total_loss: 2.3577184677124023
training step: 48509, total_loss: 4.66873025894165
training step: 48510, total_loss: 2.2233076095581055
training step: 48511, total_loss: 0.9012404084205627
training step: 48512, total_loss: 0.6484149694442749
training step: 48513, total_loss: 1.9278309345245361
training step: 48514, total_loss: 1.4966397285461426
training step: 48515, total_loss: 0.5853320360183716
training step: 48516, total_loss: 0.8803074359893799
training step: 48517, total_loss: 2.0138189792633057
training step: 48518, total_loss: 2.2183802127838135
training step: 48519, total_loss: 0.8217769265174866
training step: 48520, total_loss: 0.9069882035255432
training step: 48521, total_loss: 4.080663204193115
training step: 48522, total_loss: 0.4760233163833618
training step: 48523, total_loss: 0.09037955850362778
training step: 48524, total_loss: 0.8115572929382324
training step: 48525, total_loss: 0.5577070116996765
training step: 48526, total_loss: 3.494140625
training step: 48527, total_loss: 2.495370388031006
training step: 48528, total_loss: 0.1665034145116806
training step: 48529, total_loss: 2.601378917694092
training step: 48530, total_loss: 1.8994388580322266
training step: 48531, total_loss: 1.2409553527832031
training step: 48532, total_loss: 0.8096276521682739
training step: 48533, total_loss: 0.23092851042747498
training step: 48534, total_loss: 0.03256469964981079
training step: 48535, total_loss: 3.322183609008789
training step: 48536, total_loss: 2.2600793838500977
training step: 48537, total_loss: 1.9423387050628662
training step: 48538, total_loss: 0.6665584444999695
training step: 48539, total_loss: 1.1849901676177979
training step: 48540, total_loss: 0.053031258285045624
training step: 48541, total_loss: 1.28706693649292
training step: 48542, total_loss: 0.8761172890663147
training step: 48543, total_loss: 0.14867709577083588
training step: 48544, total_loss: 0.0075013041496276855
training step: 48545, total_loss: 0.0006667855195701122
training step: 48546, total_loss: 1.9573763608932495
training step: 48547, total_loss: 0.10740448534488678
training step: 48548, total_loss: 0.5184032917022705
training step: 48549, total_loss: 0.5396274328231812
training step: 48550, total_loss: 1.6814396381378174
training step: 48551, total_loss: 1.3991780281066895
training step: 48552, total_loss: 3.4804232120513916
training step: 48553, total_loss: 4.024021148681641
training step: 48554, total_loss: 1.366264820098877
training step: 48555, total_loss: 0.7107223272323608
training step: 48556, total_loss: 3.4890456199645996
training step: 48557, total_loss: 1.7251574993133545
training step: 48558, total_loss: 1.642197608947754
training step: 48559, total_loss: 0.5019333958625793
training step: 48560, total_loss: 0.5164004564285278
training step: 48561, total_loss: 2.525066375732422
training step: 48562, total_loss: 2.5406196117401123
training step: 48563, total_loss: 4.9240312576293945
training step: 48564, total_loss: 2.1145009994506836
training step: 48565, total_loss: 1.1338220834732056
training step: 48566, total_loss: 3.2540340423583984
training step: 48567, total_loss: 0.9736169576644897
training step: 48568, total_loss: 0.0077747199684381485
training step: 48569, total_loss: 0.3570686876773834
training step: 48570, total_loss: 2.357708692550659
training step: 48571, total_loss: 0.2716760039329529
training step: 48572, total_loss: 1.1965197324752808
training step: 48573, total_loss: 1.6681430339813232
training step: 48574, total_loss: 7.985962390899658
training step: 48575, total_loss: 2.333878517150879
training step: 48576, total_loss: 5.5734405517578125
training step: 48577, total_loss: 0.9543056488037109
training step: 48578, total_loss: 2.053384304046631
training step: 48579, total_loss: 1.7268528938293457
training step: 48580, total_loss: 2.3677282333374023
training step: 48581, total_loss: 1.2290809154510498
training step: 48582, total_loss: 1.0632030963897705
training step: 48583, total_loss: 1.5345947742462158
training step: 48584, total_loss: 2.1560325622558594
training step: 48585, total_loss: 1.3098952770233154
training step: 48586, total_loss: 1.4616538286209106
training step: 48587, total_loss: 0.7861145734786987
training step: 48588, total_loss: 0.8880218267440796
training step: 48589, total_loss: 0.816329836845398
training step: 48590, total_loss: 1.9251887798309326
training step: 48591, total_loss: 1.586379885673523
training step: 48592, total_loss: 4.841736793518066
training step: 48593, total_loss: 0.3369660973548889
training step: 48594, total_loss: 0.6880841255187988
training step: 48595, total_loss: 0.4057474732398987
training step: 48596, total_loss: 1.559728741645813
training step: 48597, total_loss: 1.0814815759658813
training step: 48598, total_loss: 1.6332697868347168
training step: 48599, total_loss: 0.3018384575843811
training step: 48600, total_loss: 2.6694729328155518
training step: 48601, total_loss: 0.24639259278774261
training step: 48602, total_loss: 0.16232381761074066
training step: 48603, total_loss: 2.2784571647644043
training step: 48604, total_loss: 1.1612496376037598
training step: 48605, total_loss: 1.07109797000885
training step: 48606, total_loss: 0.08913775533437729
training step: 48607, total_loss: 0.9768587350845337
training step: 48608, total_loss: 0.43412190675735474
training step: 48609, total_loss: 3.482166290283203
training step: 48610, total_loss: 1.7018687725067139
training step: 48611, total_loss: 1.872161626815796
training step: 48612, total_loss: 1.0883114337921143
training step: 48613, total_loss: 3.337717294692993
training step: 48614, total_loss: 0.021257244050502777
training step: 48615, total_loss: 2.5460174083709717
training step: 48616, total_loss: 0.47279632091522217
training step: 48617, total_loss: 2.918290376663208
training step: 48618, total_loss: 0.08818408101797104
training step: 48619, total_loss: 5.60763692855835
training step: 48620, total_loss: 0.07678939402103424
training step: 48621, total_loss: 3.385206699371338
training step: 48622, total_loss: 3.702164888381958
training step: 48623, total_loss: 0.6949175596237183
training step: 48624, total_loss: 2.3680408000946045
training step: 48625, total_loss: 0.13778965175151825
training step: 48626, total_loss: 5.797946929931641
training step: 48627, total_loss: 2.708728313446045
training step: 48628, total_loss: 1.0797699689865112
training step: 48629, total_loss: 1.2195141315460205
training step: 48630, total_loss: 2.145869731903076
training step: 48631, total_loss: 2.7671284675598145
training step: 48632, total_loss: 0.9752843379974365
training step: 48633, total_loss: 1.2207218408584595
training step: 48634, total_loss: 3.459059715270996
training step: 48635, total_loss: 1.8995375633239746
training step: 48636, total_loss: 4.500211715698242
training step: 48637, total_loss: 2.1843180656433105
training step: 48638, total_loss: 1.4390592575073242
training step: 48639, total_loss: 2.4313712120056152
training step: 48640, total_loss: 2.1969425678253174
training step: 48641, total_loss: 0.9539048671722412
training step: 48642, total_loss: 0.04956044256687164
training step: 48643, total_loss: 3.268458366394043
training step: 48644, total_loss: 0.9264571666717529
training step: 48645, total_loss: 1.2355209589004517
training step: 48646, total_loss: 1.1922414302825928
training step: 48647, total_loss: 3.2219600677490234
training step: 48648, total_loss: 1.901336669921875
training step: 48649, total_loss: 2.567622184753418
training step: 48650, total_loss: 0.8142363429069519
training step: 48651, total_loss: 0.899136483669281
training step: 48652, total_loss: 5.782920837402344
training step: 48653, total_loss: 0.7120916247367859
training step: 48654, total_loss: 0.447482168674469
training step: 48655, total_loss: 0.4794187545776367
training step: 48656, total_loss: 1.087944507598877
training step: 48657, total_loss: 1.0509368181228638
training step: 48658, total_loss: 1.116426706314087
training step: 48659, total_loss: 0.9367156624794006
training step: 48660, total_loss: 2.107309103012085
training step: 48661, total_loss: 0.770383894443512
training step: 48662, total_loss: 0.4398827850818634
training step: 48663, total_loss: 1.4939336776733398
training step: 48664, total_loss: 1.0682506561279297
training step: 48665, total_loss: 1.0353209972381592
training step: 48666, total_loss: 1.0823866128921509
training step: 48667, total_loss: 2.244302749633789
training step: 48668, total_loss: 2.578125476837158
training step: 48669, total_loss: 0.5844137072563171
training step: 48670, total_loss: 0.8160289525985718
training step: 48671, total_loss: 0.45389971137046814
training step: 48672, total_loss: 2.090711832046509
training step: 48673, total_loss: 0.39485597610473633
training step: 48674, total_loss: 0.4942455291748047
training step: 48675, total_loss: 3.1934752464294434
training step: 48676, total_loss: 1.037542462348938
training step: 48677, total_loss: 2.874814987182617
training step: 48678, total_loss: 2.043217420578003
training step: 48679, total_loss: 3.1984665393829346
training step: 48680, total_loss: 1.9198672771453857
training step: 48681, total_loss: 2.810065507888794
training step: 48682, total_loss: 4.045226097106934
training step: 48683, total_loss: 0.031786080449819565
training step: 48684, total_loss: 2.800015687942505
training step: 48685, total_loss: 0.5314310789108276
training step: 48686, total_loss: 1.0066546201705933
training step: 48687, total_loss: 3.1153697967529297
training step: 48688, total_loss: 1.1872103214263916
training step: 48689, total_loss: 0.9067115187644958
training step: 48690, total_loss: 2.7857754230499268
training step: 48691, total_loss: 1.1339911222457886
training step: 48692, total_loss: 0.2597644031047821
training step: 48693, total_loss: 0.7743464708328247
training step: 48694, total_loss: 2.887021541595459
training step: 48695, total_loss: 1.6614938974380493
training step: 48696, total_loss: 1.415179967880249
training step: 48697, total_loss: 1.9204962253570557
training step: 48698, total_loss: 4.146446228027344
training step: 48699, total_loss: 0.19099688529968262
training step: 48700, total_loss: 0.4803447425365448
training step: 48701, total_loss: 1.3032419681549072
training step: 48702, total_loss: 3.89394474029541
training step: 48703, total_loss: 2.498889446258545
training step: 48704, total_loss: 1.3437553644180298
training step: 48705, total_loss: 0.6309758424758911
training step: 48706, total_loss: 0.18428514897823334
training step: 48707, total_loss: 0.061368897557258606
training step: 48708, total_loss: 1.7500157356262207
training step: 48709, total_loss: 3.011823892593384
training step: 48710, total_loss: 0.0007345704361796379
training step: 48711, total_loss: 1.2907391786575317
training step: 48712, total_loss: 5.933209419250488
training step: 48713, total_loss: 0.43369364738464355
training step: 48714, total_loss: 3.099393844604492
training step: 48715, total_loss: 0.026825720444321632
training step: 48716, total_loss: 1.6423178911209106
training step: 48717, total_loss: 1.4623305797576904
training step: 48718, total_loss: 2.1580536365509033
training step: 48719, total_loss: 3.1941990852355957
training step: 48720, total_loss: 1.8244640827178955
training step: 48721, total_loss: 3.5197091102600098
training step: 48722, total_loss: 2.3473265171051025
training step: 48723, total_loss: 4.191675186157227
training step: 48724, total_loss: 0.004577687941491604
training step: 48725, total_loss: 0.9574291706085205
training step: 48726, total_loss: 2.2333507537841797
training step: 48727, total_loss: 0.8102028369903564
training step: 48728, total_loss: 2.316675901412964
training step: 48729, total_loss: 1.9186584949493408
training step: 48730, total_loss: 7.015211303951219e-05
training step: 48731, total_loss: 1.7548842430114746
training step: 48732, total_loss: 0.09002318978309631
training step: 48733, total_loss: 2.7935190200805664
training step: 48734, total_loss: 0.8424690961837769
training step: 48735, total_loss: 0.3986314535140991
training step: 48736, total_loss: 0.2599504888057709
training step: 48737, total_loss: 2.185628652572632
training step: 48738, total_loss: 2.7698254585266113
training step: 48739, total_loss: 2.294806718826294
training step: 48740, total_loss: 1.318004846572876
training step: 48741, total_loss: 1.1471565961837769
training step: 48742, total_loss: 1.697156548500061
training step: 48743, total_loss: 1.4802401065826416
training step: 48744, total_loss: 0.027439814060926437
training step: 48745, total_loss: 0.0057844421826303005
training step: 48746, total_loss: 1.2986043691635132
training step: 48747, total_loss: 1.470628261566162
training step: 48748, total_loss: 2.0692169666290283
training step: 48749, total_loss: 0.09524890780448914
training step: 48750, total_loss: 0.4834645092487335
training step: 48751, total_loss: 2.7775490283966064
training step: 48752, total_loss: 0.37701529264450073
training step: 48753, total_loss: 4.700538635253906
training step: 48754, total_loss: 0.4838070273399353
training step: 48755, total_loss: 0.37301045656204224
training step: 48756, total_loss: 0.550322413444519
training step: 48757, total_loss: 0.7153494954109192
training step: 48758, total_loss: 0.2472216635942459
training step: 48759, total_loss: 4.035245895385742
training step: 48760, total_loss: 0.2880517840385437
training step: 48761, total_loss: 1.462069034576416
training step: 48762, total_loss: 1.0454380512237549
training step: 48763, total_loss: 3.549290180206299
training step: 48764, total_loss: 1.409020185470581
training step: 48765, total_loss: 6.057547569274902
training step: 48766, total_loss: 0.38685858249664307
training step: 48767, total_loss: 1.6577019691467285
training step: 48768, total_loss: 1.842970848083496
training step: 48769, total_loss: 0.13193699717521667
training step: 48770, total_loss: 0.06074271351099014
training step: 48771, total_loss: 4.441736221313477
training step: 48772, total_loss: 3.684741497039795
training step: 48773, total_loss: 0.16369888186454773
training step: 48774, total_loss: 0.027731072157621384
training step: 48775, total_loss: 1.893282413482666
training step: 48776, total_loss: 0.4587589502334595
training step: 48777, total_loss: 5.955442905426025
training step: 48778, total_loss: 0.9185174703598022
training step: 48779, total_loss: 0.4136837124824524
training step: 48780, total_loss: 3.666656017303467
training step: 48781, total_loss: 0.6290550231933594
training step: 48782, total_loss: 1.232290506362915
training step: 48783, total_loss: 0.13735643029212952
training step: 48784, total_loss: 0.07662662863731384
training step: 48785, total_loss: 0.867531418800354
training step: 48786, total_loss: 0.07972515374422073
training step: 48787, total_loss: 0.0007545396219938993
training step: 48788, total_loss: 7.481477737426758
training step: 48789, total_loss: 2.7025527954101562
training step: 48790, total_loss: 0.5845689177513123
training step: 48791, total_loss: 0.0024888012558221817
training step: 48792, total_loss: 1.1583476066589355
training step: 48793, total_loss: 1.946765422821045
training step: 48794, total_loss: 1.4581489562988281
training step: 48795, total_loss: 1.2362080812454224
training step: 48796, total_loss: 2.9710023403167725
training step: 48797, total_loss: 0.7139775156974792
training step: 48798, total_loss: 0.16835366189479828
training step: 48799, total_loss: 2.669924736022949
training step: 48800, total_loss: 0.19418616592884064
training step: 48801, total_loss: 3.067573070526123
training step: 48802, total_loss: 1.6802270412445068
training step: 48803, total_loss: 0.08159230649471283
training step: 48804, total_loss: 2.5418288707733154
training step: 48805, total_loss: 3.564213752746582
training step: 48806, total_loss: 4.623254299163818
training step: 48807, total_loss: 2.152634859085083
training step: 48808, total_loss: 4.942039489746094
training step: 48809, total_loss: 3.4345526695251465
training step: 48810, total_loss: 1.1135993003845215
training step: 48811, total_loss: 3.0585594177246094
training step: 48812, total_loss: 0.990746796131134
training step: 48813, total_loss: 1.9815053939819336
training step: 48814, total_loss: 0.3648347854614258
training step: 48815, total_loss: 2.839254379272461
training step: 48816, total_loss: 2.051002025604248
training step: 48817, total_loss: 2.0210418701171875
training step: 48818, total_loss: 0.7460670471191406
training step: 48819, total_loss: 2.3216092586517334
training step: 48820, total_loss: 1.051513910293579
training step: 48821, total_loss: 2.674312114715576
training step: 48822, total_loss: 0.45264893770217896
training step: 48823, total_loss: 0.4817745089530945
training step: 48824, total_loss: 1.4451357126235962
training step: 48825, total_loss: 2.2609381675720215
training step: 48826, total_loss: 2.2091071605682373
training step: 48827, total_loss: 0.4724028706550598
training step: 48828, total_loss: 0.15655921399593353
training step: 48829, total_loss: 2.3866991996765137
training step: 48830, total_loss: 1.301665186882019
training step: 48831, total_loss: 2.649451494216919
training step: 48832, total_loss: 2.076601982116699
training step: 48833, total_loss: 0.8329030871391296
training step: 48834, total_loss: 3.3550281524658203
training step: 48835, total_loss: 1.5635504722595215
training step: 48836, total_loss: 3.0400643348693848
training step: 48837, total_loss: 0.4172036647796631
training step: 48838, total_loss: 3.595280885696411
training step: 48839, total_loss: 2.581937313079834
training step: 48840, total_loss: 3.6207079887390137
training step: 48841, total_loss: 1.9168683290481567
training step: 48842, total_loss: 0.595670223236084
training step: 48843, total_loss: 2.4647388458251953
training step: 48844, total_loss: 2.374983787536621
training step: 48845, total_loss: 0.6351981163024902
training step: 48846, total_loss: 1.9283666610717773
training step: 48847, total_loss: 2.1370606422424316
training step: 48848, total_loss: 1.2193005084991455
training step: 48849, total_loss: 4.85296106338501
training step: 48850, total_loss: 1.6042635440826416
training step: 48851, total_loss: 3.1190223693847656
training step: 48852, total_loss: 2.2438368797302246
training step: 48853, total_loss: 1.2657885551452637
training step: 48854, total_loss: 0.2244655191898346
training step: 48855, total_loss: 1.9737114906311035
training step: 48856, total_loss: 0.9428539872169495
training step: 48857, total_loss: 4.745316505432129
training step: 48858, total_loss: 2.4840428829193115
training step: 48859, total_loss: 0.34181857109069824
training step: 48860, total_loss: 2.985391855239868
training step: 48861, total_loss: 5.7422404289245605
training step: 48862, total_loss: 3.7563023567199707
training step: 48863, total_loss: 2.277552366256714
training step: 48864, total_loss: 0.4877985715866089
training step: 48865, total_loss: 2.1106743812561035
training step: 48866, total_loss: 1.4301122426986694
training step: 48867, total_loss: 1.348925232887268
training step: 48868, total_loss: 1.1007983684539795
training step: 48869, total_loss: 1.248457431793213
training step: 48870, total_loss: 0.694861650466919
training step: 48871, total_loss: 2.95835542678833
training step: 48872, total_loss: 1.2102925777435303
training step: 48873, total_loss: 3.682131767272949
training step: 48874, total_loss: 4.743199825286865
training step: 48875, total_loss: 0.2181863635778427
training step: 48876, total_loss: 1.6626266241073608
training step: 48877, total_loss: 2.1107192039489746
training step: 48878, total_loss: 3.547837734222412
training step: 48879, total_loss: 1.5759754180908203
training step: 48880, total_loss: 1.2570408582687378
training step: 48881, total_loss: 1.6876111030578613
training step: 48882, total_loss: 0.264724463224411
training step: 48883, total_loss: 1.0545321702957153
training step: 48884, total_loss: 1.756593942642212
training step: 48885, total_loss: 1.2087522745132446
training step: 48886, total_loss: 2.2002460956573486
training step: 48887, total_loss: 2.785327672958374
training step: 48888, total_loss: 3.354412078857422
training step: 48889, total_loss: 1.1598858833312988
training step: 48890, total_loss: 0.43675363063812256
training step: 48891, total_loss: 1.95412278175354
training step: 48892, total_loss: 2.2835376262664795
training step: 48893, total_loss: 1.4077701568603516
training step: 48894, total_loss: 1.9592841863632202
training step: 48895, total_loss: 5.420475959777832
training step: 48896, total_loss: 4.689458847045898
training step: 48897, total_loss: 1.5634582042694092
training step: 48898, total_loss: 1.544769287109375
training step: 48899, total_loss: 1.4840309619903564
training step: 48900, total_loss: 0.4506509304046631
training step: 48901, total_loss: 4.101205825805664
training step: 48902, total_loss: 1.1722983121871948
training step: 48903, total_loss: 2.2497127056121826
training step: 48904, total_loss: 0.15693630278110504
training step: 48905, total_loss: 2.0684595108032227
training step: 48906, total_loss: 0.5306986570358276
training step: 48907, total_loss: 1.07745361328125
training step: 48908, total_loss: 2.1450753211975098
training step: 48909, total_loss: 1.0983966588974
training step: 48910, total_loss: 1.3063993453979492
training step: 48911, total_loss: 2.13956880569458
training step: 48912, total_loss: 2.2078921794891357
training step: 48913, total_loss: 1.8033891916275024
training step: 48914, total_loss: 1.2267544269561768
training step: 48915, total_loss: 1.3035433292388916
training step: 48916, total_loss: 0.04513487219810486
training step: 48917, total_loss: 1.9941104650497437
training step: 48918, total_loss: 0.4139242470264435
training step: 48919, total_loss: 0.14329905807971954
training step: 48920, total_loss: 2.0112464427948
training step: 48921, total_loss: 1.7239288091659546
training step: 48922, total_loss: 4.153567790985107
training step: 48923, total_loss: 2.9819188117980957
training step: 48924, total_loss: 1.5279587507247925
training step: 48925, total_loss: 1.634645938873291
training step: 48926, total_loss: 2.50899076461792
training step: 48927, total_loss: 2.1055455207824707
training step: 48928, total_loss: 1.409645438194275
training step: 48929, total_loss: 0.04103619232773781
training step: 48930, total_loss: 0.5732154846191406
training step: 48931, total_loss: 1.7112846374511719
training step: 48932, total_loss: 0.023142918944358826
training step: 48933, total_loss: 0.3385483920574188
training step: 48934, total_loss: 3.9622371196746826
training step: 48935, total_loss: 1.351909875869751
training step: 48936, total_loss: 0.8930724859237671
training step: 48937, total_loss: 1.309432864189148
training step: 48938, total_loss: 0.7153266668319702
training step: 48939, total_loss: 2.5166919231414795
training step: 48940, total_loss: 3.533102035522461
training step: 48941, total_loss: 0.28375983238220215
training step: 48942, total_loss: 2.4783525466918945
training step: 48943, total_loss: 0.18397946655750275
training step: 48944, total_loss: 3.452242374420166
training step: 48945, total_loss: 4.245977401733398
training step: 48946, total_loss: 0.006508358288556337
training step: 48947, total_loss: 3.02359676361084
training step: 48948, total_loss: 1.8283576965332031
training step: 48949, total_loss: 1.0643597841262817
training step: 48950, total_loss: 1.7072572708129883
training step: 48951, total_loss: 0.4701930284500122
training step: 48952, total_loss: 1.4488141536712646
training step: 48953, total_loss: 0.07485351711511612
training step: 48954, total_loss: 3.329346179962158
training step: 48955, total_loss: 1.1646867990493774
training step: 48956, total_loss: 1.9205973148345947
training step: 48957, total_loss: 2.010728597640991
training step: 48958, total_loss: 1.2058725357055664
training step: 48959, total_loss: 1.9525930881500244
training step: 48960, total_loss: 0.42311182618141174
training step: 48961, total_loss: 0.009861730970442295
training step: 48962, total_loss: 1.5912859439849854
training step: 48963, total_loss: 3.8913707733154297
training step: 48964, total_loss: 1.889732837677002
training step: 48965, total_loss: 0.5257972478866577
training step: 48966, total_loss: 1.057714819908142
training step: 48967, total_loss: 5.282186508178711
training step: 48968, total_loss: 1.1407852172851562
training step: 48969, total_loss: 1.87876558303833
training step: 48970, total_loss: 2.662428855895996
training step: 48971, total_loss: 2.8817667961120605
training step: 48972, total_loss: 3.7234549522399902
training step: 48973, total_loss: 0.385932981967926
training step: 48974, total_loss: 2.6532015800476074
training step: 48975, total_loss: 2.7427926063537598
training step: 48976, total_loss: 1.8259567022323608
training step: 48977, total_loss: 2.986238956451416
training step: 48978, total_loss: 1.6262786388397217
training step: 48979, total_loss: 2.3005967140197754
training step: 48980, total_loss: 2.434194326400757
training step: 48981, total_loss: 0.997887909412384
training step: 48982, total_loss: 2.9400627613067627
training step: 48983, total_loss: 0.9431003332138062
training step: 48984, total_loss: 2.2948389053344727
training step: 48985, total_loss: 0.8826289176940918
training step: 48986, total_loss: 0.6993491649627686
training step: 48987, total_loss: 2.243846893310547
training step: 48988, total_loss: 2.6623196601867676INFO:tensorflow:Writing predictions to: test_output/predictions_49000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_49000.json

training step: 48989, total_loss: 2.0799214839935303
training step: 48990, total_loss: 0.22145837545394897
training step: 48991, total_loss: 1.329110026359558
training step: 48992, total_loss: 2.8844547271728516
training step: 48993, total_loss: 2.674698829650879
training step: 48994, total_loss: 2.2609825134277344
training step: 48995, total_loss: 1.1918606758117676
training step: 48996, total_loss: 3.851792335510254
training step: 48997, total_loss: 0.11246315389871597
training step: 48998, total_loss: 3.3143067359924316
training step: 48999, total_loss: 0.7052381038665771
training step: 49000, total_loss: 2.7295336723327637
epoch finished! shuffle=False
evaluation: 13000, total_loss: 1.7799986600875854, f1: 52.41031038600884, followup: 21.45139205842081, yesno: 77.66621025406968, heq: 46.9952837364978, dheq: 2.6

Model saved in path test_output//model_49000.ckpt
training step: 49001, total_loss: 2.971392869949341
training step: 49002, total_loss: 0.975662112236023
training step: 49003, total_loss: 0.06727652251720428
training step: 49004, total_loss: 0.34708279371261597
training step: 49005, total_loss: 1.4905294179916382
training step: 49006, total_loss: 0.8925842046737671
training step: 49007, total_loss: 1.0473524332046509
training step: 49008, total_loss: 2.9784598350524902
training step: 49009, total_loss: 2.3898353576660156
training step: 49010, total_loss: 1.8904824256896973
training step: 49011, total_loss: 0.7481357455253601
training step: 49012, total_loss: 1.9773541688919067
training step: 49013, total_loss: 2.1956448554992676
training step: 49014, total_loss: 3.4894347190856934
training step: 49015, total_loss: 2.678528308868408
training step: 49016, total_loss: 3.6883182525634766
training step: 49017, total_loss: 1.2480095624923706
training step: 49018, total_loss: 2.904235601425171
training step: 49019, total_loss: 1.831989049911499
training step: 49020, total_loss: 0.6273090839385986
training step: 49021, total_loss: 1.2883702516555786
training step: 49022, total_loss: 3.1774024963378906
training step: 49023, total_loss: 1.0162485837936401
training step: 49024, total_loss: 0.42654430866241455
training step: 49025, total_loss: 2.5492210388183594
training step: 49026, total_loss: 1.8180127143859863
training step: 49027, total_loss: 0.09131579846143723
training step: 49028, total_loss: 0.6382502913475037
training step: 49029, total_loss: 1.0367969274520874
training step: 49030, total_loss: 0.6385056972503662
training step: 49031, total_loss: 2.800544500350952
training step: 49032, total_loss: 0.6319758892059326
training step: 49033, total_loss: 1.3677507638931274
training step: 49034, total_loss: 1.9435120820999146
training step: 49035, total_loss: 0.09729956090450287
training step: 49036, total_loss: 0.0065844701603055
training step: 49037, total_loss: 1.5147411823272705
training step: 49038, total_loss: 1.3126957416534424
training step: 49039, total_loss: 0.6404128074645996
training step: 49040, total_loss: 1.0912296772003174
training step: 49041, total_loss: 2.313015937805176
training step: 49042, total_loss: 1.9462995529174805
training step: 49043, total_loss: 0.9436016082763672
training step: 49044, total_loss: 1.2549943923950195
training step: 49045, total_loss: 0.8184751868247986
training step: 49046, total_loss: 0.8360342979431152
training step: 49047, total_loss: 0.495972216129303
training step: 49048, total_loss: 0.5720476508140564
training step: 49049, total_loss: 0.63321852684021
training step: 49050, total_loss: 0.9278411865234375
training step: 49051, total_loss: 1.803116798400879
training step: 49052, total_loss: 1.0323113203048706
training step: 49053, total_loss: 0.9081913232803345
training step: 49054, total_loss: 1.0711091756820679
training step: 49055, total_loss: 3.0566701889038086
training step: 49056, total_loss: 1.8006370067596436
training step: 49057, total_loss: 1.5344589948654175
training step: 49058, total_loss: 1.4431707859039307
training step: 49059, total_loss: 0.06643450260162354
training step: 49060, total_loss: 3.410668134689331
training step: 49061, total_loss: 1.203336238861084
training step: 49062, total_loss: 0.49863678216934204
training step: 49063, total_loss: 1.2723512649536133
training step: 49064, total_loss: 4.058326721191406
training step: 49065, total_loss: 0.10414908826351166
training step: 49066, total_loss: 1.9209539890289307
training step: 49067, total_loss: 0.0017823127564042807
training step: 49068, total_loss: 0.8042558431625366
training step: 49069, total_loss: 0.7419615983963013
training step: 49070, total_loss: 0.5139060020446777
training step: 49071, total_loss: 2.018113374710083
training step: 49072, total_loss: 2.339722156524658
training step: 49073, total_loss: 2.9584193229675293
training step: 49074, total_loss: 2.3921761512756348
training step: 49075, total_loss: 1.2950413227081299
training step: 49076, total_loss: 1.379766821861267
training step: 49077, total_loss: 0.0131932832300663
training step: 49078, total_loss: 0.2679670751094818
training step: 49079, total_loss: 1.3104342222213745
training step: 49080, total_loss: 1.461201548576355
training step: 49081, total_loss: 2.533384323120117
training step: 49082, total_loss: 1.2949790954589844
training step: 49083, total_loss: 1.3497912883758545
training step: 49084, total_loss: 3.2646946907043457
training step: 49085, total_loss: 1.7369943857192993
training step: 49086, total_loss: 1.1016850471496582
training step: 49087, total_loss: 1.8287591934204102
training step: 49088, total_loss: 2.0423178672790527
training step: 49089, total_loss: 0.05145150423049927
training step: 49090, total_loss: 3.3775830268859863
training step: 49091, total_loss: 0.07940643280744553
training step: 49092, total_loss: 2.3557581901550293
training step: 49093, total_loss: 2.20532488822937
training step: 49094, total_loss: 0.040394943207502365
training step: 49095, total_loss: 0.31334665417671204
training step: 49096, total_loss: 1.7283992767333984
training step: 49097, total_loss: 0.0030312188901007175
training step: 49098, total_loss: 3.6563491821289062
training step: 49099, total_loss: 0.5690239667892456
training step: 49100, total_loss: 2.274029016494751
training step: 49101, total_loss: 1.0801455974578857
training step: 49102, total_loss: 1.6417286396026611
training step: 49103, total_loss: 0.1890106201171875
training step: 49104, total_loss: 1.9565523862838745
training step: 49105, total_loss: 1.8267408609390259
training step: 49106, total_loss: 2.8644349575042725
training step: 49107, total_loss: 0.008344775065779686
training step: 49108, total_loss: 0.13446608185768127
training step: 49109, total_loss: 5.076291084289551
training step: 49110, total_loss: 0.013640605844557285
training step: 49111, total_loss: 1.020254373550415
training step: 49112, total_loss: 3.3211193084716797
training step: 49113, total_loss: 1.1144182682037354
training step: 49114, total_loss: 1.7792081832885742
training step: 49115, total_loss: 5.478862762451172
training step: 49116, total_loss: 0.975184440612793
training step: 49117, total_loss: 0.7845698595046997
training step: 49118, total_loss: 0.433960884809494
training step: 49119, total_loss: 2.5841422080993652
training step: 49120, total_loss: 0.5607645511627197
training step: 49121, total_loss: 0.6435452699661255
training step: 49122, total_loss: 2.069056987762451
training step: 49123, total_loss: 2.6801533699035645
training step: 49124, total_loss: 3.298123359680176
training step: 49125, total_loss: 1.032860279083252
training step: 49126, total_loss: 1.9198462963104248
training step: 49127, total_loss: 1.5912959575653076
training step: 49128, total_loss: 1.9552611112594604
training step: 49129, total_loss: 2.1689236164093018
training step: 49130, total_loss: 0.28558439016342163
training step: 49131, total_loss: 0.21792498230934143
training step: 49132, total_loss: 1.7468655109405518
training step: 49133, total_loss: 2.3343021869659424
training step: 49134, total_loss: 0.9010889530181885
training step: 49135, total_loss: 0.010151827707886696
training step: 49136, total_loss: 1.1229608058929443
training step: 49137, total_loss: 0.2608088552951813
training step: 49138, total_loss: 5.269279479980469
training step: 49139, total_loss: 5.291913032531738
training step: 49140, total_loss: 0.25945591926574707
training step: 49141, total_loss: 1.3126527070999146
training step: 49142, total_loss: 2.558877468109131
training step: 49143, total_loss: 0.0028183513786643744
training step: 49144, total_loss: 2.3923444747924805
training step: 49145, total_loss: 0.4676487445831299
training step: 49146, total_loss: 1.939047932624817
training step: 49147, total_loss: 1.7528138160705566
training step: 49148, total_loss: 2.6433372497558594
training step: 49149, total_loss: 2.7363052368164062
training step: 49150, total_loss: 1.0693285465240479
training step: 49151, total_loss: 1.222435474395752
training step: 49152, total_loss: 1.391103982925415
training step: 49153, total_loss: 2.916254758834839
training step: 49154, total_loss: 1.9096922874450684
training step: 49155, total_loss: 1.0659077167510986
training step: 49156, total_loss: 1.2664551734924316
training step: 49157, total_loss: 0.9939318895339966
training step: 49158, total_loss: 0.33559370040893555
training step: 49159, total_loss: 2.8116416931152344
training step: 49160, total_loss: 2.055948257446289
training step: 49161, total_loss: 1.515048861503601
training step: 49162, total_loss: 0.396639883518219
training step: 49163, total_loss: 0.38443827629089355
training step: 49164, total_loss: 2.4768195152282715
training step: 49165, total_loss: 0.15634891390800476
training step: 49166, total_loss: 2.6985392570495605
training step: 49167, total_loss: 1.5038259029388428
training step: 49168, total_loss: 1.9822516441345215
training step: 49169, total_loss: 2.5688018798828125
training step: 49170, total_loss: 0.7169078588485718
training step: 49171, total_loss: 1.7087339162826538
training step: 49172, total_loss: 1.0682604312896729
training step: 49173, total_loss: 2.105576992034912
training step: 49174, total_loss: 1.0548032522201538
training step: 49175, total_loss: 0.5368043184280396
training step: 49176, total_loss: 0.21485307812690735
training step: 49177, total_loss: 2.194697380065918
training step: 49178, total_loss: 1.7097411155700684
training step: 49179, total_loss: 1.593509316444397
training step: 49180, total_loss: 1.8727293014526367
training step: 49181, total_loss: 1.73219895362854
training step: 49182, total_loss: 1.943501591682434
training step: 49183, total_loss: 1.709037184715271
training step: 49184, total_loss: 1.7698588371276855
training step: 49185, total_loss: 0.8158981800079346
training step: 49186, total_loss: 1.1359200477600098
training step: 49187, total_loss: 0.8422467708587646
training step: 49188, total_loss: 3.083364963531494
training step: 49189, total_loss: 1.2831135988235474
training step: 49190, total_loss: 1.0320916175842285
training step: 49191, total_loss: 1.8187838792800903
training step: 49192, total_loss: 1.9580695629119873
training step: 49193, total_loss: 1.241334080696106
training step: 49194, total_loss: 0.02230186201632023
training step: 49195, total_loss: 1.364736795425415
training step: 49196, total_loss: 3.1199839115142822
training step: 49197, total_loss: 3.516549825668335
training step: 49198, total_loss: 1.014575481414795
training step: 49199, total_loss: 4.836209774017334
training step: 49200, total_loss: 1.5836095809936523
training step: 49201, total_loss: 1.965409517288208
training step: 49202, total_loss: 2.2224743366241455
training step: 49203, total_loss: 2.2545433044433594
training step: 49204, total_loss: 1.4708623886108398
training step: 49205, total_loss: 1.8719000816345215
training step: 49206, total_loss: 0.8933275938034058
training step: 49207, total_loss: 5.03499174118042
training step: 49208, total_loss: 2.184053421020508
training step: 49209, total_loss: 2.9735963344573975
training step: 49210, total_loss: 1.3238317966461182
training step: 49211, total_loss: 1.3630563020706177
training step: 49212, total_loss: 0.6521826982498169
training step: 49213, total_loss: 0.967205286026001
training step: 49214, total_loss: 0.03334221988916397
training step: 49215, total_loss: 3.2156996726989746
training step: 49216, total_loss: 0.26628202199935913
training step: 49217, total_loss: 1.5968332290649414
training step: 49218, total_loss: 1.0852229595184326
training step: 49219, total_loss: 2.82147479057312
training step: 49220, total_loss: 4.622355937957764
training step: 49221, total_loss: 2.38360595703125
training step: 49222, total_loss: 1.9424610137939453
training step: 49223, total_loss: 1.8898731470108032
training step: 49224, total_loss: 0.6750489473342896
training step: 49225, total_loss: 0.03426077216863632
training step: 49226, total_loss: 1.6823861598968506
training step: 49227, total_loss: 4.1731109619140625
training step: 49228, total_loss: 1.8320971727371216
training step: 49229, total_loss: 2.4348607063293457
training step: 49230, total_loss: 0.6881456971168518
training step: 49231, total_loss: 2.5246975421905518
training step: 49232, total_loss: 1.5390214920043945
training step: 49233, total_loss: 0.00023201253497973084
training step: 49234, total_loss: 4.273541450500488
training step: 49235, total_loss: 0.8527905344963074
training step: 49236, total_loss: 0.979788064956665
training step: 49237, total_loss: 0.3847683072090149
training step: 49238, total_loss: 0.600142776966095
training step: 49239, total_loss: 0.6227540969848633
training step: 49240, total_loss: 3.2962822914123535
training step: 49241, total_loss: 1.3369513750076294
training step: 49242, total_loss: 0.4545938968658447
training step: 49243, total_loss: 2.040825366973877
training step: 49244, total_loss: 0.00019864128262270242
training step: 49245, total_loss: 0.026426708325743675
training step: 49246, total_loss: 1.445810317993164
training step: 49247, total_loss: 1.5182278156280518
training step: 49248, total_loss: 2.1056931018829346
training step: 49249, total_loss: 2.100754976272583
training step: 49250, total_loss: 1.1210947036743164
training step: 49251, total_loss: 1.1216737031936646
training step: 49252, total_loss: 1.7391846179962158
training step: 49253, total_loss: 3.1772799491882324
training step: 49254, total_loss: 2.491936445236206
training step: 49255, total_loss: 1.4346551895141602
training step: 49256, total_loss: 3.1628007888793945
training step: 49257, total_loss: 3.4449191093444824
training step: 49258, total_loss: 1.0613739490509033
training step: 49259, total_loss: 2.4144058227539062
training step: 49260, total_loss: 3.0282490253448486
training step: 49261, total_loss: 0.287772536277771
training step: 49262, total_loss: 0.5906779170036316
training step: 49263, total_loss: 0.8090217113494873
training step: 49264, total_loss: 0.019719891250133514
training step: 49265, total_loss: 0.17748737335205078
training step: 49266, total_loss: 2.5676183700561523
training step: 49267, total_loss: 1.9000135660171509
training step: 49268, total_loss: 0.10610431432723999
training step: 49269, total_loss: 1.592977523803711
training step: 49270, total_loss: 2.4228243827819824
training step: 49271, total_loss: 1.2543491125106812
training step: 49272, total_loss: 7.363771438598633
training step: 49273, total_loss: 0.7876638174057007
training step: 49274, total_loss: 0.10389149188995361
training step: 49275, total_loss: 1.2211805582046509
training step: 49276, total_loss: 1.0766444206237793
training step: 49277, total_loss: 1.7848516702651978
training step: 49278, total_loss: 0.38249826431274414
training step: 49279, total_loss: 1.4656785726547241
training step: 49280, total_loss: 1.0314338207244873
training step: 49281, total_loss: 0.26660585403442383
training step: 49282, total_loss: 0.6298830509185791
training step: 49283, total_loss: 0.1644863784313202
training step: 49284, total_loss: 0.5203595757484436
training step: 49285, total_loss: 3.4203920364379883
training step: 49286, total_loss: 5.438298225402832
training step: 49287, total_loss: 0.9978720545768738
training step: 49288, total_loss: 4.690639019012451
training step: 49289, total_loss: 2.1116957664489746
training step: 49290, total_loss: 2.2873611450195312
training step: 49291, total_loss: 0.10141162574291229
training step: 49292, total_loss: 0.06196651607751846
training step: 49293, total_loss: 0.7110658884048462
training step: 49294, total_loss: 1.9460252523422241
training step: 49295, total_loss: 0.19082164764404297
training step: 49296, total_loss: 0.8384593725204468
training step: 49297, total_loss: 0.9961235523223877
training step: 49298, total_loss: 3.4120140075683594
training step: 49299, total_loss: 3.4092674255371094
training step: 49300, total_loss: 2.0119102001190186
training step: 49301, total_loss: 0.7126972675323486
training step: 49302, total_loss: 2.1874470710754395
training step: 49303, total_loss: 2.674358367919922
training step: 49304, total_loss: 1.2841771841049194
training step: 49305, total_loss: 0.31305134296417236
training step: 49306, total_loss: 1.432814359664917
training step: 49307, total_loss: 2.3923864364624023
training step: 49308, total_loss: 1.6684776544570923
training step: 49309, total_loss: 1.24955415725708
training step: 49310, total_loss: 1.7135090827941895
training step: 49311, total_loss: 4.772652626037598
training step: 49312, total_loss: 0.4475851058959961
training step: 49313, total_loss: 1.8454861640930176
training step: 49314, total_loss: 1.5299094915390015
training step: 49315, total_loss: 2.188403606414795
training step: 49316, total_loss: 1.0818865299224854
training step: 49317, total_loss: 1.0904706716537476
training step: 49318, total_loss: 0.00031740355188958347
training step: 49319, total_loss: 2.0067057609558105
training step: 49320, total_loss: 1.9837408065795898
training step: 49321, total_loss: 2.643925666809082
training step: 49322, total_loss: 1.3534337282180786
training step: 49323, total_loss: 3.937148094177246
training step: 49324, total_loss: 3.8045835494995117
training step: 49325, total_loss: 3.384809732437134
training step: 49326, total_loss: 0.5068612694740295
training step: 49327, total_loss: 0.1707831174135208
training step: 49328, total_loss: 1.5306570529937744
training step: 49329, total_loss: 1.6946419477462769
training step: 49330, total_loss: 0.24897609651088715
training step: 49331, total_loss: 1.339811086654663
training step: 49332, total_loss: 2.231945037841797
training step: 49333, total_loss: 3.108712673187256
training step: 49334, total_loss: 3.7248785495758057
training step: 49335, total_loss: 1.4690420627593994
training step: 49336, total_loss: 0.003214264055714011
training step: 49337, total_loss: 0.25780534744262695
training step: 49338, total_loss: 0.8761550188064575
training step: 49339, total_loss: 1.454967975616455
training step: 49340, total_loss: 0.36212512850761414
training step: 49341, total_loss: 0.3010891377925873
training step: 49342, total_loss: 0.6443604230880737
training step: 49343, total_loss: 3.262125253677368
training step: 49344, total_loss: 1.374040126800537
training step: 49345, total_loss: 1.804527759552002
training step: 49346, total_loss: 2.3439290523529053
training step: 49347, total_loss: 0.7635223269462585
training step: 49348, total_loss: 2.680591583251953
training step: 49349, total_loss: 2.1710610389709473
training step: 49350, total_loss: 0.07517306506633759
training step: 49351, total_loss: 2.06013822555542
training step: 49352, total_loss: 0.47169196605682373
training step: 49353, total_loss: 3.282938241958618
training step: 49354, total_loss: 5.233946800231934
training step: 49355, total_loss: 0.9174155592918396
training step: 49356, total_loss: 2.710818290710449
training step: 49357, total_loss: 0.5901883244514465
training step: 49358, total_loss: 1.9838833808898926
training step: 49359, total_loss: 0.0901113748550415
training step: 49360, total_loss: 3.784931182861328
training step: 49361, total_loss: 0.8756290078163147
training step: 49362, total_loss: 2.5895724296569824
training step: 49363, total_loss: 2.6643810272216797
training step: 49364, total_loss: 2.528472423553467
training step: 49365, total_loss: 3.758927583694458
training step: 49366, total_loss: 2.612337112426758
training step: 49367, total_loss: 3.157045841217041
training step: 49368, total_loss: 0.3393106162548065
training step: 49369, total_loss: 0.527552604675293
training step: 49370, total_loss: 0.24542668461799622
training step: 49371, total_loss: 2.1819772720336914
training step: 49372, total_loss: 0.025743698701262474
training step: 49373, total_loss: 1.067697525024414
training step: 49374, total_loss: 4.260412693023682
training step: 49375, total_loss: 1.743176817893982
training step: 49376, total_loss: 1.2413939237594604
training step: 49377, total_loss: 2.0160582065582275
training step: 49378, total_loss: 2.387744665145874
training step: 49379, total_loss: 3.0552735328674316
training step: 49380, total_loss: 1.5553221702575684
training step: 49381, total_loss: 0.7551800012588501
training step: 49382, total_loss: 2.0736172199249268
training step: 49383, total_loss: 0.8445711135864258
training step: 49384, total_loss: 2.169407367706299
training step: 49385, total_loss: 0.0003602875513024628
training step: 49386, total_loss: 3.335206985473633
training step: 49387, total_loss: 2.013382911682129
training step: 49388, total_loss: 1.0768604278564453
training step: 49389, total_loss: 1.851980447769165
training step: 49390, total_loss: 0.4446585774421692
training step: 49391, total_loss: 2.0237839221954346
training step: 49392, total_loss: 1.3125962018966675
training step: 49393, total_loss: 2.5234451293945312
training step: 49394, total_loss: 3.820662021636963
training step: 49395, total_loss: 3.338146209716797
training step: 49396, total_loss: 2.0307633876800537
training step: 49397, total_loss: 0.37783145904541016
training step: 49398, total_loss: 2.8301916122436523
training step: 49399, total_loss: 0.1857374608516693
training step: 49400, total_loss: 2.5836379528045654
training step: 49401, total_loss: 1.2920057773590088
training step: 49402, total_loss: 0.45743006467819214
training step: 49403, total_loss: 0.07046616077423096
training step: 49404, total_loss: 4.25773811340332
training step: 49405, total_loss: 0.925918698310852
training step: 49406, total_loss: 1.5193558931350708
training step: 49407, total_loss: 1.681333065032959
training step: 49408, total_loss: 4.650676727294922
training step: 49409, total_loss: 2.4478249549865723
training step: 49410, total_loss: 0.3796781301498413
training step: 49411, total_loss: 1.9453420639038086
training step: 49412, total_loss: 0.18862347304821014
training step: 49413, total_loss: 3.341982364654541
training step: 49414, total_loss: 1.0549836158752441
training step: 49415, total_loss: 2.1987743377685547
training step: 49416, total_loss: 1.7059099674224854
training step: 49417, total_loss: 1.3351054191589355
training step: 49418, total_loss: 2.506868839263916
training step: 49419, total_loss: 1.0917247533798218
training step: 49420, total_loss: 0.7302584052085876
training step: 49421, total_loss: 2.2527518272399902
training step: 49422, total_loss: 2.160696029663086
training step: 49423, total_loss: 1.8780361413955688
training step: 49424, total_loss: 0.8221338987350464
training step: 49425, total_loss: 2.9974334239959717
training step: 49426, total_loss: 2.3041749000549316
training step: 49427, total_loss: 2.5326194763183594
training step: 49428, total_loss: 1.0106364488601685
training step: 49429, total_loss: 0.896732747554779
training step: 49430, total_loss: 1.3258981704711914
training step: 49431, total_loss: 1.6880693435668945
training step: 49432, total_loss: 1.009046196937561
training step: 49433, total_loss: 2.0286056995391846
training step: 49434, total_loss: 0.7793909907341003
training step: 49435, total_loss: 0.012200668454170227
training step: 49436, total_loss: 1.9270453453063965
training step: 49437, total_loss: 2.3340768814086914
training step: 49438, total_loss: 0.7165096998214722
training step: 49439, total_loss: 3.191950798034668
training step: 49440, total_loss: 2.0743420124053955
training step: 49441, total_loss: 5.366673469543457
training step: 49442, total_loss: 1.1337660551071167
training step: 49443, total_loss: 2.7399473190307617
training step: 49444, total_loss: 1.7858877182006836
training step: 49445, total_loss: 4.966858863830566
training step: 49446, total_loss: 2.6799635887145996
training step: 49447, total_loss: 0.6161607503890991
training step: 49448, total_loss: 1.7015588283538818
training step: 49449, total_loss: 3.063962459564209
training step: 49450, total_loss: 9.011811926029623e-05
training step: 49451, total_loss: 0.05651118606328964
training step: 49452, total_loss: 0.7599918246269226
training step: 49453, total_loss: 3.582843780517578
training step: 49454, total_loss: 0.7849805951118469
training step: 49455, total_loss: 4.6198039054870605
training step: 49456, total_loss: 1.3159725666046143
training step: 49457, total_loss: 1.6848368644714355
training step: 49458, total_loss: 2.950369358062744
training step: 49459, total_loss: 2.258859157562256
training step: 49460, total_loss: 0.2675599455833435
training step: 49461, total_loss: 0.37287354469299316
training step: 49462, total_loss: 0.7151845693588257
training step: 49463, total_loss: 2.0433850288391113
training step: 49464, total_loss: 2.624767780303955
training step: 49465, total_loss: 0.6286346912384033
training step: 49466, total_loss: 0.08433099836111069
training step: 49467, total_loss: 1.6314765214920044
training step: 49468, total_loss: 5.902164459228516
training step: 49469, total_loss: 1.1034648418426514
training step: 49470, total_loss: 2.8716225624084473
training step: 49471, total_loss: 2.8368921279907227
training step: 49472, total_loss: 0.2936052680015564
training step: 49473, total_loss: 0.6403824090957642
training step: 49474, total_loss: 1.055454134941101
training step: 49475, total_loss: 1.580143928527832
training step: 49476, total_loss: 1.277726650238037
training step: 49477, total_loss: 0.4661514461040497
training step: 49478, total_loss: 1.9948006868362427
training step: 49479, total_loss: 0.9247462153434753
training step: 49480, total_loss: 0.5008385181427002
training step: 49481, total_loss: 3.2511305809020996
training step: 49482, total_loss: 1.2362934350967407
training step: 49483, total_loss: 2.083375930786133
training step: 49484, total_loss: 1.6649250984191895
training step: 49485, total_loss: 0.07591202855110168
training step: 49486, total_loss: 1.8056466579437256
training step: 49487, total_loss: 4.9563093185424805
training step: 49488, total_loss: 1.8405320644378662
training step: 49489, total_loss: 2.312943696975708
training step: 49490, total_loss: 0.8555814027786255
training step: 49491, total_loss: 2.585573196411133
training step: 49492, total_loss: 2.2678122520446777
training step: 49493, total_loss: 5.692026138305664
training step: 49494, total_loss: 3.27215313911438
training step: 49495, total_loss: 0.9481180906295776
training step: 49496, total_loss: 3.1501388549804688
training step: 49497, total_loss: 0.08499646186828613
training step: 49498, total_loss: 0.019142791628837585
training step: 49499, total_loss: 0.4715369939804077
training step: 49500, total_loss: 0.05072890967130661
training step: 49501, total_loss: 0.8973967432975769
training step: 49502, total_loss: 0.7728132009506226
training step: 49503, total_loss: 2.69364595413208
training step: 49504, total_loss: 3.5892605781555176
training step: 49505, total_loss: 0.9103810787200928
training step: 49506, total_loss: 1.6779104471206665
training step: 49507, total_loss: 1.2138317823410034
training step: 49508, total_loss: 0.8942314386367798
training step: 49509, total_loss: 0.22706663608551025
training step: 49510, total_loss: 0.047275565564632416
training step: 49511, total_loss: 0.16508691012859344
training step: 49512, total_loss: 0.2061317414045334
training step: 49513, total_loss: 2.9420411586761475
training step: 49514, total_loss: 2.287428140640259
training step: 49515, total_loss: 0.14880099892616272
training step: 49516, total_loss: 3.392324924468994
training step: 49517, total_loss: 0.6667554378509521
training step: 49518, total_loss: 0.31139105558395386
training step: 49519, total_loss: 7.835201263427734
training step: 49520, total_loss: 1.5144907236099243
training step: 49521, total_loss: 1.450575590133667
training step: 49522, total_loss: 0.8883259296417236
training step: 49523, total_loss: 1.2943060398101807
training step: 49524, total_loss: 0.054075948894023895
training step: 49525, total_loss: 1.5084645748138428
training step: 49526, total_loss: 1.2593718767166138
training step: 49527, total_loss: 2.6038265228271484
training step: 49528, total_loss: 0.22569581866264343
training step: 49529, total_loss: 4.0447893142700195
training step: 49530, total_loss: 2.349336624145508
training step: 49531, total_loss: 3.67187237739563
training step: 49532, total_loss: 0.02669803611934185
training step: 49533, total_loss: 1.5806418657302856
training step: 49534, total_loss: 1.9033269882202148
training step: 49535, total_loss: 1.4549798965454102
training step: 49536, total_loss: 0.1674952208995819
training step: 49537, total_loss: 4.709019660949707
training step: 49538, total_loss: 0.8517485857009888
training step: 49539, total_loss: 1.3574026823043823
training step: 49540, total_loss: 1.2666674852371216
training step: 49541, total_loss: 2.405730724334717
training step: 49542, total_loss: 0.08164092898368835
training step: 49543, total_loss: 1.4666659832000732
training step: 49544, total_loss: 0.17756283283233643
training step: 49545, total_loss: 1.0193357467651367
training step: 49546, total_loss: 1.567176103591919
training step: 49547, total_loss: 0.09738199412822723
training step: 49548, total_loss: 2.362423896789551
training step: 49549, total_loss: 1.5427712202072144
training step: 49550, total_loss: 0.016469720751047134
training step: 49551, total_loss: 0.9447402954101562
training step: 49552, total_loss: 1.3098877668380737
training step: 49553, total_loss: 4.35710334777832
training step: 49554, total_loss: 1.1846203804016113
training step: 49555, total_loss: 3.043192148208618
training step: 49556, total_loss: 0.15045009553432465
training step: 49557, total_loss: 1.826932430267334
training step: 49558, total_loss: 0.32973259687423706
training step: 49559, total_loss: 0.011707071214914322
training step: 49560, total_loss: 0.05806929990649223
training step: 49561, total_loss: 1.346738576889038
training step: 49562, total_loss: 1.4851644039154053
training step: 49563, total_loss: 3.8858482837677
training step: 49564, total_loss: 1.4381945133209229
training step: 49565, total_loss: 1.6151347160339355
training step: 49566, total_loss: 0.9396255612373352
training step: 49567, total_loss: 5.265432834625244
training step: 49568, total_loss: 1.9497525691986084
training step: 49569, total_loss: 0.1687687635421753
training step: 49570, total_loss: 0.6979471445083618
training step: 49571, total_loss: 1.8529490232467651
training step: 49572, total_loss: 1.0660474300384521
training step: 49573, total_loss: 3.3822216987609863
training step: 49574, total_loss: 0.1798509955406189
training step: 49575, total_loss: 4.096126556396484
training step: 49576, total_loss: 1.9160733222961426
training step: 49577, total_loss: 0.11121010780334473
training step: 49578, total_loss: 1.9060912132263184
training step: 49579, total_loss: 0.438232421875
training step: 49580, total_loss: 2.1126530170440674
training step: 49581, total_loss: 8.630262163933367e-05
training step: 49582, total_loss: 1.9692258834838867
training step: 49583, total_loss: 0.708080530166626
training step: 49584, total_loss: 0.00020322449563536793
training step: 49585, total_loss: 1.3445672988891602
training step: 49586, total_loss: 2.398756742477417
training step: 49587, total_loss: 2.2640514373779297
training step: 49588, total_loss: 1.746612548828125
training step: 49589, total_loss: 2.8670122623443604
training step: 49590, total_loss: 1.082977056503296
training step: 49591, total_loss: 0.42107707262039185
training step: 49592, total_loss: 1.6003129482269287
training step: 49593, total_loss: 1.8267062902450562
training step: 49594, total_loss: 2.6868996620178223
training step: 49595, total_loss: 1.9520001411437988
training step: 49596, total_loss: 0.21698838472366333
training step: 49597, total_loss: 0.1951759159564972
training step: 49598, total_loss: 0.4726259708404541
training step: 49599, total_loss: 1.18156099319458
training step: 49600, total_loss: 6.173045635223389
training step: 49601, total_loss: 1.2431316375732422
training step: 49602, total_loss: 3.073246479034424
training step: 49603, total_loss: 2.142179012298584
training step: 49604, total_loss: 1.0918716192245483
training step: 49605, total_loss: 0.8595532178878784
training step: 49606, total_loss: 0.06513716280460358
training step: 49607, total_loss: 2.434302806854248
training step: 49608, total_loss: 5.486729621887207
training step: 49609, total_loss: 0.19471994042396545
training step: 49610, total_loss: 2.4002373218536377
training step: 49611, total_loss: 1.341559886932373
training step: 49612, total_loss: 2.5136704444885254
training step: 49613, total_loss: 2.8765506744384766
training step: 49614, total_loss: 0.5814731121063232
training step: 49615, total_loss: 3.5156731605529785
training step: 49616, total_loss: 2.238765001296997
training step: 49617, total_loss: 1.6738390922546387
training step: 49618, total_loss: 0.29677727818489075
training step: 49619, total_loss: 1.0847251415252686
training step: 49620, total_loss: 2.2635092735290527
training step: 49621, total_loss: 2.0732085704803467
training step: 49622, total_loss: 0.21965926885604858
training step: 49623, total_loss: 1.1647709608078003
training step: 49624, total_loss: 1.674259901046753
training step: 49625, total_loss: 1.8967266082763672
training step: 49626, total_loss: 0.21643313765525818
training step: 49627, total_loss: 3.0574450492858887
training step: 49628, total_loss: 2.5130209922790527
training step: 49629, total_loss: 3.400219440460205
training step: 49630, total_loss: 0.10815170407295227
training step: 49631, total_loss: 1.5205092430114746
training step: 49632, total_loss: 3.0351266860961914
training step: 49633, total_loss: 3.6916589736938477
training step: 49634, total_loss: 0.6022992134094238
training step: 49635, total_loss: 1.3360013961791992
training step: 49636, total_loss: 1.7400548458099365
training step: 49637, total_loss: 0.09596575796604156
training step: 49638, total_loss: 2.063673496246338
training step: 49639, total_loss: 2.4353983402252197
training step: 49640, total_loss: 1.0112383365631104
training step: 49641, total_loss: 1.513373613357544
training step: 49642, total_loss: 1.2063682079315186
training step: 49643, total_loss: 0.35141462087631226
training step: 49644, total_loss: 0.5153452157974243
training step: 49645, total_loss: 2.577021598815918
training step: 49646, total_loss: 2.1786575317382812
training step: 49647, total_loss: 1.4934301376342773
training step: 49648, total_loss: 0.30036574602127075
training step: 49649, total_loss: 1.6885769367218018
training step: 49650, total_loss: 0.16228795051574707
training step: 49651, total_loss: 0.1823086142539978
training step: 49652, total_loss: 1.9453634023666382
training step: 49653, total_loss: 2.2674906253814697
training step: 49654, total_loss: 1.2620991468429565
training step: 49655, total_loss: 1.215138554573059
training step: 49656, total_loss: 2.665910243988037
training step: 49657, total_loss: 0.8197780847549438
training step: 49658, total_loss: 0.5148717761039734
training step: 49659, total_loss: 0.5616170167922974
training step: 49660, total_loss: 0.230045348405838
training step: 49661, total_loss: 0.0004177226801402867
training step: 49662, total_loss: 1.1360538005828857
training step: 49663, total_loss: 2.0114808082580566
training step: 49664, total_loss: 1.2414963245391846
training step: 49665, total_loss: 2.257171630859375
training step: 49666, total_loss: 0.9646438360214233
training step: 49667, total_loss: 0.12287885695695877
training step: 49668, total_loss: 2.457240581512451
training step: 49669, total_loss: 5.269655704498291
training step: 49670, total_loss: 0.01899237185716629
training step: 49671, total_loss: 1.4617904424667358
training step: 49672, total_loss: 2.200387716293335
training step: 49673, total_loss: 3.6671557426452637
training step: 49674, total_loss: 1.298276662826538
training step: 49675, total_loss: 5.4009623527526855
training step: 49676, total_loss: 2.621577739715576
training step: 49677, total_loss: 1.6374473571777344
training step: 49678, total_loss: 1.9971802234649658
training step: 49679, total_loss: 1.4480617046356201
training step: 49680, total_loss: 1.7107222080230713
training step: 49681, total_loss: 1.819068193435669
training step: 49682, total_loss: 1.771254301071167
training step: 49683, total_loss: 1.0579102039337158
training step: 49684, total_loss: 0.38993075489997864
training step: 49685, total_loss: 2.045596122741699
training step: 49686, total_loss: 0.3813696801662445
training step: 49687, total_loss: 1.9850823879241943
training step: 49688, total_loss: 3.510587215423584
training step: 49689, total_loss: 2.2448678016662598
training step: 49690, total_loss: 2.8563380241394043
training step: 49691, total_loss: 0.6879896521568298
training step: 49692, total_loss: 1.014211654663086
training step: 49693, total_loss: 0.16744230687618256
training step: 49694, total_loss: 0.1368153989315033
training step: 49695, total_loss: 2.066861629486084
training step: 49696, total_loss: 0.9293806552886963
training step: 49697, total_loss: 2.848964214324951
training step: 49698, total_loss: 0.7363553643226624
training step: 49699, total_loss: 0.8090291023254395
training step: 49700, total_loss: 0.7971348762512207
training step: 49701, total_loss: 0.6584482192993164
training step: 49702, total_loss: 2.1839945316314697
training step: 49703, total_loss: 2.0762510299682617
training step: 49704, total_loss: 3.074773073196411
training step: 49705, total_loss: 1.1186633110046387
training step: 49706, total_loss: 1.660881757736206
training step: 49707, total_loss: 2.3389062881469727
training step: 49708, total_loss: 2.2476375102996826
training step: 49709, total_loss: 2.21170711517334
training step: 49710, total_loss: 2.7903828620910645
training step: 49711, total_loss: 0.7456197142601013
training step: 49712, total_loss: 3.738952159881592
training step: 49713, total_loss: 0.05744390934705734
training step: 49714, total_loss: 0.994680643081665
training step: 49715, total_loss: 3.3648500442504883
training step: 49716, total_loss: 0.18792948126792908
training step: 49717, total_loss: 1.7068248987197876
training step: 49718, total_loss: 0.6606696844100952
training step: 49719, total_loss: 3.792623519897461
training step: 49720, total_loss: 0.5614615678787231
training step: 49721, total_loss: 1.4099849462509155
training step: 49722, total_loss: 3.1865248680114746
training step: 49723, total_loss: 2.071580410003662
training step: 49724, total_loss: 0.923843264579773
training step: 49725, total_loss: 3.248490810394287
training step: 49726, total_loss: 0.001851694192737341
training step: 49727, total_loss: 1.6202237606048584
training step: 49728, total_loss: 1.4405714273452759
training step: 49729, total_loss: 3.330595016479492
training step: 49730, total_loss: 2.3136186599731445
training step: 49731, total_loss: 0.33070802688598633
training step: 49732, total_loss: 2.831331253051758
training step: 49733, total_loss: 2.323965072631836
training step: 49734, total_loss: 1.8041977882385254
training step: 49735, total_loss: 4.601417064666748
training step: 49736, total_loss: 0.5362296104431152
training step: 49737, total_loss: 0.5943832397460938
training step: 49738, total_loss: 1.1483420133590698
training step: 49739, total_loss: 1.1039321422576904
training step: 49740, total_loss: 2.912242889404297
training step: 49741, total_loss: 0.8334470987319946
training step: 49742, total_loss: 2.9906907081604004
training step: 49743, total_loss: 1.778371810913086
training step: 49744, total_loss: 1.5004894733428955
training step: 49745, total_loss: 1.092843770980835
training step: 49746, total_loss: 0.7056522965431213
training step: 49747, total_loss: 2.0615768432617188
training step: 49748, total_loss: 3.3274412155151367
training step: 49749, total_loss: 3.2462806701660156
training step: 49750, total_loss: 1.5485656261444092
training step: 49751, total_loss: 0.618028461933136
training step: 49752, total_loss: 2.3116683959960938
training step: 49753, total_loss: 0.6505576968193054
training step: 49754, total_loss: 0.3290966749191284
training step: 49755, total_loss: 0.9624102115631104
training step: 49756, total_loss: 1.575409173965454
training step: 49757, total_loss: 0.21349257230758667
training step: 49758, total_loss: 0.001287803752347827
training step: 49759, total_loss: 2.4027528762817383
training step: 49760, total_loss: 3.107755661010742
training step: 49761, total_loss: 1.5548105239868164
training step: 49762, total_loss: 0.25932571291923523
training step: 49763, total_loss: 2.2494678497314453
training step: 49764, total_loss: 2.8922996520996094
training step: 49765, total_loss: 0.21470919251441956
training step: 49766, total_loss: 0.4301687479019165
training step: 49767, total_loss: 2.0247459411621094
training step: 49768, total_loss: 4.866883277893066
training step: 49769, total_loss: 2.8488337993621826
training step: 49770, total_loss: 1.9028279781341553
training step: 49771, total_loss: 0.9416735172271729
training step: 49772, total_loss: 0.3855552077293396
training step: 49773, total_loss: 4.362764358520508
training step: 49774, total_loss: 0.5380592346191406
training step: 49775, total_loss: 1.9428203105926514
training step: 49776, total_loss: 0.9338570833206177
training step: 49777, total_loss: 2.910600185394287
training step: 49778, total_loss: 1.4319639205932617
training step: 49779, total_loss: 2.5725133419036865
training step: 49780, total_loss: 2.007274866104126
training step: 49781, total_loss: 3.181652307510376
training step: 49782, total_loss: 0.7000395655632019
training step: 49783, total_loss: 2.2679247856140137
training step: 49784, total_loss: 1.9503217935562134
training step: 49785, total_loss: 1.4401438236236572
training step: 49786, total_loss: 0.5428822636604309
training step: 49787, total_loss: 1.5186080932617188
training step: 49788, total_loss: 0.013442007824778557
training step: 49789, total_loss: 4.152299880981445
training step: 49790, total_loss: 0.5445359945297241
training step: 49791, total_loss: 1.2743804454803467
training step: 49792, total_loss: 0.6790738701820374
training step: 49793, total_loss: 1.5883867740631104
training step: 49794, total_loss: 1.2139135599136353
training step: 49795, total_loss: 5.185048580169678
training step: 49796, total_loss: 1.226524829864502
training step: 49797, total_loss: 2.156249523162842
training step: 49798, total_loss: 3.2378745079040527
training step: 49799, total_loss: 2.3610236644744873
training step: 49800, total_loss: 1.309852123260498
training step: 49801, total_loss: 1.432100772857666
training step: 49802, total_loss: 2.7806057929992676
training step: 49803, total_loss: 1.293571949005127
training step: 49804, total_loss: 0.29723674058914185
training step: 49805, total_loss: 0.3027999997138977
training step: 49806, total_loss: 0.9436684846878052
training step: 49807, total_loss: 0.11465078592300415
training step: 49808, total_loss: 1.2760416269302368
training step: 49809, total_loss: 2.545898675918579
training step: 49810, total_loss: 1.4589557647705078
training step: 49811, total_loss: 0.5203070044517517
training step: 49812, total_loss: 1.0340759754180908
training step: 49813, total_loss: 2.5190324783325195
training step: 49814, total_loss: 0.8159283995628357
training step: 49815, total_loss: 1.3952858448028564
training step: 49816, total_loss: 0.41600024700164795
training step: 49817, total_loss: 0.3191395103931427
training step: 49818, total_loss: 4.148184299468994
training step: 49819, total_loss: 2.041790723800659
training step: 49820, total_loss: 0.16131407022476196
training step: 49821, total_loss: 0.28214719891548157
training step: 49822, total_loss: 0.8044506311416626
training step: 49823, total_loss: 0.9032312035560608
training step: 49824, total_loss: 0.6057711839675903
training step: 49825, total_loss: 3.041550636291504
training step: 49826, total_loss: 1.600940465927124
training step: 49827, total_loss: 2.84011173248291
training step: 49828, total_loss: 4.227127552032471
training step: 49829, total_loss: 0.4719051122665405
training step: 49830, total_loss: 1.9084832668304443
training step: 49831, total_loss: 1.1720008850097656
training step: 49832, total_loss: 1.8356389999389648
training step: 49833, total_loss: 3.114908218383789
training step: 49834, total_loss: 4.577995300292969
training step: 49835, total_loss: 1.0864284038543701
training step: 49836, total_loss: 1.7031433582305908
training step: 49837, total_loss: 0.5456452965736389
training step: 49838, total_loss: 2.7336554527282715
training step: 49839, total_loss: 0.33718568086624146
training step: 49840, total_loss: 0.03287344053387642
training step: 49841, total_loss: 1.08803129196167
training step: 49842, total_loss: 1.5703922510147095
training step: 49843, total_loss: 3.4254612922668457
training step: 49844, total_loss: 2.1986799240112305
training step: 49845, total_loss: 2.470097541809082
training step: 49846, total_loss: 2.2688045501708984
training step: 49847, total_loss: 2.6091206073760986
training step: 49848, total_loss: 1.5718899965286255
training step: 49849, total_loss: 0.21063826978206635
training step: 49850, total_loss: 0.24397215247154236
training step: 49851, total_loss: 0.056471846997737885
training step: 49852, total_loss: 0.07998252660036087
training step: 49853, total_loss: 0.728192925453186
training step: 49854, total_loss: 1.7531685829162598
training step: 49855, total_loss: 1.3023066520690918
training step: 49856, total_loss: 4.404627323150635
training step: 49857, total_loss: 3.094703197479248
training step: 49858, total_loss: 0.354644775390625
training step: 49859, total_loss: 0.5375921726226807
training step: 49860, total_loss: 0.8653632402420044
training step: 49861, total_loss: 2.712263584136963
training step: 49862, total_loss: 0.19945506751537323
training step: 49863, total_loss: 4.383780479431152
training step: 49864, total_loss: 0.38042113184928894
training step: 49865, total_loss: 2.61061429977417
training step: 49866, total_loss: 0.0855492651462555
training step: 49867, total_loss: 1.848163366317749
training step: 49868, total_loss: 1.64676833152771
training step: 49869, total_loss: 5.185311794281006
training step: 49870, total_loss: 1.6869096755981445
training step: 49871, total_loss: 0.9225260019302368
training step: 49872, total_loss: 1.2832413911819458
training step: 49873, total_loss: 0.08370648324489594
training step: 49874, total_loss: 0.013525770977139473
training step: 49875, total_loss: 0.44010668992996216
training step: 49876, total_loss: 0.00012885674368590117
training step: 49877, total_loss: 3.0680816173553467
training step: 49878, total_loss: 0.18226277828216553
training step: 49879, total_loss: 0.0014761837664991617
training step: 49880, total_loss: 1.5715054273605347
training step: 49881, total_loss: 0.5408083200454712
training step: 49882, total_loss: 1.1271800994873047
training step: 49883, total_loss: 2.0330114364624023
training step: 49884, total_loss: 2.4370789527893066
training step: 49885, total_loss: 0.6361538767814636
training step: 49886, total_loss: 3.5801055431365967
training step: 49887, total_loss: 1.8405413627624512
training step: 49888, total_loss: 0.8491077423095703
training step: 49889, total_loss: 3.622502326965332
training step: 49890, total_loss: 0.7189072370529175
training step: 49891, total_loss: 0.652045488357544
training step: 49892, total_loss: 2.0152814388275146
training step: 49893, total_loss: 0.2720915675163269
training step: 49894, total_loss: 2.4286556243896484
training step: 49895, total_loss: 1.4883666038513184
training step: 49896, total_loss: 1.5245996713638306
training step: 49897, total_loss: 0.6371073722839355
training step: 49898, total_loss: 0.7724255323410034
training step: 49899, total_loss: 1.202972650527954
training step: 49900, total_loss: 0.5850029587745667
training step: 49901, total_loss: 1.2137720584869385
training step: 49902, total_loss: 2.102686882019043
training step: 49903, total_loss: 2.7466135025024414
training step: 49904, total_loss: 0.16360211372375488
training step: 49905, total_loss: 3.1410555839538574
training step: 49906, total_loss: 0.8693780899047852
training step: 49907, total_loss: 0.9429208040237427
training step: 49908, total_loss: 2.065434694290161
training step: 49909, total_loss: 0.7764873504638672
training step: 49910, total_loss: 1.1180123090744019
training step: 49911, total_loss: 1.7008371353149414
training step: 49912, total_loss: 2.6553587913513184
training step: 49913, total_loss: 0.5096570253372192
training step: 49914, total_loss: 2.2318201065063477
training step: 49915, total_loss: 0.9730913639068604INFO:tensorflow:Writing predictions to: test_output/predictions_50000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_50000.json

training step: 49916, total_loss: 2.1751997470855713
training step: 49917, total_loss: 1.9091761112213135
training step: 49918, total_loss: 2.553772211074829
training step: 49919, total_loss: 2.9979279041290283
training step: 49920, total_loss: 1.4267663955688477
training step: 49921, total_loss: 1.527030348777771
training step: 49922, total_loss: 3.525432586669922
training step: 49923, total_loss: 1.1513986587524414
training step: 49924, total_loss: 0.4291723966598511
training step: 49925, total_loss: 1.25925612449646
training step: 49926, total_loss: 0.015780942514538765
training step: 49927, total_loss: 0.16449007391929626
training step: 49928, total_loss: 1.6062142848968506
training step: 49929, total_loss: 0.2189835011959076
training step: 49930, total_loss: 0.6062291860580444
training step: 49931, total_loss: 1.034964919090271
training step: 49932, total_loss: 1.5929749011993408
training step: 49933, total_loss: 3.6076676845550537
training step: 49934, total_loss: 0.23838874697685242
training step: 49935, total_loss: 2.397390842437744
training step: 49936, total_loss: 2.5210464000701904
training step: 49937, total_loss: 0.009039689786732197
training step: 49938, total_loss: 1.6383610963821411
training step: 49939, total_loss: 1.9842994213104248
training step: 49940, total_loss: 2.1845784187316895
training step: 49941, total_loss: 2.4974379539489746
training step: 49942, total_loss: 0.5399962663650513
training step: 49943, total_loss: 3.10227370262146
training step: 49944, total_loss: 1.768554449081421
training step: 49945, total_loss: 2.805436134338379
training step: 49946, total_loss: 1.6510391235351562
training step: 49947, total_loss: 2.3600480556488037
training step: 49948, total_loss: 2.0735414028167725
training step: 49949, total_loss: 0.575214684009552
training step: 49950, total_loss: 5.296107292175293
training step: 49951, total_loss: 2.6208081245422363
training step: 49952, total_loss: 3.4929370880126953
training step: 49953, total_loss: 1.5285038948059082
training step: 49954, total_loss: 0.6275414824485779
training step: 49955, total_loss: 3.020906448364258
training step: 49956, total_loss: 5.230464935302734
training step: 49957, total_loss: 1.0947985649108887
training step: 49958, total_loss: 1.1121827363967896
training step: 49959, total_loss: 0.4736272096633911
training step: 49960, total_loss: 0.7020407915115356
training step: 49961, total_loss: 0.14143122732639313
training step: 49962, total_loss: 0.29635077714920044
training step: 49963, total_loss: 1.7413008213043213
training step: 49964, total_loss: 3.5705161094665527
training step: 49965, total_loss: 0.7277634739875793
training step: 49966, total_loss: 1.8539446592330933
training step: 49967, total_loss: 0.24068009853363037
training step: 49968, total_loss: 0.3501095175743103
training step: 49969, total_loss: 3.544041156768799
training step: 49970, total_loss: 2.923799991607666
training step: 49971, total_loss: 1.153778076171875
training step: 49972, total_loss: 1.482344627380371
training step: 49973, total_loss: 0.5165151953697205
training step: 49974, total_loss: 2.948307752609253
training step: 49975, total_loss: 0.5311806201934814
training step: 49976, total_loss: 0.024746213108301163
training step: 49977, total_loss: 0.9588279128074646
training step: 49978, total_loss: 2.7309834957122803
training step: 49979, total_loss: 0.08704444766044617
training step: 49980, total_loss: 2.433751106262207
training step: 49981, total_loss: 0.7374987602233887
training step: 49982, total_loss: 3.4219846725463867
training step: 49983, total_loss: 2.0856075286865234
training step: 49984, total_loss: 0.056325219571590424
training step: 49985, total_loss: 0.03637061268091202
training step: 49986, total_loss: 4.4010910987854
training step: 49987, total_loss: 0.9108316898345947
training step: 49988, total_loss: 0.11739419400691986
training step: 49989, total_loss: 1.3322186470031738
training step: 49990, total_loss: 1.5684161186218262
training step: 49991, total_loss: 3.436039686203003
training step: 49992, total_loss: 1.3306787014007568
training step: 49993, total_loss: 3.0485429763793945
training step: 49994, total_loss: 0.7610100507736206
training step: 49995, total_loss: 0.4406791925430298
training step: 49996, total_loss: 3.59733247756958
training step: 49997, total_loss: 2.080052137374878
training step: 49998, total_loss: 4.25211238861084
training step: 49999, total_loss: 0.22966983914375305
training step: 50000, total_loss: 0.4864254295825958
epoch finished! shuffle=False
evaluation: 14000, total_loss: 1.8829320669174194, f1: 55.60300966397784, followup: 26.471930625285257, yesno: 75.91662863228359, heq: 49.80982808458847, dheq: 3.4

Model saved in path test_output//model_50000.ckpt
training step: 50001, total_loss: 0.0635628029704094
training step: 50002, total_loss: 0.39324161410331726
training step: 50003, total_loss: 0.3429638743400574
training step: 50004, total_loss: 4.109709739685059
training step: 50005, total_loss: 0.32357197999954224
training step: 50006, total_loss: 0.05881384760141373
training step: 50007, total_loss: 5.566070079803467
training step: 50008, total_loss: 1.2990416288375854
training step: 50009, total_loss: 1.1535418033599854
training step: 50010, total_loss: 2.2470922470092773
training step: 50011, total_loss: 1.0255866050720215
training step: 50012, total_loss: 3.2981884479522705
training step: 50013, total_loss: 0.08233477920293808
training step: 50014, total_loss: 1.7473552227020264
training step: 50015, total_loss: 1.1617190837860107
training step: 50016, total_loss: 1.9396638870239258
training step: 50017, total_loss: 2.529743194580078
training step: 50018, total_loss: 2.5485167503356934
training step: 50019, total_loss: 0.19037538766860962
training step: 50020, total_loss: 3.499063491821289
training step: 50021, total_loss: 2.345811367034912
training step: 50022, total_loss: 0.0006327360169962049
training step: 50023, total_loss: 2.6271605491638184
training step: 50024, total_loss: 1.1256096363067627
training step: 50025, total_loss: 0.4997157156467438
training step: 50026, total_loss: 2.3355154991149902
training step: 50027, total_loss: 0.5313535928726196
training step: 50028, total_loss: 1.0073390007019043
training step: 50029, total_loss: 1.7549526691436768
training step: 50030, total_loss: 1.3566793203353882
training step: 50031, total_loss: 0.8224354982376099
training step: 50032, total_loss: 0.918536365032196
training step: 50033, total_loss: 2.8843140602111816
training step: 50034, total_loss: 1.0896220207214355
training step: 50035, total_loss: 0.37073370814323425
training step: 50036, total_loss: 2.797436237335205
training step: 50037, total_loss: 1.3674376010894775
training step: 50038, total_loss: 1.5494773387908936
training step: 50039, total_loss: 1.6573381423950195
training step: 50040, total_loss: 0.9324753284454346
training step: 50041, total_loss: 2.5784530639648438
training step: 50042, total_loss: 0.6918051242828369
training step: 50043, total_loss: 0.013625351712107658
training step: 50044, total_loss: 1.4718635082244873
training step: 50045, total_loss: 0.22785848379135132
training step: 50046, total_loss: 2.047473192214966
training step: 50047, total_loss: 0.08893100917339325
training step: 50048, total_loss: 1.6298823356628418
training step: 50049, total_loss: 0.0026118496898561716
training step: 50050, total_loss: 0.03712911158800125
training step: 50051, total_loss: 0.04922188073396683
training step: 50052, total_loss: 0.18031072616577148
training step: 50053, total_loss: 0.3115721344947815
training step: 50054, total_loss: 1.903586506843567
training step: 50055, total_loss: 1.953842282295227
training step: 50056, total_loss: 0.15461385250091553
training step: 50057, total_loss: 0.21357189118862152
training step: 50058, total_loss: 1.8023321628570557
training step: 50059, total_loss: 2.4392459392547607
training step: 50060, total_loss: 0.6727445125579834
training step: 50061, total_loss: 1.883387565612793
training step: 50062, total_loss: 1.9778164625167847
training step: 50063, total_loss: 1.4818508625030518
training step: 50064, total_loss: 0.4767201840877533
training step: 50065, total_loss: 1.0543241500854492
training step: 50066, total_loss: 0.936693012714386
training step: 50067, total_loss: 0.9690974950790405
training step: 50068, total_loss: 3.231106758117676
training step: 50069, total_loss: 3.0330615043640137
training step: 50070, total_loss: 2.575012445449829
training step: 50071, total_loss: 1.7738224267959595
training step: 50072, total_loss: 1.2704150676727295
training step: 50073, total_loss: 2.489750385284424
training step: 50074, total_loss: 3.067312240600586
training step: 50075, total_loss: 3.3251280784606934
training step: 50076, total_loss: 1.0428566932678223
training step: 50077, total_loss: 1.3543360233306885
training step: 50078, total_loss: 0.5814031958580017
training step: 50079, total_loss: 2.3975765705108643
training step: 50080, total_loss: 2.0571823120117188
training step: 50081, total_loss: 3.5120811462402344
training step: 50082, total_loss: 3.7831108570098877
training step: 50083, total_loss: 1.3433843851089478
training step: 50084, total_loss: 4.779932975769043
training step: 50085, total_loss: 2.397146701812744
training step: 50086, total_loss: 0.6826035976409912
training step: 50087, total_loss: 1.4108736515045166
training step: 50088, total_loss: 2.8674306869506836
training step: 50089, total_loss: 0.6865782737731934
training step: 50090, total_loss: 1.826122760772705
training step: 50091, total_loss: 1.3940417766571045
training step: 50092, total_loss: 0.5111417770385742
training step: 50093, total_loss: 1.2809919118881226
training step: 50094, total_loss: 0.7146108150482178
training step: 50095, total_loss: 1.7409167289733887
training step: 50096, total_loss: 0.41162437200546265
training step: 50097, total_loss: 0.13726095855236053
training step: 50098, total_loss: 1.5077919960021973
training step: 50099, total_loss: 0.48124533891677856
training step: 50100, total_loss: 1.3338873386383057
training step: 50101, total_loss: 3.1286354064941406
training step: 50102, total_loss: 2.0872297286987305
training step: 50103, total_loss: 1.006069302558899
training step: 50104, total_loss: 2.534717559814453
training step: 50105, total_loss: 1.3885231018066406
training step: 50106, total_loss: 0.9393038749694824
training step: 50107, total_loss: 1.661665678024292
training step: 50108, total_loss: 1.4362561702728271
training step: 50109, total_loss: 1.5708595514297485
training step: 50110, total_loss: 1.3003754615783691
training step: 50111, total_loss: 2.600639820098877
training step: 50112, total_loss: 0.7077022194862366
training step: 50113, total_loss: 1.6990642547607422
training step: 50114, total_loss: 2.5967819690704346
training step: 50115, total_loss: 2.2966904640197754
training step: 50116, total_loss: 1.8239073753356934
training step: 50117, total_loss: 2.5938100814819336
training step: 50118, total_loss: 0.46094632148742676
training step: 50119, total_loss: 0.008358543738722801
training step: 50120, total_loss: 1.33543062210083
training step: 50121, total_loss: 2.4047555923461914
training step: 50122, total_loss: 1.850614309310913
training step: 50123, total_loss: 2.5332064628601074
training step: 50124, total_loss: 0.9406434297561646
training step: 50125, total_loss: 0.20914749801158905
training step: 50126, total_loss: 2.3385517597198486
training step: 50127, total_loss: 0.2873576879501343
training step: 50128, total_loss: 0.9171885848045349
training step: 50129, total_loss: 0.7564674615859985
training step: 50130, total_loss: 2.058246374130249
training step: 50131, total_loss: 1.1758637428283691
training step: 50132, total_loss: 0.786565899848938
training step: 50133, total_loss: 3.56014084815979
training step: 50134, total_loss: 2.660615921020508
training step: 50135, total_loss: 2.068467617034912
training step: 50136, total_loss: 0.20317141711711884
training step: 50137, total_loss: 2.7544565200805664
training step: 50138, total_loss: 2.1245367527008057
training step: 50139, total_loss: 2.6832432746887207
training step: 50140, total_loss: 1.2550721168518066
training step: 50141, total_loss: 1.1168982982635498
training step: 50142, total_loss: 0.5585645437240601
training step: 50143, total_loss: 1.8319836854934692
training step: 50144, total_loss: 2.5694546699523926
training step: 50145, total_loss: 2.0359063148498535
training step: 50146, total_loss: 1.9524885416030884
training step: 50147, total_loss: 0.4343745708465576
training step: 50148, total_loss: 0.01436492707580328
training step: 50149, total_loss: 2.504378318786621
training step: 50150, total_loss: 3.621058225631714
training step: 50151, total_loss: 1.5623365640640259
training step: 50152, total_loss: 0.5712945461273193
training step: 50153, total_loss: 0.5678441524505615
training step: 50154, total_loss: 0.0028599780052900314
training step: 50155, total_loss: 0.7026754021644592
training step: 50156, total_loss: 1.4011187553405762
training step: 50157, total_loss: 0.7073565125465393
training step: 50158, total_loss: 1.8268240690231323
training step: 50159, total_loss: 1.0549542903900146
training step: 50160, total_loss: 1.0912225246429443
training step: 50161, total_loss: 0.3711303770542145
training step: 50162, total_loss: 0.36340653896331787
training step: 50163, total_loss: 0.9396405220031738
training step: 50164, total_loss: 2.726527690887451
training step: 50165, total_loss: 0.22884976863861084
training step: 50166, total_loss: 1.3433609008789062
training step: 50167, total_loss: 1.0774905681610107
training step: 50168, total_loss: 1.9585243463516235
training step: 50169, total_loss: 2.7599077224731445
training step: 50170, total_loss: 1.3206392526626587
training step: 50171, total_loss: 0.6108086109161377
training step: 50172, total_loss: 1.45613431930542
training step: 50173, total_loss: 2.2030065059661865
training step: 50174, total_loss: 3.8115460872650146
training step: 50175, total_loss: 6.218659400939941
training step: 50176, total_loss: 1.0021014213562012
training step: 50177, total_loss: 0.1627521514892578
training step: 50178, total_loss: 0.7071484327316284
training step: 50179, total_loss: 0.0006692997412756085
training step: 50180, total_loss: 0.42790138721466064
training step: 50181, total_loss: 0.006886238232254982
training step: 50182, total_loss: 2.037813425064087
training step: 50183, total_loss: 3.034480094909668
training step: 50184, total_loss: 1.0020036697387695
training step: 50185, total_loss: 0.9981397390365601
training step: 50186, total_loss: 0.8798294067382812
training step: 50187, total_loss: 5.001743316650391
training step: 50188, total_loss: 2.7706680297851562
training step: 50189, total_loss: 1.3581962585449219
training step: 50190, total_loss: 0.5650971531867981
training step: 50191, total_loss: 2.246528387069702
training step: 50192, total_loss: 2.462244987487793
training step: 50193, total_loss: 1.9625468254089355
training step: 50194, total_loss: 0.00825971644371748
training step: 50195, total_loss: 1.0607221126556396
training step: 50196, total_loss: 0.027738798409700394
training step: 50197, total_loss: 0.5288963913917542
training step: 50198, total_loss: 1.1261792182922363
training step: 50199, total_loss: 1.867185354232788
training step: 50200, total_loss: 1.4805681705474854
training step: 50201, total_loss: 0.2765428125858307
training step: 50202, total_loss: 2.951139450073242
training step: 50203, total_loss: 1.2932178974151611
training step: 50204, total_loss: 1.4461677074432373
training step: 50205, total_loss: 1.078821063041687
training step: 50206, total_loss: 2.158829689025879
training step: 50207, total_loss: 3.322568655014038
training step: 50208, total_loss: 2.9500479698181152
training step: 50209, total_loss: 1.3919892311096191
training step: 50210, total_loss: 3.3788113594055176
training step: 50211, total_loss: 2.8019542694091797
training step: 50212, total_loss: 0.6056919097900391
training step: 50213, total_loss: 0.0008998450939543545
training step: 50214, total_loss: 1.335972785949707
training step: 50215, total_loss: 1.528232455253601
training step: 50216, total_loss: 2.1511783599853516
training step: 50217, total_loss: 0.8293776512145996
training step: 50218, total_loss: 2.188460350036621
training step: 50219, total_loss: 1.1385571956634521
training step: 50220, total_loss: 4.567478656768799
training step: 50221, total_loss: 0.8895489573478699
training step: 50222, total_loss: 0.08147267997264862
training step: 50223, total_loss: 3.164712429046631
training step: 50224, total_loss: 1.5964516401290894
training step: 50225, total_loss: 0.07469557970762253
training step: 50226, total_loss: 2.379774570465088
training step: 50227, total_loss: 1.9064819812774658
training step: 50228, total_loss: 6.102514266967773
training step: 50229, total_loss: 3.187455415725708
training step: 50230, total_loss: 0.33148232102394104
training step: 50231, total_loss: 2.9306650161743164
training step: 50232, total_loss: 2.337981700897217
training step: 50233, total_loss: 2.6289477348327637
training step: 50234, total_loss: 2.3145158290863037
training step: 50235, total_loss: 1.8175519704818726
training step: 50236, total_loss: 2.796342134475708
training step: 50237, total_loss: 0.6177241206169128
training step: 50238, total_loss: 1.1907458305358887
training step: 50239, total_loss: 0.2604609727859497
training step: 50240, total_loss: 3.262861728668213
training step: 50241, total_loss: 0.5147044062614441
training step: 50242, total_loss: 2.117950439453125
training step: 50243, total_loss: 1.3196959495544434
training step: 50244, total_loss: 1.4367293119430542
training step: 50245, total_loss: 7.149291038513184
training step: 50246, total_loss: 0.7405048608779907
training step: 50247, total_loss: 1.8286099433898926
training step: 50248, total_loss: 2.288961410522461
training step: 50249, total_loss: 1.530858039855957
training step: 50250, total_loss: 0.1742624193429947
training step: 50251, total_loss: 2.594618082046509
training step: 50252, total_loss: 0.028279870748519897
training step: 50253, total_loss: 2.2063474655151367
training step: 50254, total_loss: 2.1483278274536133
training step: 50255, total_loss: 1.8018276691436768
training step: 50256, total_loss: 2.281970977783203
training step: 50257, total_loss: 2.54762601852417
training step: 50258, total_loss: 0.06380102783441544
training step: 50259, total_loss: 2.6903488636016846
training step: 50260, total_loss: 0.49276822805404663
training step: 50261, total_loss: 3.4572267532348633
training step: 50262, total_loss: 1.0992095470428467
training step: 50263, total_loss: 0.7018452882766724
training step: 50264, total_loss: 1.9382070302963257
training step: 50265, total_loss: 0.7253775596618652
training step: 50266, total_loss: 1.3523592948913574
training step: 50267, total_loss: 0.6927841901779175
training step: 50268, total_loss: 3.5640790462493896
training step: 50269, total_loss: 1.2122654914855957
training step: 50270, total_loss: 3.5340185165405273
training step: 50271, total_loss: 3.679715633392334
training step: 50272, total_loss: 1.9864583015441895
training step: 50273, total_loss: 1.4094340801239014
training step: 50274, total_loss: 0.3898872137069702
training step: 50275, total_loss: 2.8775758743286133
training step: 50276, total_loss: 1.4101231098175049
training step: 50277, total_loss: 0.04576852172613144
training step: 50278, total_loss: 2.974133253097534
training step: 50279, total_loss: 1.3655003309249878
training step: 50280, total_loss: 1.7809131145477295
training step: 50281, total_loss: 1.0023497343063354
training step: 50282, total_loss: 1.085445523262024
training step: 50283, total_loss: 1.536142110824585
training step: 50284, total_loss: 0.7499152421951294
training step: 50285, total_loss: 1.225031852722168
training step: 50286, total_loss: 1.0489369630813599
training step: 50287, total_loss: 0.7297775149345398
training step: 50288, total_loss: 1.3426746129989624
training step: 50289, total_loss: 1.3665874004364014
training step: 50290, total_loss: 1.3533244132995605
training step: 50291, total_loss: 2.0942068099975586
training step: 50292, total_loss: 0.5556614398956299
training step: 50293, total_loss: 0.40717145800590515
training step: 50294, total_loss: 1.824069619178772
training step: 50295, total_loss: 1.2590641975402832
training step: 50296, total_loss: 0.021235911175608635
training step: 50297, total_loss: 0.8942793011665344
training step: 50298, total_loss: 0.7590160965919495
training step: 50299, total_loss: 2.0132498741149902
training step: 50300, total_loss: 0.9147454500198364
training step: 50301, total_loss: 1.0138087272644043
training step: 50302, total_loss: 2.143949508666992
training step: 50303, total_loss: 0.8013074398040771
training step: 50304, total_loss: 0.5959590673446655
training step: 50305, total_loss: 1.4806506633758545
training step: 50306, total_loss: 4.11453104019165
training step: 50307, total_loss: 0.021668896079063416
training step: 50308, total_loss: 3.0941452980041504
training step: 50309, total_loss: 0.8531075716018677
training step: 50310, total_loss: 3.9405877590179443
training step: 50311, total_loss: 1.2106249332427979
training step: 50312, total_loss: 3.219254970550537
training step: 50313, total_loss: 3.9374842643737793
training step: 50314, total_loss: 0.46606361865997314
training step: 50315, total_loss: 0.6707174777984619
training step: 50316, total_loss: 0.024154312908649445
training step: 50317, total_loss: 0.15416738390922546
training step: 50318, total_loss: 1.292060136795044
training step: 50319, total_loss: 1.9888603687286377
training step: 50320, total_loss: 3.559896945953369
training step: 50321, total_loss: 2.7588565349578857
training step: 50322, total_loss: 1.7537975311279297
training step: 50323, total_loss: 4.255684852600098
training step: 50324, total_loss: 1.1894475221633911
training step: 50325, total_loss: 0.42456498742103577
training step: 50326, total_loss: 2.2271440029144287
training step: 50327, total_loss: 1.1937731504440308
training step: 50328, total_loss: 1.416339635848999
training step: 50329, total_loss: 3.6713764667510986
training step: 50330, total_loss: 3.70025897026062
training step: 50331, total_loss: 2.8174777030944824
training step: 50332, total_loss: 3.2906713485717773
training step: 50333, total_loss: 0.20584607124328613
training step: 50334, total_loss: 0.5727138519287109
training step: 50335, total_loss: 2.9870638847351074
training step: 50336, total_loss: 2.7174322605133057
training step: 50337, total_loss: 0.20633146166801453
training step: 50338, total_loss: 1.466752052307129
training step: 50339, total_loss: 2.5158820152282715
training step: 50340, total_loss: 2.5635030269622803
training step: 50341, total_loss: 0.6885197758674622
training step: 50342, total_loss: 2.242403268814087
training step: 50343, total_loss: 1.4678685665130615
training step: 50344, total_loss: 1.7255290746688843
training step: 50345, total_loss: 0.111686572432518
training step: 50346, total_loss: 3.3238072395324707
training step: 50347, total_loss: 4.267326354980469
training step: 50348, total_loss: 3.141963243484497
training step: 50349, total_loss: 1.430051326751709
training step: 50350, total_loss: 3.0815658569335938
training step: 50351, total_loss: 1.528372049331665
training step: 50352, total_loss: 2.5159857273101807
training step: 50353, total_loss: 1.5326663255691528
training step: 50354, total_loss: 0.6635349988937378
training step: 50355, total_loss: 0.6483312249183655
training step: 50356, total_loss: 1.5339782238006592
training step: 50357, total_loss: 3.2662134170532227
training step: 50358, total_loss: 1.2987844944000244
training step: 50359, total_loss: 1.6924431324005127
training step: 50360, total_loss: 1.0382522344589233
training step: 50361, total_loss: 1.9985859394073486
training step: 50362, total_loss: 3.6519014835357666
training step: 50363, total_loss: 2.6656689643859863
training step: 50364, total_loss: 0.14106324315071106
training step: 50365, total_loss: 1.1933876276016235
training step: 50366, total_loss: 2.3183932304382324
training step: 50367, total_loss: 1.5219392776489258
training step: 50368, total_loss: 0.00015311238530557603
training step: 50369, total_loss: 0.9095974564552307
training step: 50370, total_loss: 0.6076900959014893
training step: 50371, total_loss: 1.591212511062622
training step: 50372, total_loss: 2.0922117233276367
training step: 50373, total_loss: 1.1582107543945312
training step: 50374, total_loss: 0.8887430429458618
training step: 50375, total_loss: 1.6492953300476074
training step: 50376, total_loss: 0.8775739073753357
training step: 50377, total_loss: 1.5994300842285156
training step: 50378, total_loss: 2.858147382736206
training step: 50379, total_loss: 1.0086838006973267
training step: 50380, total_loss: 1.3368918895721436
training step: 50381, total_loss: 0.05176258087158203
training step: 50382, total_loss: 0.5435154438018799
training step: 50383, total_loss: 2.2199742794036865
training step: 50384, total_loss: 2.0630247592926025
training step: 50385, total_loss: 2.120041608810425
training step: 50386, total_loss: 2.6971826553344727
training step: 50387, total_loss: 0.20493948459625244
training step: 50388, total_loss: 1.8684613704681396
training step: 50389, total_loss: 2.3047852516174316
training step: 50390, total_loss: 0.9473158121109009
training step: 50391, total_loss: 2.2061357498168945
training step: 50392, total_loss: 1.8634201288223267
training step: 50393, total_loss: 0.09698370099067688
training step: 50394, total_loss: 2.368818521499634
training step: 50395, total_loss: 2.5801172256469727
training step: 50396, total_loss: 1.7258442640304565
training step: 50397, total_loss: 3.19447922706604
training step: 50398, total_loss: 5.104759216308594
training step: 50399, total_loss: 0.6702162027359009
training step: 50400, total_loss: 2.1214334964752197
training step: 50401, total_loss: 5.281704902648926
training step: 50402, total_loss: 1.8979111909866333
training step: 50403, total_loss: 3.0051021575927734
training step: 50404, total_loss: 3.727084159851074
training step: 50405, total_loss: 1.0185222625732422
training step: 50406, total_loss: 0.7212668657302856
training step: 50407, total_loss: 1.8706008195877075
training step: 50408, total_loss: 3.207561731338501
training step: 50409, total_loss: 1.8293061256408691
training step: 50410, total_loss: 1.3904443979263306
training step: 50411, total_loss: 1.1832263469696045
training step: 50412, total_loss: 1.724037528038025
training step: 50413, total_loss: 2.775217294692993
training step: 50414, total_loss: 2.2497432231903076
training step: 50415, total_loss: 2.846275568008423
training step: 50416, total_loss: 4.547577857971191
training step: 50417, total_loss: 3.5113775730133057
training step: 50418, total_loss: 2.033965587615967
training step: 50419, total_loss: 1.7895234823226929
training step: 50420, total_loss: 2.1958975791931152
training step: 50421, total_loss: 1.1362812519073486
training step: 50422, total_loss: 3.6580793857574463
training step: 50423, total_loss: 0.3942004144191742
training step: 50424, total_loss: 1.9351942539215088
training step: 50425, total_loss: 0.05065969005227089
training step: 50426, total_loss: 1.1629034280776978
training step: 50427, total_loss: 2.8250622749328613
training step: 50428, total_loss: 0.8078665733337402
training step: 50429, total_loss: 1.4724493026733398
training step: 50430, total_loss: 2.525108575820923
training step: 50431, total_loss: 3.4688937664031982
training step: 50432, total_loss: 2.4393486976623535
training step: 50433, total_loss: 1.3071843385696411
training step: 50434, total_loss: 2.6117358207702637
training step: 50435, total_loss: 0.9513607025146484
training step: 50436, total_loss: 1.963085412979126
training step: 50437, total_loss: 1.668535590171814
training step: 50438, total_loss: 2.3983590602874756
training step: 50439, total_loss: 1.0261675119400024
training step: 50440, total_loss: 0.4252970218658447
training step: 50441, total_loss: 1.5252785682678223
training step: 50442, total_loss: 1.1278043985366821
training step: 50443, total_loss: 2.2929177284240723
training step: 50444, total_loss: 1.4634068012237549
training step: 50445, total_loss: 0.7108784914016724
training step: 50446, total_loss: 5.060993194580078
training step: 50447, total_loss: 1.008188009262085
training step: 50448, total_loss: 2.110766649246216
training step: 50449, total_loss: 1.836024522781372
training step: 50450, total_loss: 0.824312150478363
training step: 50451, total_loss: 2.200296401977539
training step: 50452, total_loss: 0.9527577757835388
training step: 50453, total_loss: 1.0805628299713135
training step: 50454, total_loss: 0.41030195355415344
training step: 50455, total_loss: 2.412557363510132
training step: 50456, total_loss: 1.9198527336120605
training step: 50457, total_loss: 1.92110013961792
training step: 50458, total_loss: 1.8902485370635986
training step: 50459, total_loss: 0.85102379322052
training step: 50460, total_loss: 0.4112338423728943
training step: 50461, total_loss: 0.7580525875091553
training step: 50462, total_loss: 0.5563464164733887
training step: 50463, total_loss: 2.271185874938965
training step: 50464, total_loss: 1.6474486589431763
training step: 50465, total_loss: 2.314112663269043
training step: 50466, total_loss: 1.1042582988739014
training step: 50467, total_loss: 3.6358745098114014
training step: 50468, total_loss: 1.0532572269439697
training step: 50469, total_loss: 2.656041145324707
training step: 50470, total_loss: 0.964177131652832
training step: 50471, total_loss: 2.1153645515441895
training step: 50472, total_loss: 3.247685194015503
training step: 50473, total_loss: 1.8642940521240234
training step: 50474, total_loss: 1.3430795669555664
training step: 50475, total_loss: 1.961140751838684
training step: 50476, total_loss: 2.829097270965576
training step: 50477, total_loss: 1.2596908807754517
training step: 50478, total_loss: 6.449838638305664
training step: 50479, total_loss: 0.49402526021003723
training step: 50480, total_loss: 1.2260241508483887
training step: 50481, total_loss: 1.7376805543899536
training step: 50482, total_loss: 3.421487331390381
training step: 50483, total_loss: 1.4953333139419556
training step: 50484, total_loss: 0.5601624846458435
training step: 50485, total_loss: 1.1399061679840088
training step: 50486, total_loss: 1.395708441734314
training step: 50487, total_loss: 1.065688133239746
training step: 50488, total_loss: 1.878066062927246
training step: 50489, total_loss: 2.9403457641601562
training step: 50490, total_loss: 0.8586598038673401
training step: 50491, total_loss: 3.6538100242614746
training step: 50492, total_loss: 3.863680839538574
training step: 50493, total_loss: 2.2049694061279297
training step: 50494, total_loss: 2.2656893730163574
training step: 50495, total_loss: 0.22749489545822144
training step: 50496, total_loss: 2.054521083831787
training step: 50497, total_loss: 2.647839069366455
training step: 50498, total_loss: 1.6170310974121094
training step: 50499, total_loss: 0.012974054552614689
training step: 50500, total_loss: 1.441266417503357
training step: 50501, total_loss: 0.20393678545951843
training step: 50502, total_loss: 1.3383848667144775
training step: 50503, total_loss: 0.1455605924129486
training step: 50504, total_loss: 2.774562120437622
training step: 50505, total_loss: 0.35995572805404663
training step: 50506, total_loss: 0.001761583611369133
training step: 50507, total_loss: 0.2754354476928711
training step: 50508, total_loss: 0.0005455788923427463
training step: 50509, total_loss: 2.6999881267547607
training step: 50510, total_loss: 2.3404932022094727
training step: 50511, total_loss: 1.98123037815094
training step: 50512, total_loss: 0.40295735001564026
training step: 50513, total_loss: 0.7993967533111572
training step: 50514, total_loss: 2.0081684589385986
training step: 50515, total_loss: 1.1413837671279907
training step: 50516, total_loss: 2.0128324031829834
training step: 50517, total_loss: 1.3365256786346436
training step: 50518, total_loss: 1.2368688583374023
training step: 50519, total_loss: 0.4141756296157837
training step: 50520, total_loss: 0.21593068540096283
training step: 50521, total_loss: 0.6689330339431763
training step: 50522, total_loss: 1.2742022275924683
training step: 50523, total_loss: 1.5952095985412598
training step: 50524, total_loss: 2.3755545616149902
training step: 50525, total_loss: 1.2704153060913086
training step: 50526, total_loss: 0.5119730830192566
training step: 50527, total_loss: 1.4480009078979492
training step: 50528, total_loss: 1.7788805961608887
training step: 50529, total_loss: 3.295327663421631
training step: 50530, total_loss: 0.7331249713897705
training step: 50531, total_loss: 3.936100721359253
training step: 50532, total_loss: 2.5138959884643555
training step: 50533, total_loss: 2.3676323890686035
training step: 50534, total_loss: 4.0771942138671875
training step: 50535, total_loss: 0.24815398454666138
training step: 50536, total_loss: 0.5217311382293701
training step: 50537, total_loss: 1.9013832807540894
training step: 50538, total_loss: 1.2649922370910645
training step: 50539, total_loss: 0.2879832684993744
training step: 50540, total_loss: 1.9917285442352295
training step: 50541, total_loss: 1.754209280014038
training step: 50542, total_loss: 2.061108112335205
training step: 50543, total_loss: 1.2764002084732056
training step: 50544, total_loss: 0.4824906587600708
training step: 50545, total_loss: 3.336716651916504
training step: 50546, total_loss: 1.427596092224121
training step: 50547, total_loss: 1.5306761264801025
training step: 50548, total_loss: 2.444441795349121
training step: 50549, total_loss: 0.5262309312820435
training step: 50550, total_loss: 0.19217760860919952
training step: 50551, total_loss: 2.4936089515686035
training step: 50552, total_loss: 0.9980475902557373
training step: 50553, total_loss: 1.8807413578033447
training step: 50554, total_loss: 0.7765887975692749
training step: 50555, total_loss: 3.8703222274780273
training step: 50556, total_loss: 0.5062500834465027
training step: 50557, total_loss: 1.868221402168274
training step: 50558, total_loss: 0.36426493525505066
training step: 50559, total_loss: 2.304483413696289
training step: 50560, total_loss: 1.2560093402862549
training step: 50561, total_loss: 2.256213665008545
training step: 50562, total_loss: 2.1049139499664307
training step: 50563, total_loss: 1.6440532207489014
training step: 50564, total_loss: 0.577499508857727
training step: 50565, total_loss: 0.17998436093330383
training step: 50566, total_loss: 2.105781316757202
training step: 50567, total_loss: 4.433324337005615
training step: 50568, total_loss: 1.2178308963775635
training step: 50569, total_loss: 1.6245206594467163
training step: 50570, total_loss: 1.7287898063659668
training step: 50571, total_loss: 0.7852932810783386
training step: 50572, total_loss: 1.9025676250457764
training step: 50573, total_loss: 0.8819403648376465
training step: 50574, total_loss: 2.479893684387207
training step: 50575, total_loss: 1.7625312805175781
training step: 50576, total_loss: 1.1281185150146484
training step: 50577, total_loss: 1.7127385139465332
training step: 50578, total_loss: 1.80137300491333
training step: 50579, total_loss: 3.360356092453003
training step: 50580, total_loss: 1.5782725811004639
training step: 50581, total_loss: 3.332730770111084
training step: 50582, total_loss: 1.318493127822876
training step: 50583, total_loss: 0.5214237570762634
training step: 50584, total_loss: 1.9838327169418335
training step: 50585, total_loss: 1.5878037214279175
training step: 50586, total_loss: 2.1057910919189453
training step: 50587, total_loss: 0.14936527609825134
training step: 50588, total_loss: 1.2964794635772705
training step: 50589, total_loss: 0.39665716886520386
training step: 50590, total_loss: 0.5619783997535706
training step: 50591, total_loss: 3.8291678428649902
training step: 50592, total_loss: 0.7760583758354187
training step: 50593, total_loss: 2.339294910430908
training step: 50594, total_loss: 1.0229246616363525
training step: 50595, total_loss: 0.8512171506881714
training step: 50596, total_loss: 1.2819602489471436
training step: 50597, total_loss: 0.6145089268684387
training step: 50598, total_loss: 0.3278311789035797
training step: 50599, total_loss: 1.6398133039474487
training step: 50600, total_loss: 0.7052053213119507
training step: 50601, total_loss: 0.06528275460004807
training step: 50602, total_loss: 1.2495566606521606
training step: 50603, total_loss: 3.9960832595825195
training step: 50604, total_loss: 0.005640905350446701
training step: 50605, total_loss: 0.54023277759552
training step: 50606, total_loss: 0.019652923569083214
training step: 50607, total_loss: 5.285591125488281
training step: 50608, total_loss: 0.41898006200790405
training step: 50609, total_loss: 2.114710569381714
training step: 50610, total_loss: 3.532534599304199
training step: 50611, total_loss: 1.9420571327209473
training step: 50612, total_loss: 0.5132794380187988
training step: 50613, total_loss: 1.5732710361480713
training step: 50614, total_loss: 0.09854225069284439
training step: 50615, total_loss: 0.9907526969909668
training step: 50616, total_loss: 0.008615237660706043
training step: 50617, total_loss: 1.656402826309204
training step: 50618, total_loss: 0.6796953678131104
training step: 50619, total_loss: 2.183577537536621
training step: 50620, total_loss: 0.7588869333267212
training step: 50621, total_loss: 1.864985466003418
training step: 50622, total_loss: 1.1050055027008057
training step: 50623, total_loss: 1.7481586933135986
training step: 50624, total_loss: 0.8337713479995728
training step: 50625, total_loss: 0.8101464509963989
training step: 50626, total_loss: 4.086308479309082
training step: 50627, total_loss: 3.700467348098755
training step: 50628, total_loss: 1.6569247245788574
training step: 50629, total_loss: 2.9899377822875977
training step: 50630, total_loss: 0.7543338537216187
training step: 50631, total_loss: 0.6667841672897339
training step: 50632, total_loss: 0.7436239719390869
training step: 50633, total_loss: 2.385128974914551
training step: 50634, total_loss: 0.2393476665019989
training step: 50635, total_loss: 0.0164454597979784
training step: 50636, total_loss: 0.6580303311347961
training step: 50637, total_loss: 1.2475683689117432
training step: 50638, total_loss: 0.21105682849884033
training step: 50639, total_loss: 2.9808685779571533
training step: 50640, total_loss: 0.12200014293193817
training step: 50641, total_loss: 0.4875653088092804
training step: 50642, total_loss: 3.6778461933135986
training step: 50643, total_loss: 3.9708621501922607
training step: 50644, total_loss: 2.703209400177002
training step: 50645, total_loss: 1.0437372922897339
training step: 50646, total_loss: 2.97957444190979
training step: 50647, total_loss: 1.3519883155822754
training step: 50648, total_loss: 1.472780704498291
training step: 50649, total_loss: 0.9884014129638672
training step: 50650, total_loss: 0.3172604739665985
training step: 50651, total_loss: 4.920360565185547
training step: 50652, total_loss: 3.235616683959961
training step: 50653, total_loss: 2.4837026596069336
training step: 50654, total_loss: 0.05718042701482773
training step: 50655, total_loss: 1.5839909315109253
training step: 50656, total_loss: 0.6229199171066284
training step: 50657, total_loss: 0.9869753122329712
training step: 50658, total_loss: 0.22971954941749573
training step: 50659, total_loss: 1.3150571584701538
training step: 50660, total_loss: 0.7170917987823486
training step: 50661, total_loss: 0.31170132756233215
training step: 50662, total_loss: 0.784325122833252
training step: 50663, total_loss: 1.1989538669586182
training step: 50664, total_loss: 1.574554681777954
training step: 50665, total_loss: 0.4422933757305145
training step: 50666, total_loss: 0.5896744728088379
training step: 50667, total_loss: 1.2087013721466064
training step: 50668, total_loss: 0.960879385471344
training step: 50669, total_loss: 1.9818356037139893
training step: 50670, total_loss: 1.466908574104309
training step: 50671, total_loss: 0.02348596230149269
training step: 50672, total_loss: 0.5875614881515503
training step: 50673, total_loss: 0.0863078385591507
training step: 50674, total_loss: 4.246563911437988
training step: 50675, total_loss: 3.1288440227508545
training step: 50676, total_loss: 1.0947996377944946
training step: 50677, total_loss: 0.2833775281906128
training step: 50678, total_loss: 0.7702970504760742
training step: 50679, total_loss: 2.2207183837890625
training step: 50680, total_loss: 0.47236207127571106
training step: 50681, total_loss: 6.365863800048828
training step: 50682, total_loss: 1.8102686405181885
training step: 50683, total_loss: 2.679415702819824
training step: 50684, total_loss: 0.26154881715774536
training step: 50685, total_loss: 1.7389696836471558
training step: 50686, total_loss: 2.5058016777038574
training step: 50687, total_loss: 1.0245654582977295
training step: 50688, total_loss: 2.3955302238464355
training step: 50689, total_loss: 1.2501734495162964
training step: 50690, total_loss: 0.9734579920768738
training step: 50691, total_loss: 1.4980859756469727
training step: 50692, total_loss: 0.00042873778147622943
training step: 50693, total_loss: 0.050537653267383575
training step: 50694, total_loss: 0.6513271927833557
training step: 50695, total_loss: 0.48387646675109863
training step: 50696, total_loss: 0.5068108439445496
training step: 50697, total_loss: 1.5515395402908325
training step: 50698, total_loss: 2.300856113433838
training step: 50699, total_loss: 0.6278104782104492
training step: 50700, total_loss: 4.045738220214844
training step: 50701, total_loss: 0.9983908534049988
training step: 50702, total_loss: 3.409743547439575
training step: 50703, total_loss: 2.166170358657837
training step: 50704, total_loss: 1.9357435703277588
training step: 50705, total_loss: 5.887433052062988
training step: 50706, total_loss: 4.417829513549805
training step: 50707, total_loss: 0.3916904926300049
training step: 50708, total_loss: 2.369960069656372
training step: 50709, total_loss: 0.7501484751701355
training step: 50710, total_loss: 1.6650621891021729
training step: 50711, total_loss: 1.8729302883148193
training step: 50712, total_loss: 2.645956039428711
training step: 50713, total_loss: 1.930501937866211
training step: 50714, total_loss: 1.8485761880874634
training step: 50715, total_loss: 1.990285873413086
training step: 50716, total_loss: 1.2891151905059814
training step: 50717, total_loss: 3.195305824279785
training step: 50718, total_loss: 1.420659065246582
training step: 50719, total_loss: 1.3513500690460205
training step: 50720, total_loss: 2.800528049468994
training step: 50721, total_loss: 1.7049881219863892
training step: 50722, total_loss: 1.771459937095642
training step: 50723, total_loss: 0.3132697641849518
training step: 50724, total_loss: 0.06315518170595169
training step: 50725, total_loss: 4.104947090148926
training step: 50726, total_loss: 1.1484583616256714
training step: 50727, total_loss: 0.21311895549297333
training step: 50728, total_loss: 0.6074777841567993
training step: 50729, total_loss: 0.6115238666534424
training step: 50730, total_loss: 1.684114933013916
training step: 50731, total_loss: 5.240504264831543
training step: 50732, total_loss: 1.407313585281372
training step: 50733, total_loss: 1.802592158317566
training step: 50734, total_loss: 1.4563405513763428
training step: 50735, total_loss: 1.2607886791229248
training step: 50736, total_loss: 0.48291417956352234
training step: 50737, total_loss: 2.2400972843170166
training step: 50738, total_loss: 1.4234331846237183
training step: 50739, total_loss: 3.13144588470459
training step: 50740, total_loss: 1.5002496242523193
training step: 50741, total_loss: 1.5798505544662476
training step: 50742, total_loss: 2.457084894180298
training step: 50743, total_loss: 2.693293333053589
training step: 50744, total_loss: 1.768251657485962
training step: 50745, total_loss: 1.7831236124038696
training step: 50746, total_loss: 2.761017322540283
training step: 50747, total_loss: 1.3954488039016724
training step: 50748, total_loss: 1.161399245262146
training step: 50749, total_loss: 0.7937184572219849
training step: 50750, total_loss: 2.005718231201172
training step: 50751, total_loss: 3.771331548690796
training step: 50752, total_loss: 1.3985958099365234
training step: 50753, total_loss: 3.3510167598724365
training step: 50754, total_loss: 1.1479593515396118
training step: 50755, total_loss: 1.1092153787612915
training step: 50756, total_loss: 2.641627788543701
training step: 50757, total_loss: 1.2357368469238281
training step: 50758, total_loss: 0.029928017407655716
training step: 50759, total_loss: 3.5384068489074707
training step: 50760, total_loss: 1.415075659751892
training step: 50761, total_loss: 0.03987053781747818
training step: 50762, total_loss: 0.06545635312795639
training step: 50763, total_loss: 2.4994750022888184
training step: 50764, total_loss: 0.7991636991500854
training step: 50765, total_loss: 1.1934844255447388
training step: 50766, total_loss: 2.1373066902160645
training step: 50767, total_loss: 0.8162476420402527
training step: 50768, total_loss: 2.0318193435668945
training step: 50769, total_loss: 1.1567871570587158
training step: 50770, total_loss: 0.3744693100452423
training step: 50771, total_loss: 3.266636848449707
training step: 50772, total_loss: 2.760765552520752
training step: 50773, total_loss: 1.6692347526550293
training step: 50774, total_loss: 1.2561880350112915
training step: 50775, total_loss: 0.28862079977989197
training step: 50776, total_loss: 3.601740598678589
training step: 50777, total_loss: 0.3970103859901428
training step: 50778, total_loss: 0.9851470589637756
training step: 50779, total_loss: 1.2826181650161743
training step: 50780, total_loss: 2.4478864669799805
training step: 50781, total_loss: 1.1734888553619385
training step: 50782, total_loss: 0.8143028616905212
training step: 50783, total_loss: 1.39421808719635
training step: 50784, total_loss: 1.283380389213562
training step: 50785, total_loss: 0.05866078659892082
training step: 50786, total_loss: 0.29296937584877014
training step: 50787, total_loss: 1.7373831272125244
training step: 50788, total_loss: 0.49803033471107483
training step: 50789, total_loss: 3.37274169921875
training step: 50790, total_loss: 4.6599249839782715
training step: 50791, total_loss: 3.0065999031066895
training step: 50792, total_loss: 1.0706968307495117
training step: 50793, total_loss: 2.310856580734253
training step: 50794, total_loss: 2.199305534362793
training step: 50795, total_loss: 0.0724676325917244
training step: 50796, total_loss: 1.4798760414123535
training step: 50797, total_loss: 2.9273452758789062
training step: 50798, total_loss: 1.0977323055267334
training step: 50799, total_loss: 2.331753730773926
training step: 50800, total_loss: 7.425104141235352
training step: 50801, total_loss: 0.5274609327316284
training step: 50802, total_loss: 1.0209587812423706
training step: 50803, total_loss: 1.0819475650787354
training step: 50804, total_loss: 2.800128936767578
training step: 50805, total_loss: 0.13230904936790466
training step: 50806, total_loss: 3.1194651126861572
training step: 50807, total_loss: 4.503113269805908
training step: 50808, total_loss: 3.09785795211792
training step: 50809, total_loss: 4.280556678771973
training step: 50810, total_loss: 1.751668095588684
training step: 50811, total_loss: 0.03747035935521126
training step: 50812, total_loss: 2.416471004486084
training step: 50813, total_loss: 2.496776580810547
training step: 50814, total_loss: 1.4740102291107178
training step: 50815, total_loss: 2.2785866260528564
training step: 50816, total_loss: 0.0008465321152471006
training step: 50817, total_loss: 1.7168515920639038
training step: 50818, total_loss: 0.10927106440067291
training step: 50819, total_loss: 0.41699326038360596
training step: 50820, total_loss: 3.6305012702941895
training step: 50821, total_loss: 0.7345417737960815
training step: 50822, total_loss: 2.652853012084961
training step: 50823, total_loss: 1.1508337259292603
training step: 50824, total_loss: 0.11942479759454727
training step: 50825, total_loss: 5.595475196838379
training step: 50826, total_loss: 1.2574176788330078
training step: 50827, total_loss: 0.5952811241149902
training step: 50828, total_loss: 1.4923335313796997
training step: 50829, total_loss: 0.2406097650527954
training step: 50830, total_loss: 3.1358695030212402
training step: 50831, total_loss: 3.8565638065338135
training step: 50832, total_loss: 4.538028240203857
training step: 50833, total_loss: 1.1995069980621338
training step: 50834, total_loss: 1.1714287996292114
training step: 50835, total_loss: 1.0972799062728882
training step: 50836, total_loss: 0.9489307403564453
training step: 50837, total_loss: 2.802964448928833
training step: 50838, total_loss: 3.178823947906494
training step: 50839, total_loss: 0.2904346287250519
training step: 50840, total_loss: 1.3295000791549683
training step: 50841, total_loss: 1.2867305278778076
training step: 50842, total_loss: 3.748230457305908
training step: 50843, total_loss: 0.5549726486206055
training step: 50844, total_loss: 0.8892066478729248
training step: 50845, total_loss: 0.7403414845466614
training step: 50846, total_loss: 0.9007443189620972
training step: 50847, total_loss: 2.8024978637695312
training step: 50848, total_loss: 0.8114242553710938
training step: 50849, total_loss: 3.0118675231933594
training step: 50850, total_loss: 0.18183991312980652
training step: 50851, total_loss: 2.1944539546966553
training step: 50852, total_loss: 1.4395835399627686
training step: 50853, total_loss: 0.8546224236488342
training step: 50854, total_loss: 3.99284029006958
training step: 50855, total_loss: 2.711970806121826
training step: 50856, total_loss: 3.1547935009002686
training step: 50857, total_loss: 5.16886043548584
training step: 50858, total_loss: 0.015137807466089725
training step: 50859, total_loss: 0.9957609176635742
training step: 50860, total_loss: 0.8296794891357422
training step: 50861, total_loss: 2.1826086044311523
training step: 50862, total_loss: 1.1474230289459229
training step: 50863, total_loss: 0.00028891852707602084
training step: 50864, total_loss: 0.7812031507492065
training step: 50865, total_loss: 0.8132537603378296
training step: 50866, total_loss: 1.0941616296768188
training step: 50867, total_loss: 1.4072368144989014
training step: 50868, total_loss: 1.585649013519287
training step: 50869, total_loss: 0.6793431043624878
training step: 50870, total_loss: 1.5534136295318604
training step: 50871, total_loss: 3.515376567840576
training step: 50872, total_loss: 1.4596614837646484
training step: 50873, total_loss: 0.31092533469200134
training step: 50874, total_loss: 1.2507495880126953
training step: 50875, total_loss: 1.2614094018936157
training step: 50876, total_loss: 1.2950024604797363
training step: 50877, total_loss: 2.8816592693328857
training step: 50878, total_loss: 0.64213627576828
training step: 50879, total_loss: 1.3307240009307861
training step: 50880, total_loss: 1.412927508354187
training step: 50881, total_loss: 0.6528599858283997
training step: 50882, total_loss: 1.4265817403793335
training step: 50883, total_loss: 1.126923680305481
training step: 50884, total_loss: 1.1234480142593384
training step: 50885, total_loss: 1.9914566278457642
training step: 50886, total_loss: 0.05694979801774025
training step: 50887, total_loss: 1.1559141874313354
training step: 50888, total_loss: 0.7225751280784607
training step: 50889, total_loss: 2.286486864089966
training step: 50890, total_loss: 0.2728193998336792
training step: 50891, total_loss: 0.959324061870575
training step: 50892, total_loss: 0.31546223163604736
training step: 50893, total_loss: 0.853101909160614
training step: 50894, total_loss: 1.1092982292175293
training step: 50895, total_loss: 2.596357822418213
training step: 50896, total_loss: 9.381309509277344
training step: 50897, total_loss: 1.390321969985962
training step: 50898, total_loss: 1.6150729656219482
training step: 50899, total_loss: 1.857811450958252
training step: 50900, total_loss: 0.8362568616867065
training step: 50901, total_loss: 0.34483978152275085
training step: 50902, total_loss: 0.17505045235157013
training step: 50903, total_loss: 1.1090996265411377
training step: 50904, total_loss: 1.3939409255981445
training step: 50905, total_loss: 1.0184348821640015
training step: 50906, total_loss: 0.084941066801548
training step: 50907, total_loss: 0.5956985354423523
training step: 50908, total_loss: 0.7110273838043213
training step: 50909, total_loss: 1.7431700229644775
training step: 50910, total_loss: 2.4320688247680664
training step: 50911, total_loss: 2.8748607635498047
training step: 50912, total_loss: 0.8415393829345703
training step: 50913, total_loss: 3.7641043663024902
training step: 50914, total_loss: 0.7951781153678894
training step: 50915, total_loss: 0.19422772526741028
training step: 50916, total_loss: 0.9269031286239624
training step: 50917, total_loss: 1.7629835605621338
training step: 50918, total_loss: 1.7881443500518799
training step: 50919, total_loss: 1.2154603004455566
training step: 50920, total_loss: 0.5741443634033203
training step: 50921, total_loss: 0.7581557035446167
training step: 50922, total_loss: 0.8215485215187073
training step: 50923, total_loss: 3.2148146629333496
training step: 50924, total_loss: 1.4085981845855713
training step: 50925, total_loss: 1.7342529296875
training step: 50926, total_loss: 2.6236014366149902
training step: 50927, total_loss: 1.6496500968933105
training step: 50928, total_loss: 1.5052236318588257
training step: 50929, total_loss: 1.402918815612793
training step: 50930, total_loss: 2.7547411918640137
training step: 50931, total_loss: 2.128187656402588
training step: 50932, total_loss: 1.0414713621139526
training step: 50933, total_loss: 1.9307093620300293
training step: 50934, total_loss: 2.699720859527588
training step: 50935, total_loss: 4.302471160888672
training step: 50936, total_loss: 1.0549848079681396
training step: 50937, total_loss: 0.7604038715362549
training step: 50938, total_loss: 2.8623576164245605
training step: 50939, total_loss: 1.4669206142425537
training step: 50940, total_loss: 0.7741573452949524
training step: 50941, total_loss: 1.5372294187545776
training step: 50942, total_loss: 0.574730634689331
training step: 50943, total_loss: 0.7722377777099609
training step: 50944, total_loss: 4.913568496704102
training step: 50945, total_loss: 0.9391025304794312
training step: 50946, total_loss: 1.0984700918197632
training step: 50947, total_loss: 1.9008584022521973
training step: 50948, total_loss: 2.1178336143493652
training step: 50949, total_loss: 3.0630083084106445
training step: 50950, total_loss: 1.9051897525787354
training step: 50951, total_loss: 1.4644502401351929
training step: 50952, total_loss: 2.558121919631958
training step: 50953, total_loss: 1.2454869747161865
training step: 50954, total_loss: 1.6211804151535034
training step: 50955, total_loss: 0.8182172775268555
training step: 50956, total_loss: 0.4686494469642639
training step: 50957, total_loss: 3.153932571411133
training step: 50958, total_loss: 2.4361085891723633
training step: 50959, total_loss: 0.07270479202270508
training step: 50960, total_loss: 2.7833669185638428
training step: 50961, total_loss: 0.4661839008331299
training step: 50962, total_loss: 2.7851080894470215
training step: 50963, total_loss: 1.4919893741607666
training step: 50964, total_loss: 0.9501914978027344
training step: 50965, total_loss: 0.23613488674163818
training step: 50966, total_loss: 2.2731704711914062
training step: 50967, total_loss: 2.710050106048584
training step: 50968, total_loss: 1.0416477918624878
training step: 50969, total_loss: 2.0082504749298096
training step: 50970, total_loss: 1.4616929292678833
training step: 50971, total_loss: 0.002334904856979847
training step: 50972, total_loss: 2.696223735809326
training step: 50973, total_loss: 0.9973615407943726
training step: 50974, total_loss: 1.415173053741455
training step: 50975, total_loss: 4.313108444213867
training step: 50976, total_loss: 0.9147127866744995
training step: 50977, total_loss: 1.5262634754180908
training step: 50978, total_loss: 1.3083449602127075
training step: 50979, total_loss: 0.14825710654258728
training step: 50980, total_loss: 2.7121307849884033
training step: 50981, total_loss: 1.714766025543213
training step: 50982, total_loss: 0.8469533920288086
training step: 50983, total_loss: 3.9433834552764893
training step: 50984, total_loss: 1.2967338562011719
training step: 50985, total_loss: 2.0305163860321045
training step: 50986, total_loss: 0.7149454355239868
training step: 50987, total_loss: 1.1255896091461182
training step: 50988, total_loss: 2.8850693702697754
training step: 50989, total_loss: 3.958979606628418
training step: 50990, total_loss: 1.5771136283874512
training step: 50991, total_loss: 3.319746971130371
training step: 50992, total_loss: 0.5017462372779846
training step: 50993, total_loss: 3.4732589721679688
training step: 50994, total_loss: 3.0290093421936035
training step: 50995, total_loss: 0.8114998936653137
training step: 50996, total_loss: 0.10485269129276276
training step: 50997, total_loss: 2.725017547607422INFO:tensorflow:Writing predictions to: test_output/predictions_51000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_51000.json

training step: 50998, total_loss: 0.8479437828063965
training step: 50999, total_loss: 2.1571526527404785
training step: 51000, total_loss: 1.3046112060546875
epoch finished! shuffle=False
evaluation: 15000, total_loss: 1.8673226833343506, f1: 55.97230361672422, followup: 29.012627415183324, yesno: 58.9989350372737, heq: 50.66179826563213, dheq: 3.6

Model saved in path test_output//model_51000.ckpt
training step: 51001, total_loss: 0.30409693717956543
training step: 51002, total_loss: 0.9436479210853577
training step: 51003, total_loss: 0.360944926738739
training step: 51004, total_loss: 3.163050651550293
training step: 51005, total_loss: 0.5377614498138428
training step: 51006, total_loss: 0.565393328666687
training step: 51007, total_loss: 1.3091962337493896
training step: 51008, total_loss: 1.8879765272140503
training step: 51009, total_loss: 1.6555119752883911
training step: 51010, total_loss: 2.590826988220215
training step: 51011, total_loss: 0.15793964266777039
training step: 51012, total_loss: 0.7029410600662231
training step: 51013, total_loss: 3.6020326614379883
training step: 51014, total_loss: 2.1679553985595703
training step: 51015, total_loss: 1.1102615594863892
training step: 51016, total_loss: 1.5289299488067627
training step: 51017, total_loss: 1.5075690746307373
training step: 51018, total_loss: 1.5974513292312622
training step: 51019, total_loss: 4.951502323150635
training step: 51020, total_loss: 0.9955393075942993
training step: 51021, total_loss: 1.2814197540283203
training step: 51022, total_loss: 0.618887722492218
training step: 51023, total_loss: 1.435770034790039
training step: 51024, total_loss: 2.478271961212158
training step: 51025, total_loss: 0.3020634949207306
training step: 51026, total_loss: 2.5513954162597656
training step: 51027, total_loss: 2.245900869369507
training step: 51028, total_loss: 1.0726536512374878
training step: 51029, total_loss: 0.2367391586303711
training step: 51030, total_loss: 0.17102645337581635
training step: 51031, total_loss: 1.94807767868042
training step: 51032, total_loss: 0.6936346292495728
training step: 51033, total_loss: 1.8317434787750244
training step: 51034, total_loss: 1.9918208122253418
training step: 51035, total_loss: 1.6709214448928833
training step: 51036, total_loss: 1.9202015399932861
training step: 51037, total_loss: 1.943040132522583
training step: 51038, total_loss: 0.31752827763557434
training step: 51039, total_loss: 0.9577926397323608
training step: 51040, total_loss: 1.3652763366699219
training step: 51041, total_loss: 2.1921072006225586
training step: 51042, total_loss: 1.339217185974121
training step: 51043, total_loss: 0.8778901100158691
training step: 51044, total_loss: 0.0007211900665424764
training step: 51045, total_loss: 0.41572362184524536
training step: 51046, total_loss: 0.33703044056892395
training step: 51047, total_loss: 0.787629246711731
training step: 51048, total_loss: 1.6238479614257812
training step: 51049, total_loss: 0.034557897597551346
training step: 51050, total_loss: 3.1810450553894043
training step: 51051, total_loss: 1.476733684539795
training step: 51052, total_loss: 0.45307981967926025
training step: 51053, total_loss: 3.7029075622558594
training step: 51054, total_loss: 1.5982582569122314
training step: 51055, total_loss: 3.327190399169922
training step: 51056, total_loss: 1.571812391281128
training step: 51057, total_loss: 1.9166454076766968
training step: 51058, total_loss: 2.3792357444763184
training step: 51059, total_loss: 1.737884283065796
training step: 51060, total_loss: 3.417191743850708
training step: 51061, total_loss: 0.7970033884048462
training step: 51062, total_loss: 3.744274616241455
training step: 51063, total_loss: 2.200786590576172
training step: 51064, total_loss: 1.7414796352386475
training step: 51065, total_loss: 3.380398750305176
training step: 51066, total_loss: 1.09677255153656
training step: 51067, total_loss: 2.9642844200134277
training step: 51068, total_loss: 0.705083966255188
training step: 51069, total_loss: 0.20307353138923645
training step: 51070, total_loss: 0.8990418314933777
training step: 51071, total_loss: 1.8322057723999023
training step: 51072, total_loss: 1.0623208284378052
training step: 51073, total_loss: 4.218271732330322
training step: 51074, total_loss: 0.8795086145401001
training step: 51075, total_loss: 3.867687702178955
training step: 51076, total_loss: 3.1723685264587402
training step: 51077, total_loss: 0.8502943515777588
training step: 51078, total_loss: 0.9542585015296936
training step: 51079, total_loss: 0.5874218940734863
training step: 51080, total_loss: 0.6544933915138245
training step: 51081, total_loss: 1.1127676963806152
training step: 51082, total_loss: 0.7454374432563782
training step: 51083, total_loss: 1.0306575298309326
training step: 51084, total_loss: 0.8073915243148804
training step: 51085, total_loss: 0.9393466711044312
training step: 51086, total_loss: 1.1775352954864502
training step: 51087, total_loss: 0.6272590756416321
training step: 51088, total_loss: 1.1570277214050293
training step: 51089, total_loss: 1.4960938692092896
training step: 51090, total_loss: 3.01179838180542
training step: 51091, total_loss: 1.3907114267349243
training step: 51092, total_loss: 0.6819789409637451
training step: 51093, total_loss: 1.222075343132019
training step: 51094, total_loss: 1.3242946863174438
training step: 51095, total_loss: 0.10633349418640137
training step: 51096, total_loss: 0.8557400107383728
training step: 51097, total_loss: 4.4504899978637695
training step: 51098, total_loss: 1.1124411821365356
training step: 51099, total_loss: 0.5012954473495483
training step: 51100, total_loss: 1.123319387435913
training step: 51101, total_loss: 1.0634645223617554
training step: 51102, total_loss: 0.31505700945854187
training step: 51103, total_loss: 0.8551889061927795
training step: 51104, total_loss: 0.7935844659805298
training step: 51105, total_loss: 1.5372343063354492
training step: 51106, total_loss: 0.2984769940376282
training step: 51107, total_loss: 1.6601835489273071
training step: 51108, total_loss: 1.88578200340271
training step: 51109, total_loss: 2.4263501167297363
training step: 51110, total_loss: 2.527637243270874
training step: 51111, total_loss: 3.77023983001709
training step: 51112, total_loss: 0.11271151155233383
training step: 51113, total_loss: 1.2818291187286377
training step: 51114, total_loss: 2.990873336791992
training step: 51115, total_loss: 1.9805147647857666
training step: 51116, total_loss: 1.861406683921814
training step: 51117, total_loss: 1.5578405857086182
training step: 51118, total_loss: 3.884645938873291
training step: 51119, total_loss: 3.332307815551758
training step: 51120, total_loss: 2.8258142471313477
training step: 51121, total_loss: 0.3079203963279724
training step: 51122, total_loss: 1.2573531866073608
training step: 51123, total_loss: 3.5980989933013916
training step: 51124, total_loss: 0.1966835856437683
training step: 51125, total_loss: 1.1145899295806885
training step: 51126, total_loss: 2.3520400524139404
training step: 51127, total_loss: 2.1921792030334473
training step: 51128, total_loss: 2.6824779510498047
training step: 51129, total_loss: 2.7539474964141846
training step: 51130, total_loss: 2.1725234985351562
training step: 51131, total_loss: 1.0126672983169556
training step: 51132, total_loss: 0.7016913890838623
training step: 51133, total_loss: 2.301271438598633
training step: 51134, total_loss: 2.298430919647217
training step: 51135, total_loss: 0.7335976362228394
training step: 51136, total_loss: 0.8022924065589905
training step: 51137, total_loss: 2.3961896896362305
training step: 51138, total_loss: 2.1756138801574707
training step: 51139, total_loss: 2.083686590194702
training step: 51140, total_loss: 0.4901611804962158
training step: 51141, total_loss: 0.00010066705726785585
training step: 51142, total_loss: 1.8065743446350098
training step: 51143, total_loss: 0.355438232421875
training step: 51144, total_loss: 3.146575927734375
training step: 51145, total_loss: 0.6094900965690613
training step: 51146, total_loss: 2.4834189414978027
training step: 51147, total_loss: 0.026294562965631485
training step: 51148, total_loss: 1.054308295249939
training step: 51149, total_loss: 1.6798514127731323
training step: 51150, total_loss: 2.970318555831909
training step: 51151, total_loss: 1.0089333057403564
training step: 51152, total_loss: 1.309682846069336
training step: 51153, total_loss: 2.278568744659424
training step: 51154, total_loss: 1.0855772495269775
training step: 51155, total_loss: 2.435710906982422
training step: 51156, total_loss: 2.6444530487060547
training step: 51157, total_loss: 2.0264217853546143
training step: 51158, total_loss: 1.8221266269683838
training step: 51159, total_loss: 2.520378828048706
training step: 51160, total_loss: 1.1398818492889404
training step: 51161, total_loss: 0.5488413572311401
training step: 51162, total_loss: 1.5867830514907837
training step: 51163, total_loss: 3.621396541595459
training step: 51164, total_loss: 0.6828514933586121
training step: 51165, total_loss: 2.609994888305664
training step: 51166, total_loss: 0.3536016345024109
training step: 51167, total_loss: 3.197509765625
training step: 51168, total_loss: 1.9285850524902344
training step: 51169, total_loss: 0.7723773121833801
training step: 51170, total_loss: 2.575300693511963
training step: 51171, total_loss: 1.9572644233703613
training step: 51172, total_loss: 0.1060110554099083
training step: 51173, total_loss: 1.5775165557861328
training step: 51174, total_loss: 1.419989824295044
training step: 51175, total_loss: 0.9169066548347473
training step: 51176, total_loss: 1.8613026142120361
training step: 51177, total_loss: 4.523135185241699
training step: 51178, total_loss: 1.6032423973083496
training step: 51179, total_loss: 1.412352442741394
training step: 51180, total_loss: 0.5181238651275635
training step: 51181, total_loss: 1.8954243659973145
training step: 51182, total_loss: 0.8924782872200012
training step: 51183, total_loss: 4.169527053833008
training step: 51184, total_loss: 1.4393723011016846
training step: 51185, total_loss: 1.754465103149414
training step: 51186, total_loss: 1.0844378471374512
training step: 51187, total_loss: 3.1032142639160156
training step: 51188, total_loss: 3.556112051010132
training step: 51189, total_loss: 2.278946876525879
training step: 51190, total_loss: 1.3113001585006714
training step: 51191, total_loss: 1.686047077178955
training step: 51192, total_loss: 1.6664763689041138
training step: 51193, total_loss: 2.0360512733459473
training step: 51194, total_loss: 0.6252472400665283
training step: 51195, total_loss: 1.678591012954712
training step: 51196, total_loss: 1.6057045459747314
training step: 51197, total_loss: 0.5340893864631653
training step: 51198, total_loss: 2.7970428466796875
training step: 51199, total_loss: 3.708332061767578
training step: 51200, total_loss: 2.5447998046875
training step: 51201, total_loss: 1.5062403678894043
training step: 51202, total_loss: 4.206020832061768
training step: 51203, total_loss: 1.5521643161773682
training step: 51204, total_loss: 2.31790828704834
training step: 51205, total_loss: 0.23206956684589386
training step: 51206, total_loss: 2.015467643737793
training step: 51207, total_loss: 2.4199490547180176
training step: 51208, total_loss: 0.4446321129798889
training step: 51209, total_loss: 0.2617637813091278
training step: 51210, total_loss: 2.4715640544891357
training step: 51211, total_loss: 1.7084108591079712
training step: 51212, total_loss: 1.1480727195739746
training step: 51213, total_loss: 2.659054756164551
training step: 51214, total_loss: 0.0022465744987130165
training step: 51215, total_loss: 0.1658790111541748
training step: 51216, total_loss: 0.719306230545044
training step: 51217, total_loss: 1.3525861501693726
training step: 51218, total_loss: 1.8188953399658203
training step: 51219, total_loss: 1.7725752592086792
training step: 51220, total_loss: 0.031155452132225037
training step: 51221, total_loss: 0.9412447810173035
training step: 51222, total_loss: 0.8146642446517944
training step: 51223, total_loss: 2.3675551414489746
training step: 51224, total_loss: 1.344430923461914
training step: 51225, total_loss: 0.0451296903192997
training step: 51226, total_loss: 0.027156587690114975
training step: 51227, total_loss: 1.1901237964630127
training step: 51228, total_loss: 2.452402114868164
training step: 51229, total_loss: 0.4722996652126312
training step: 51230, total_loss: 1.1284042596817017
training step: 51231, total_loss: 1.6332573890686035
training step: 51232, total_loss: 2.826082706451416
training step: 51233, total_loss: 2.2394866943359375
training step: 51234, total_loss: 1.7110168933868408
training step: 51235, total_loss: 2.4115712642669678
training step: 51236, total_loss: 0.822135329246521
training step: 51237, total_loss: 1.0344094038009644
training step: 51238, total_loss: 2.7093515396118164
training step: 51239, total_loss: 0.9854297637939453
training step: 51240, total_loss: 2.584106206893921
training step: 51241, total_loss: 2.2127254009246826
training step: 51242, total_loss: 1.4863901138305664
training step: 51243, total_loss: 1.4708802700042725
training step: 51244, total_loss: 0.4416824281215668
training step: 51245, total_loss: 1.575629472732544
training step: 51246, total_loss: 0.08014686405658722
training step: 51247, total_loss: 1.2245590686798096
training step: 51248, total_loss: 0.9445114135742188
training step: 51249, total_loss: 0.6466166973114014
training step: 51250, total_loss: 0.2680948078632355
training step: 51251, total_loss: 1.0074571371078491
training step: 51252, total_loss: 2.7391910552978516
training step: 51253, total_loss: 5.159641265869141
training step: 51254, total_loss: 0.9928981065750122
training step: 51255, total_loss: 0.0012292405590415
training step: 51256, total_loss: 2.7472116947174072
training step: 51257, total_loss: 2.8698983192443848
training step: 51258, total_loss: 4.478145599365234
training step: 51259, total_loss: 1.6618421077728271
training step: 51260, total_loss: 0.3278892934322357
training step: 51261, total_loss: 1.0577911138534546
training step: 51262, total_loss: 2.9224650859832764
training step: 51263, total_loss: 1.9005988836288452
training step: 51264, total_loss: 0.1686946451663971
training step: 51265, total_loss: 3.060389995574951
training step: 51266, total_loss: 1.6739935874938965
training step: 51267, total_loss: 3.088459014892578
training step: 51268, total_loss: 1.80422842502594
training step: 51269, total_loss: 1.3065458536148071
training step: 51270, total_loss: 1.1661607027053833
training step: 51271, total_loss: 1.138658881187439
training step: 51272, total_loss: 1.8369946479797363
training step: 51273, total_loss: 2.3636186122894287
training step: 51274, total_loss: 0.3492705225944519
training step: 51275, total_loss: 1.0525922775268555
training step: 51276, total_loss: 2.8610353469848633
training step: 51277, total_loss: 1.7560672760009766
training step: 51278, total_loss: 0.6686740517616272
training step: 51279, total_loss: 1.0709500312805176
training step: 51280, total_loss: 2.487626314163208
training step: 51281, total_loss: 0.6825695633888245
training step: 51282, total_loss: 1.0140897035598755
training step: 51283, total_loss: 3.9985766410827637
training step: 51284, total_loss: 4.450811386108398
training step: 51285, total_loss: 3.197885751724243
training step: 51286, total_loss: 4.17399787902832
training step: 51287, total_loss: 0.7326117157936096
training step: 51288, total_loss: 1.6360547542572021
training step: 51289, total_loss: 2.3400421142578125
training step: 51290, total_loss: 0.5367051362991333
training step: 51291, total_loss: 1.719678282737732
training step: 51292, total_loss: 1.8848772048950195
training step: 51293, total_loss: 1.991063117980957
training step: 51294, total_loss: 2.801887035369873
training step: 51295, total_loss: 0.5308812856674194
training step: 51296, total_loss: 3.346566677093506
training step: 51297, total_loss: 1.8529000282287598
training step: 51298, total_loss: 1.7489573955535889
training step: 51299, total_loss: 2.827760696411133
training step: 51300, total_loss: 2.9470138549804688
training step: 51301, total_loss: 1.626471996307373
training step: 51302, total_loss: 1.7221145629882812
training step: 51303, total_loss: 1.2646281719207764
training step: 51304, total_loss: 0.791895866394043
training step: 51305, total_loss: 1.3923890590667725
training step: 51306, total_loss: 1.7622231245040894
training step: 51307, total_loss: 0.37522512674331665
training step: 51308, total_loss: 0.34809744358062744
training step: 51309, total_loss: 0.0633346438407898
training step: 51310, total_loss: 0.4420176148414612
training step: 51311, total_loss: 1.1929495334625244
training step: 51312, total_loss: 1.157626748085022
training step: 51313, total_loss: 0.633182168006897
training step: 51314, total_loss: 3.627587080001831
training step: 51315, total_loss: 1.4721193313598633
training step: 51316, total_loss: 0.016843734309077263
training step: 51317, total_loss: 0.9649381041526794
training step: 51318, total_loss: 3.841761589050293
training step: 51319, total_loss: 1.9233797788619995
training step: 51320, total_loss: 0.6442086696624756
training step: 51321, total_loss: 2.1210670471191406
training step: 51322, total_loss: 2.079641342163086
training step: 51323, total_loss: 1.9668529033660889
training step: 51324, total_loss: 0.6090462803840637
training step: 51325, total_loss: 2.065608024597168
training step: 51326, total_loss: 1.2458304166793823
training step: 51327, total_loss: 0.28372395038604736
training step: 51328, total_loss: 3.2426846027374268
training step: 51329, total_loss: 0.6716236472129822
training step: 51330, total_loss: 0.4716307520866394
training step: 51331, total_loss: 0.6041174530982971
training step: 51332, total_loss: 0.5225217342376709
training step: 51333, total_loss: 1.2235236167907715
training step: 51334, total_loss: 1.8901352882385254
training step: 51335, total_loss: 1.2863584756851196
training step: 51336, total_loss: 1.7008519172668457
training step: 51337, total_loss: 1.2668633460998535
training step: 51338, total_loss: 1.052412509918213
training step: 51339, total_loss: 1.3994817733764648
training step: 51340, total_loss: 0.2342991828918457
training step: 51341, total_loss: 2.666893720626831
training step: 51342, total_loss: 0.7851194143295288
training step: 51343, total_loss: 2.7369425296783447
training step: 51344, total_loss: 1.216323733329773
training step: 51345, total_loss: 4.706669330596924
training step: 51346, total_loss: 1.8634494543075562
training step: 51347, total_loss: 6.0313334465026855
training step: 51348, total_loss: 1.3238651752471924
training step: 51349, total_loss: 0.6014629602432251
training step: 51350, total_loss: 1.7582635879516602
training step: 51351, total_loss: 3.3077235221862793
training step: 51352, total_loss: 0.3405199944972992
training step: 51353, total_loss: 3.4636240005493164
training step: 51354, total_loss: 0.03011586144566536
training step: 51355, total_loss: 0.8210487961769104
training step: 51356, total_loss: 1.2284119129180908
training step: 51357, total_loss: 1.3879048824310303
training step: 51358, total_loss: 1.6268329620361328
training step: 51359, total_loss: 1.0433018207550049
training step: 51360, total_loss: 0.5588688850402832
training step: 51361, total_loss: 0.343375563621521
training step: 51362, total_loss: 1.6717643737792969
training step: 51363, total_loss: 1.0397332906723022
training step: 51364, total_loss: 2.124504327774048
training step: 51365, total_loss: 0.9362858533859253
training step: 51366, total_loss: 0.9804638624191284
training step: 51367, total_loss: 2.6482911109924316
training step: 51368, total_loss: 2.2082724571228027
training step: 51369, total_loss: 2.805906295776367
training step: 51370, total_loss: 3.3555960655212402
training step: 51371, total_loss: 0.6970483660697937
training step: 51372, total_loss: 0.12348707020282745
training step: 51373, total_loss: 2.4082603454589844
training step: 51374, total_loss: 1.541334867477417
training step: 51375, total_loss: 0.00012545945355668664
training step: 51376, total_loss: 2.586724042892456
training step: 51377, total_loss: 1.7646641731262207
training step: 51378, total_loss: 2.9993081092834473
training step: 51379, total_loss: 0.6993009448051453
training step: 51380, total_loss: 2.1806111335754395
training step: 51381, total_loss: 0.9880426526069641
training step: 51382, total_loss: 0.955988883972168
training step: 51383, total_loss: 3.3373427391052246
training step: 51384, total_loss: 1.2837880849838257
training step: 51385, total_loss: 3.060758590698242
training step: 51386, total_loss: 1.3897004127502441
training step: 51387, total_loss: 2.1459615230560303
training step: 51388, total_loss: 1.799347162246704
training step: 51389, total_loss: 2.801971197128296
training step: 51390, total_loss: 1.7888330221176147
training step: 51391, total_loss: 2.762948513031006
training step: 51392, total_loss: 1.4737956523895264
training step: 51393, total_loss: 0.10580802708864212
training step: 51394, total_loss: 1.2825359106063843
training step: 51395, total_loss: 0.921007513999939
training step: 51396, total_loss: 1.70186448097229
training step: 51397, total_loss: 0.18569804728031158
training step: 51398, total_loss: 0.23101142048835754
training step: 51399, total_loss: 0.6636595726013184
training step: 51400, total_loss: 1.7245124578475952
training step: 51401, total_loss: 2.1366658210754395
training step: 51402, total_loss: 2.319758892059326
training step: 51403, total_loss: 3.6178135871887207
training step: 51404, total_loss: 1.764214038848877
training step: 51405, total_loss: 1.6777307987213135
training step: 51406, total_loss: 0.8219069838523865
training step: 51407, total_loss: 1.7685281038284302
training step: 51408, total_loss: 2.2165353298187256
training step: 51409, total_loss: 2.6837027072906494
training step: 51410, total_loss: 2.3162925243377686
training step: 51411, total_loss: 4.456295013427734
training step: 51412, total_loss: 0.5740925073623657
training step: 51413, total_loss: 4.0517778396606445
training step: 51414, total_loss: 0.6960408687591553
training step: 51415, total_loss: 3.224902629852295
training step: 51416, total_loss: 0.023801159113645554
training step: 51417, total_loss: 1.531419277191162
training step: 51418, total_loss: 0.9841461181640625
training step: 51419, total_loss: 1.4737273454666138
training step: 51420, total_loss: 1.7708730697631836
training step: 51421, total_loss: 0.9623960256576538
training step: 51422, total_loss: 0.9878865480422974
training step: 51423, total_loss: 1.9410417079925537
training step: 51424, total_loss: 1.3790236711502075
training step: 51425, total_loss: 0.2961474061012268
training step: 51426, total_loss: 0.11089707911014557
training step: 51427, total_loss: 0.908238410949707
training step: 51428, total_loss: 0.9570720791816711
training step: 51429, total_loss: 0.7405210733413696
training step: 51430, total_loss: 0.9524796009063721
training step: 51431, total_loss: 1.2061405181884766
training step: 51432, total_loss: 1.1186411380767822
training step: 51433, total_loss: 0.2586733102798462
training step: 51434, total_loss: 2.0939340591430664
training step: 51435, total_loss: 0.07705815136432648
training step: 51436, total_loss: 2.1149978637695312
training step: 51437, total_loss: 1.139449119567871
training step: 51438, total_loss: 1.423205852508545
training step: 51439, total_loss: 1.367330551147461
training step: 51440, total_loss: 0.0017760548507794738
training step: 51441, total_loss: 0.8288218975067139
training step: 51442, total_loss: 0.9312468767166138
training step: 51443, total_loss: 0.17193752527236938
training step: 51444, total_loss: 1.4286105632781982
training step: 51445, total_loss: 1.4573290348052979
training step: 51446, total_loss: 0.8390973806381226
training step: 51447, total_loss: 2.5483083724975586
training step: 51448, total_loss: 0.30851805210113525
training step: 51449, total_loss: 1.232270359992981
training step: 51450, total_loss: 4.2690815925598145
training step: 51451, total_loss: 0.5629016757011414
training step: 51452, total_loss: 0.9352049827575684
training step: 51453, total_loss: 1.5667561292648315
training step: 51454, total_loss: 0.2829391360282898
training step: 51455, total_loss: 0.0014685910427942872
training step: 51456, total_loss: 1.3648130893707275
training step: 51457, total_loss: 1.7638001441955566
training step: 51458, total_loss: 1.1298097372055054
training step: 51459, total_loss: 1.2874705791473389
training step: 51460, total_loss: 4.078129291534424
training step: 51461, total_loss: 0.11278791725635529
training step: 51462, total_loss: 4.150380611419678
training step: 51463, total_loss: 0.30310216546058655
training step: 51464, total_loss: 0.32144054770469666
training step: 51465, total_loss: 3.618631362915039
training step: 51466, total_loss: 0.11844746768474579
training step: 51467, total_loss: 1.8672572374343872
training step: 51468, total_loss: 1.6706743240356445
training step: 51469, total_loss: 1.686073660850525
training step: 51470, total_loss: 1.3870484828948975
training step: 51471, total_loss: 1.5316238403320312
training step: 51472, total_loss: 0.499740332365036
training step: 51473, total_loss: 1.8287127017974854
training step: 51474, total_loss: 2.103740692138672
training step: 51475, total_loss: 1.1582026481628418
training step: 51476, total_loss: 2.621887683868408
training step: 51477, total_loss: 2.881164073944092
training step: 51478, total_loss: 0.040486644953489304
training step: 51479, total_loss: 0.7598905563354492
training step: 51480, total_loss: 2.788018226623535
training step: 51481, total_loss: 3.085165023803711
training step: 51482, total_loss: 1.9074673652648926
training step: 51483, total_loss: 1.9324920177459717
training step: 51484, total_loss: 1.9710673093795776
training step: 51485, total_loss: 2.58048152923584
training step: 51486, total_loss: 1.619005799293518
training step: 51487, total_loss: 2.0515048503875732
training step: 51488, total_loss: 3.116778612136841
training step: 51489, total_loss: 0.7510868310928345
training step: 51490, total_loss: 2.3485028743743896
training step: 51491, total_loss: 1.4495785236358643
training step: 51492, total_loss: 0.041624486446380615
training step: 51493, total_loss: 0.5104694962501526
training step: 51494, total_loss: 2.7534022331237793
training step: 51495, total_loss: 1.6181647777557373
training step: 51496, total_loss: 4.149506092071533
training step: 51497, total_loss: 1.6358180046081543
training step: 51498, total_loss: 0.8789939880371094
training step: 51499, total_loss: 0.26243510842323303
training step: 51500, total_loss: 2.973470449447632
training step: 51501, total_loss: 1.4043904542922974
training step: 51502, total_loss: 2.0688936710357666
training step: 51503, total_loss: 2.188199520111084
training step: 51504, total_loss: 1.3381633758544922
training step: 51505, total_loss: 5.325828552246094
training step: 51506, total_loss: 1.5050281286239624
training step: 51507, total_loss: 1.0608261823654175
training step: 51508, total_loss: 1.851264476776123
training step: 51509, total_loss: 2.7396392822265625
training step: 51510, total_loss: 2.1812796592712402
training step: 51511, total_loss: 0.31239545345306396
training step: 51512, total_loss: 1.9308518171310425
training step: 51513, total_loss: 2.286146640777588
training step: 51514, total_loss: 3.883763551712036
training step: 51515, total_loss: 2.3551456928253174
training step: 51516, total_loss: 1.3964431285858154
training step: 51517, total_loss: 0.3903046250343323
training step: 51518, total_loss: 1.3298789262771606
training step: 51519, total_loss: 1.6710909605026245
training step: 51520, total_loss: 1.3326430320739746
training step: 51521, total_loss: 0.22473949193954468
training step: 51522, total_loss: 3.1777596473693848
training step: 51523, total_loss: 0.4159718155860901
training step: 51524, total_loss: 0.07925435155630112
training step: 51525, total_loss: 1.3099616765975952
training step: 51526, total_loss: 1.5591416358947754
training step: 51527, total_loss: 1.721524953842163
training step: 51528, total_loss: 0.2812695801258087
training step: 51529, total_loss: 1.0260188579559326
training step: 51530, total_loss: 1.994537591934204
training step: 51531, total_loss: 1.4453967809677124
training step: 51532, total_loss: 0.9984214305877686
training step: 51533, total_loss: 1.0947794914245605
training step: 51534, total_loss: 1.944525957107544
training step: 51535, total_loss: 1.7029962539672852
training step: 51536, total_loss: 1.1168396472930908
training step: 51537, total_loss: 2.825910806655884
training step: 51538, total_loss: 0.18804800510406494
training step: 51539, total_loss: 2.3985390663146973
training step: 51540, total_loss: 1.1805793046951294
training step: 51541, total_loss: 1.6884324550628662
training step: 51542, total_loss: 1.336705207824707
training step: 51543, total_loss: 1.4085872173309326
training step: 51544, total_loss: 0.3355749845504761
training step: 51545, total_loss: 1.1405372619628906
training step: 51546, total_loss: 0.12188281118869781
training step: 51547, total_loss: 1.9210805892944336
training step: 51548, total_loss: 1.4558988809585571
training step: 51549, total_loss: 2.3366293907165527
training step: 51550, total_loss: 0.2019074708223343
training step: 51551, total_loss: 1.9263882637023926
training step: 51552, total_loss: 1.2607804536819458
training step: 51553, total_loss: 0.2229139506816864
training step: 51554, total_loss: 1.0999748706817627
training step: 51555, total_loss: 0.8346035480499268
training step: 51556, total_loss: 0.5681923627853394
training step: 51557, total_loss: 2.2032570838928223
training step: 51558, total_loss: 1.0735174417495728
training step: 51559, total_loss: 0.5005180239677429
training step: 51560, total_loss: 3.988920211791992
training step: 51561, total_loss: 1.1683015823364258
training step: 51562, total_loss: 1.5493344068527222
training step: 51563, total_loss: 0.5015629529953003
training step: 51564, total_loss: 0.8772616386413574
training step: 51565, total_loss: 3.3021883964538574
training step: 51566, total_loss: 0.9401957988739014
training step: 51567, total_loss: 1.5302925109863281
training step: 51568, total_loss: 0.7878127694129944
training step: 51569, total_loss: 3.512099266052246
training step: 51570, total_loss: 3.0267269611358643
training step: 51571, total_loss: 3.2044448852539062
training step: 51572, total_loss: 2.6536335945129395
training step: 51573, total_loss: 0.7040281891822815
training step: 51574, total_loss: 1.2801220417022705
training step: 51575, total_loss: 4.2586188316345215
training step: 51576, total_loss: 0.6243176460266113
training step: 51577, total_loss: 4.326910018920898
training step: 51578, total_loss: 2.243788242340088
training step: 51579, total_loss: 2.9148712158203125
training step: 51580, total_loss: 3.0156540870666504
training step: 51581, total_loss: 2.603407621383667
training step: 51582, total_loss: 3.3264389038085938
training step: 51583, total_loss: 0.5252736210823059
training step: 51584, total_loss: 2.8081109523773193
training step: 51585, total_loss: 1.7750364542007446
training step: 51586, total_loss: 0.3166152238845825
training step: 51587, total_loss: 0.5685497522354126
training step: 51588, total_loss: 1.6349475383758545
training step: 51589, total_loss: 1.1740498542785645
training step: 51590, total_loss: 7.905780792236328
training step: 51591, total_loss: 4.173643112182617
training step: 51592, total_loss: 3.91705060005188
training step: 51593, total_loss: 2.1611599922180176
training step: 51594, total_loss: 1.8376065492630005
training step: 51595, total_loss: 1.270958662033081
training step: 51596, total_loss: 0.25060850381851196
training step: 51597, total_loss: 1.558835506439209
training step: 51598, total_loss: 1.5887744426727295
training step: 51599, total_loss: 0.8126519322395325
training step: 51600, total_loss: 3.1545629501342773
training step: 51601, total_loss: 1.1466362476348877
training step: 51602, total_loss: 1.969503402709961
training step: 51603, total_loss: 0.006865604780614376
training step: 51604, total_loss: 1.7508370876312256
training step: 51605, total_loss: 1.7286581993103027
training step: 51606, total_loss: 0.3494904637336731
training step: 51607, total_loss: 0.013632446527481079
training step: 51608, total_loss: 0.6607391238212585
training step: 51609, total_loss: 0.20472490787506104
training step: 51610, total_loss: 5.739256858825684
training step: 51611, total_loss: 1.3111000061035156
training step: 51612, total_loss: 0.017807641997933388
training step: 51613, total_loss: 0.000696888891980052
training step: 51614, total_loss: 0.4041151702404022
training step: 51615, total_loss: 0.05529235675930977
training step: 51616, total_loss: 2.7925891876220703
training step: 51617, total_loss: 0.15050141513347626
training step: 51618, total_loss: 1.8204960823059082
training step: 51619, total_loss: 3.256293296813965
training step: 51620, total_loss: 2.8222949504852295
training step: 51621, total_loss: 0.7081246376037598
training step: 51622, total_loss: 1.1047871112823486
training step: 51623, total_loss: 3.545567035675049
training step: 51624, total_loss: 0.7415887117385864
training step: 51625, total_loss: 0.6317669153213501
training step: 51626, total_loss: 3.5062248706817627
training step: 51627, total_loss: 3.9725372791290283
training step: 51628, total_loss: 2.092329263687134
training step: 51629, total_loss: 1.434906005859375
training step: 51630, total_loss: 1.7226070165634155
training step: 51631, total_loss: 1.7353551387786865
training step: 51632, total_loss: 2.347011089324951
training step: 51633, total_loss: 1.7488901615142822
training step: 51634, total_loss: 1.588844656944275
training step: 51635, total_loss: 0.8138580918312073
training step: 51636, total_loss: 0.1055002212524414
training step: 51637, total_loss: 2.4979043006896973
training step: 51638, total_loss: 0.23137497901916504
training step: 51639, total_loss: 2.044725179672241
training step: 51640, total_loss: 2.0837812423706055
training step: 51641, total_loss: 0.579569935798645
training step: 51642, total_loss: 2.9596877098083496
training step: 51643, total_loss: 1.2051688432693481
training step: 51644, total_loss: 4.353285312652588
training step: 51645, total_loss: 0.9178984761238098
training step: 51646, total_loss: 1.003178358078003
training step: 51647, total_loss: 1.2569773197174072
training step: 51648, total_loss: 1.2575082778930664
training step: 51649, total_loss: 2.154816150665283
training step: 51650, total_loss: 1.153724193572998
training step: 51651, total_loss: 0.14929425716400146
training step: 51652, total_loss: 1.4421463012695312
training step: 51653, total_loss: 1.6910841464996338
training step: 51654, total_loss: 0.8582799434661865
training step: 51655, total_loss: 1.7571642398834229
training step: 51656, total_loss: 1.913763165473938
training step: 51657, total_loss: 2.046572208404541
training step: 51658, total_loss: 2.4191949367523193
training step: 51659, total_loss: 0.9602406024932861
training step: 51660, total_loss: 0.335650771856308
training step: 51661, total_loss: 0.732277512550354
training step: 51662, total_loss: 1.2640831470489502
training step: 51663, total_loss: 1.7610609531402588
training step: 51664, total_loss: 2.8713080883026123
training step: 51665, total_loss: 1.6055612564086914
training step: 51666, total_loss: 2.1662650108337402
training step: 51667, total_loss: 0.5565921664237976
training step: 51668, total_loss: 0.6792026162147522
training step: 51669, total_loss: 1.7215663194656372
training step: 51670, total_loss: 4.498934745788574
training step: 51671, total_loss: 0.5068469643592834
training step: 51672, total_loss: 0.5936934351921082
training step: 51673, total_loss: 2.126659631729126
training step: 51674, total_loss: 1.0839104652404785
training step: 51675, total_loss: 0.8227484226226807
training step: 51676, total_loss: 0.8397955894470215
training step: 51677, total_loss: 2.2642970085144043
training step: 51678, total_loss: 0.7436118125915527
training step: 51679, total_loss: 3.3556935787200928
training step: 51680, total_loss: 0.04852331429719925
training step: 51681, total_loss: 0.4668876528739929
training step: 51682, total_loss: 1.0549969673156738
training step: 51683, total_loss: 0.2627773880958557
training step: 51684, total_loss: 3.567025661468506
training step: 51685, total_loss: 0.8342463970184326
training step: 51686, total_loss: 1.9592156410217285
training step: 51687, total_loss: 2.672567367553711
training step: 51688, total_loss: 0.47955816984176636
training step: 51689, total_loss: 2.271798610687256
training step: 51690, total_loss: 0.32637494802474976
training step: 51691, total_loss: 1.222738265991211
training step: 51692, total_loss: 4.234919548034668
training step: 51693, total_loss: 0.8562902808189392
training step: 51694, total_loss: 0.734044075012207
training step: 51695, total_loss: 0.5100566148757935
training step: 51696, total_loss: 0.8021621108055115
training step: 51697, total_loss: 3.1294796466827393
training step: 51698, total_loss: 3.7541770935058594
training step: 51699, total_loss: 0.0009766637813299894
training step: 51700, total_loss: 0.8597092628479004
training step: 51701, total_loss: 0.7959554195404053
training step: 51702, total_loss: 1.3440985679626465
training step: 51703, total_loss: 0.48836350440979004
training step: 51704, total_loss: 2.2010889053344727
training step: 51705, total_loss: 0.38907867670059204
training step: 51706, total_loss: 0.00015269537107087672
training step: 51707, total_loss: 0.2406468391418457
training step: 51708, total_loss: 0.017345957458019257
training step: 51709, total_loss: 1.566650390625
training step: 51710, total_loss: 1.1221964359283447
training step: 51711, total_loss: 0.322028785943985
training step: 51712, total_loss: 4.326817989349365
training step: 51713, total_loss: 2.594925880432129
training step: 51714, total_loss: 2.7873597145080566
training step: 51715, total_loss: 1.9457777738571167
training step: 51716, total_loss: 1.1721168756484985
training step: 51717, total_loss: 0.7108436226844788
training step: 51718, total_loss: 1.4018927812576294
training step: 51719, total_loss: 3.187180519104004
training step: 51720, total_loss: 0.05532189458608627
training step: 51721, total_loss: 0.4164169430732727
training step: 51722, total_loss: 0.16473069787025452
training step: 51723, total_loss: 3.1037445068359375
training step: 51724, total_loss: 0.890028178691864
training step: 51725, total_loss: 0.8269172310829163
training step: 51726, total_loss: 0.7512659430503845
training step: 51727, total_loss: 4.0791916847229
training step: 51728, total_loss: 0.1790146827697754
training step: 51729, total_loss: 1.369401454925537
training step: 51730, total_loss: 2.69431209564209
training step: 51731, total_loss: 1.1461396217346191
training step: 51732, total_loss: 2.1477906703948975
training step: 51733, total_loss: 0.04896072670817375
training step: 51734, total_loss: 3.640106439590454
training step: 51735, total_loss: 1.7069692611694336
training step: 51736, total_loss: 2.9786691665649414
training step: 51737, total_loss: 0.7599295377731323
training step: 51738, total_loss: 1.4474643468856812
training step: 51739, total_loss: 1.1563061475753784
training step: 51740, total_loss: 1.3051255941390991
training step: 51741, total_loss: 5.430809497833252
training step: 51742, total_loss: 2.8142080307006836
training step: 51743, total_loss: 1.1016781330108643
training step: 51744, total_loss: 1.7339940071105957
training step: 51745, total_loss: 0.8895111680030823
training step: 51746, total_loss: 4.717310905456543
training step: 51747, total_loss: 1.3097907304763794
training step: 51748, total_loss: 1.9152600765228271
training step: 51749, total_loss: 2.2480454444885254
training step: 51750, total_loss: 0.9529967904090881
training step: 51751, total_loss: 3.1012954711914062
training step: 51752, total_loss: 0.2801167368888855
training step: 51753, total_loss: 4.529588222503662
training step: 51754, total_loss: 1.3945461511611938
training step: 51755, total_loss: 0.23447734117507935
training step: 51756, total_loss: 1.4946407079696655
training step: 51757, total_loss: 1.5799710750579834
training step: 51758, total_loss: 2.3902230262756348
training step: 51759, total_loss: 0.6101127862930298
training step: 51760, total_loss: 0.8791539669036865
training step: 51761, total_loss: 2.5857770442962646
training step: 51762, total_loss: 3.912165880203247
training step: 51763, total_loss: 2.0077171325683594
training step: 51764, total_loss: 1.2592353820800781
training step: 51765, total_loss: 0.40710920095443726
training step: 51766, total_loss: 1.3850942850112915
training step: 51767, total_loss: 1.8761485815048218
training step: 51768, total_loss: 0.8616577386856079
training step: 51769, total_loss: 1.189300298690796
training step: 51770, total_loss: 0.4728078246116638
training step: 51771, total_loss: 0.06782343238592148
training step: 51772, total_loss: 1.3754425048828125
training step: 51773, total_loss: 2.890470504760742
training step: 51774, total_loss: 2.399747610092163
training step: 51775, total_loss: 0.09710574895143509
training step: 51776, total_loss: 2.055469036102295
training step: 51777, total_loss: 0.030057910829782486
training step: 51778, total_loss: 2.170865535736084
training step: 51779, total_loss: 1.3444960117340088
training step: 51780, total_loss: 0.4120199978351593
training step: 51781, total_loss: 0.9411077499389648
training step: 51782, total_loss: 2.6977438926696777
training step: 51783, total_loss: 1.1131706237792969
training step: 51784, total_loss: 3.090043067932129
training step: 51785, total_loss: 1.2010042667388916
training step: 51786, total_loss: 1.6852691173553467
training step: 51787, total_loss: 0.7796261310577393
training step: 51788, total_loss: 1.392494797706604
training step: 51789, total_loss: 1.6594105958938599
training step: 51790, total_loss: 0.3105221390724182
training step: 51791, total_loss: 0.11427902430295944
training step: 51792, total_loss: 2.0036585330963135
training step: 51793, total_loss: 0.3636469542980194
training step: 51794, total_loss: 1.9787440299987793
training step: 51795, total_loss: 0.7629976272583008
training step: 51796, total_loss: 0.24927830696105957
training step: 51797, total_loss: 1.752977967262268
training step: 51798, total_loss: 1.49222993850708
training step: 51799, total_loss: 1.0775002241134644
training step: 51800, total_loss: 2.128328561782837
training step: 51801, total_loss: 2.753706693649292
training step: 51802, total_loss: 1.0374939441680908
training step: 51803, total_loss: 0.11401151865720749
training step: 51804, total_loss: 3.054978847503662
training step: 51805, total_loss: 1.2182035446166992
training step: 51806, total_loss: 1.789255142211914
training step: 51807, total_loss: 0.645167887210846
training step: 51808, total_loss: 0.6674891710281372
training step: 51809, total_loss: 2.2566304206848145
training step: 51810, total_loss: 0.423814594745636
training step: 51811, total_loss: 0.013157488778233528
training step: 51812, total_loss: 1.5491480827331543
training step: 51813, total_loss: 3.8710708618164062
training step: 51814, total_loss: 3.6491799354553223
training step: 51815, total_loss: 0.9243096113204956
training step: 51816, total_loss: 0.4953274130821228
training step: 51817, total_loss: 1.0020506381988525
training step: 51818, total_loss: 0.4049021601676941
training step: 51819, total_loss: 3.0783138275146484
training step: 51820, total_loss: 3.0505123138427734
training step: 51821, total_loss: 5.009757995605469
training step: 51822, total_loss: 2.2675952911376953
training step: 51823, total_loss: 0.004650572780519724
training step: 51824, total_loss: 1.9278937578201294
training step: 51825, total_loss: 0.8136441111564636
training step: 51826, total_loss: 1.4724078178405762
training step: 51827, total_loss: 0.20895035564899445
training step: 51828, total_loss: 0.684043288230896
training step: 51829, total_loss: 1.130784034729004
training step: 51830, total_loss: 2.1695432662963867
training step: 51831, total_loss: 0.9511947631835938
training step: 51832, total_loss: 3.1216773986816406
training step: 51833, total_loss: 1.7966022491455078
training step: 51834, total_loss: 2.4964427947998047
training step: 51835, total_loss: 1.2066508531570435
training step: 51836, total_loss: 3.23402738571167
training step: 51837, total_loss: 3.1460213661193848
training step: 51838, total_loss: 1.782181739807129
training step: 51839, total_loss: 0.12282641232013702
training step: 51840, total_loss: 0.015727397054433823
training step: 51841, total_loss: 3.8864665031433105
training step: 51842, total_loss: 1.3316032886505127
training step: 51843, total_loss: 1.2857095003128052
training step: 51844, total_loss: 0.4269578456878662
training step: 51845, total_loss: 0.5705715417861938
training step: 51846, total_loss: 0.04002562165260315
training step: 51847, total_loss: 2.2154335975646973
training step: 51848, total_loss: 0.669119656085968
training step: 51849, total_loss: 3.17510724067688
training step: 51850, total_loss: 0.026632551103830338
training step: 51851, total_loss: 0.8182134628295898
training step: 51852, total_loss: 3.2984671592712402
training step: 51853, total_loss: 6.490703582763672
training step: 51854, total_loss: 4.117738246917725
training step: 51855, total_loss: 0.3627353012561798
training step: 51856, total_loss: 1.9619125127792358
training step: 51857, total_loss: 0.7905793786048889
training step: 51858, total_loss: 1.160030722618103
training step: 51859, total_loss: 6.973215579986572
training step: 51860, total_loss: 0.7313216328620911
training step: 51861, total_loss: 0.7141218185424805
training step: 51862, total_loss: 1.6316356658935547
training step: 51863, total_loss: 3.7626686096191406
training step: 51864, total_loss: 1.190806269645691
training step: 51865, total_loss: 1.3137450218200684
training step: 51866, total_loss: 2.088937520980835
training step: 51867, total_loss: 0.6417457461357117
training step: 51868, total_loss: 0.06513515114784241
training step: 51869, total_loss: 2.293116569519043
training step: 51870, total_loss: 0.32360413670539856
training step: 51871, total_loss: 1.5575416088104248
training step: 51872, total_loss: 0.7245074510574341
training step: 51873, total_loss: 1.2763736248016357
training step: 51874, total_loss: 0.5615192651748657
training step: 51875, total_loss: 1.2285535335540771
training step: 51876, total_loss: 0.6578236818313599
training step: 51877, total_loss: 0.3712846040725708
training step: 51878, total_loss: 2.057887077331543
training step: 51879, total_loss: 0.2651486098766327
training step: 51880, total_loss: 1.79297935962677
training step: 51881, total_loss: 0.7233043909072876
training step: 51882, total_loss: 2.907761573791504
training step: 51883, total_loss: 2.347506284713745
training step: 51884, total_loss: 1.5448334217071533
training step: 51885, total_loss: 0.7785117030143738
training step: 51886, total_loss: 0.6105313897132874
training step: 51887, total_loss: 0.007622121833264828
training step: 51888, total_loss: 2.200615644454956
training step: 51889, total_loss: 0.3803638815879822
training step: 51890, total_loss: 1.6235175132751465
training step: 51891, total_loss: 1.9189163446426392
training step: 51892, total_loss: 4.154008865356445
training step: 51893, total_loss: 3.989461660385132
training step: 51894, total_loss: 0.7912595868110657
training step: 51895, total_loss: 2.0410306453704834
training step: 51896, total_loss: 4.97614049911499
training step: 51897, total_loss: 1.4226844310760498
training step: 51898, total_loss: 0.04494417458772659
training step: 51899, total_loss: 0.12502524256706238
training step: 51900, total_loss: 2.8080201148986816
training step: 51901, total_loss: 0.7112131714820862
training step: 51902, total_loss: 1.5073364973068237
training step: 51903, total_loss: 2.3719072341918945
training step: 51904, total_loss: 2.3944239616394043
training step: 51905, total_loss: 1.6853467226028442
training step: 51906, total_loss: 0.3039657473564148
training step: 51907, total_loss: 1.884903907775879
training step: 51908, total_loss: 0.9815647602081299
training step: 51909, total_loss: 0.03140440955758095
training step: 51910, total_loss: 1.8852097988128662
training step: 51911, total_loss: 0.24903243780136108
training step: 51912, total_loss: 3.118460178375244
training step: 51913, total_loss: 0.039942484349012375
training step: 51914, total_loss: 0.4255019426345825
training step: 51915, total_loss: 1.0611425638198853
training step: 51916, total_loss: 0.30402612686157227
training step: 51917, total_loss: 0.704054594039917
training step: 51918, total_loss: 0.9047828912734985
training step: 51919, total_loss: 0.8615458011627197
training step: 51920, total_loss: 1.3606421947479248
training step: 51921, total_loss: 0.17706261575222015
training step: 51922, total_loss: 2.355278968811035
training step: 51923, total_loss: 2.233733654022217
training step: 51924, total_loss: 2.2446937561035156INFO:tensorflow:Writing predictions to: test_output/predictions_52000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_52000.json

training step: 51925, total_loss: 1.0497750043869019
training step: 51926, total_loss: 0.7061660289764404
training step: 51927, total_loss: 0.48457229137420654
training step: 51928, total_loss: 0.4409458637237549
training step: 51929, total_loss: 0.01216825470328331
training step: 51930, total_loss: 2.848684549331665
training step: 51931, total_loss: 1.7756011486053467
training step: 51932, total_loss: 0.12297534942626953
training step: 51933, total_loss: 3.3494272232055664
training step: 51934, total_loss: 0.9724597930908203
training step: 51935, total_loss: 1.0314319133758545
training step: 51936, total_loss: 0.9144080877304077
training step: 51937, total_loss: 1.7398631572723389
training step: 51938, total_loss: 1.0443334579467773
training step: 51939, total_loss: 0.23313570022583008
training step: 51940, total_loss: 0.8229953050613403
training step: 51941, total_loss: 1.0073609352111816
training step: 51942, total_loss: 1.706843376159668
training step: 51943, total_loss: 0.6735985279083252
training step: 51944, total_loss: 3.2274038791656494
training step: 51945, total_loss: 2.9679837226867676
training step: 51946, total_loss: 2.0177955627441406
training step: 51947, total_loss: 0.9326935410499573
training step: 51948, total_loss: 1.826459288597107
training step: 51949, total_loss: 6.266251087188721
training step: 51950, total_loss: 1.5078279972076416
training step: 51951, total_loss: 2.2128286361694336
training step: 51952, total_loss: 2.5238964557647705
training step: 51953, total_loss: 2.4248435497283936
training step: 51954, total_loss: 0.06607312709093094
training step: 51955, total_loss: 2.1954970359802246
training step: 51956, total_loss: 0.7509349584579468
training step: 51957, total_loss: 1.4033865928649902
training step: 51958, total_loss: 0.08672487735748291
training step: 51959, total_loss: 2.838409900665283
training step: 51960, total_loss: 2.5929629802703857
training step: 51961, total_loss: 2.458289384841919
training step: 51962, total_loss: 2.268045425415039
training step: 51963, total_loss: 2.7322168350219727
training step: 51964, total_loss: 2.5721092224121094
training step: 51965, total_loss: 3.7054200172424316
training step: 51966, total_loss: 3.0709943771362305
training step: 51967, total_loss: 1.1959973573684692
training step: 51968, total_loss: 3.317385673522949
training step: 51969, total_loss: 0.0016108581330627203
training step: 51970, total_loss: 2.8050320148468018
training step: 51971, total_loss: 3.018294334411621
training step: 51972, total_loss: 0.1503865122795105
training step: 51973, total_loss: 0.15063953399658203
training step: 51974, total_loss: 1.6392302513122559
training step: 51975, total_loss: 0.757996678352356
training step: 51976, total_loss: 1.691967487335205
training step: 51977, total_loss: 2.444575548171997
training step: 51978, total_loss: 1.6972293853759766
training step: 51979, total_loss: 3.375945806503296
training step: 51980, total_loss: 6.268542766571045
training step: 51981, total_loss: 2.283729076385498
training step: 51982, total_loss: 0.9765902757644653
training step: 51983, total_loss: 2.3308634757995605
training step: 51984, total_loss: 3.3866934776306152
training step: 51985, total_loss: 1.660088300704956
training step: 51986, total_loss: 0.3687604069709778
training step: 51987, total_loss: 0.04609786346554756
training step: 51988, total_loss: 0.03946477174758911
training step: 51989, total_loss: 1.1535693407058716
training step: 51990, total_loss: 1.1292057037353516
training step: 51991, total_loss: 4.054595470428467
training step: 51992, total_loss: 4.560853958129883
training step: 51993, total_loss: 0.18837329745292664
training step: 51994, total_loss: 2.650250196456909
training step: 51995, total_loss: 0.376952588558197
training step: 51996, total_loss: 1.819928765296936
training step: 51997, total_loss: 4.55997896194458
training step: 51998, total_loss: 1.5934820175170898
training step: 51999, total_loss: 0.03121144510805607
training step: 52000, total_loss: 0.6616275310516357
epoch finished! shuffle=False
evaluation: 16000, total_loss: 1.8348524570465088, f1: 55.25198531852083, followup: 32.89213448957858, yesno: 67.82291191236878, heq: 50.31188194127491, dheq: 3.2

Model saved in path test_output//model_52000.ckpt
training step: 52001, total_loss: 1.5709033012390137
training step: 52002, total_loss: 1.0013028383255005
training step: 52003, total_loss: 3.066581964492798
training step: 52004, total_loss: 0.552193820476532
training step: 52005, total_loss: 2.0141689777374268
training step: 52006, total_loss: 1.801535964012146
training step: 52007, total_loss: 1.5053297281265259
training step: 52008, total_loss: 4.3122029304504395
training step: 52009, total_loss: 1.8284834623336792
training step: 52010, total_loss: 2.0819287300109863
training step: 52011, total_loss: 1.4960211515426636
training step: 52012, total_loss: 0.0031507520470768213
training step: 52013, total_loss: 3.3253183364868164
training step: 52014, total_loss: 0.016388531774282455
training step: 52015, total_loss: 4.055605411529541
training step: 52016, total_loss: 1.4331990480422974
training step: 52017, total_loss: 0.8541739583015442
training step: 52018, total_loss: 0.5167713165283203
training step: 52019, total_loss: 3.999363422393799
training step: 52020, total_loss: 0.5456019043922424
training step: 52021, total_loss: 1.0613281726837158
training step: 52022, total_loss: 1.0466067790985107
training step: 52023, total_loss: 1.9596806764602661
training step: 52024, total_loss: 1.172170877456665
training step: 52025, total_loss: 0.3993409276008606
training step: 52026, total_loss: 1.8667024374008179
training step: 52027, total_loss: 1.2558969259262085
training step: 52028, total_loss: 0.3781099021434784
training step: 52029, total_loss: 0.1512182056903839
training step: 52030, total_loss: 1.9647289514541626
training step: 52031, total_loss: 0.11306539177894592
training step: 52032, total_loss: 1.6985770463943481
training step: 52033, total_loss: 1.5713748931884766
training step: 52034, total_loss: 2.88021183013916
training step: 52035, total_loss: 0.4668152928352356
training step: 52036, total_loss: 0.644462525844574
training step: 52037, total_loss: 1.0252124071121216
training step: 52038, total_loss: 2.912553071975708
training step: 52039, total_loss: 2.8701930046081543
training step: 52040, total_loss: 0.32682713866233826
training step: 52041, total_loss: 3.5919909477233887
training step: 52042, total_loss: 4.9675092697143555
training step: 52043, total_loss: 4.6018877029418945
training step: 52044, total_loss: 1.993390440940857
training step: 52045, total_loss: 1.066442608833313
training step: 52046, total_loss: 0.6105222105979919
training step: 52047, total_loss: 1.9038422107696533
training step: 52048, total_loss: 1.5007517337799072
training step: 52049, total_loss: 2.9698104858398438
training step: 52050, total_loss: 1.7822229862213135
training step: 52051, total_loss: 2.179741382598877
training step: 52052, total_loss: 4.620067119598389
training step: 52053, total_loss: 2.1150898933410645
training step: 52054, total_loss: 0.0062744528986513615
training step: 52055, total_loss: 1.8016369342803955
training step: 52056, total_loss: 2.5987730026245117
training step: 52057, total_loss: 0.4833920896053314
training step: 52058, total_loss: 1.4452033042907715
training step: 52059, total_loss: 1.943516731262207
training step: 52060, total_loss: 4.877100467681885
training step: 52061, total_loss: 2.4237921237945557
training step: 52062, total_loss: 0.9310930371284485
training step: 52063, total_loss: 2.0630810260772705
training step: 52064, total_loss: 0.1345796436071396
training step: 52065, total_loss: 1.782796859741211
training step: 52066, total_loss: 0.22905510663986206
training step: 52067, total_loss: 1.3951067924499512
training step: 52068, total_loss: 0.8426742553710938
training step: 52069, total_loss: 1.6088624000549316
training step: 52070, total_loss: 2.3633499145507812
training step: 52071, total_loss: 0.8094159364700317
training step: 52072, total_loss: 1.2089990377426147
training step: 52073, total_loss: 2.407362461090088
training step: 52074, total_loss: 2.4251208305358887
training step: 52075, total_loss: 0.12375887483358383
training step: 52076, total_loss: 2.3152499198913574
training step: 52077, total_loss: 1.8630802631378174
training step: 52078, total_loss: 2.6778345108032227
training step: 52079, total_loss: 0.9643593430519104
training step: 52080, total_loss: 1.7536084651947021
training step: 52081, total_loss: 1.7682870626449585
training step: 52082, total_loss: 1.353345513343811
training step: 52083, total_loss: 3.433267116546631
training step: 52084, total_loss: 1.2540833950042725
training step: 52085, total_loss: 2.3100204467773438
training step: 52086, total_loss: 2.3491809368133545
training step: 52087, total_loss: 0.7709258198738098
training step: 52088, total_loss: 0.6893109083175659
training step: 52089, total_loss: 2.1297481060028076
training step: 52090, total_loss: 5.457103729248047
training step: 52091, total_loss: 0.778171181678772
training step: 52092, total_loss: 0.41021105647087097
training step: 52093, total_loss: 3.2913925647735596
training step: 52094, total_loss: 0.6357760429382324
training step: 52095, total_loss: 0.4561547040939331
training step: 52096, total_loss: 2.5873005390167236
training step: 52097, total_loss: 1.7786136865615845
training step: 52098, total_loss: 1.5695818662643433
training step: 52099, total_loss: 2.4938766956329346
training step: 52100, total_loss: 0.3574138879776001
training step: 52101, total_loss: 0.20734304189682007
training step: 52102, total_loss: 0.9802959561347961
training step: 52103, total_loss: 3.032698154449463
training step: 52104, total_loss: 0.6516493558883667
training step: 52105, total_loss: 4.452337265014648
training step: 52106, total_loss: 3.1142306327819824
training step: 52107, total_loss: 2.270209312438965
training step: 52108, total_loss: 1.5649645328521729
training step: 52109, total_loss: 0.9350820779800415
training step: 52110, total_loss: 0.5558090806007385
training step: 52111, total_loss: 2.193739891052246
training step: 52112, total_loss: 0.1220582127571106
training step: 52113, total_loss: 1.7052384614944458
training step: 52114, total_loss: 1.7148441076278687
training step: 52115, total_loss: 2.876779794692993
training step: 52116, total_loss: 0.9134122729301453
training step: 52117, total_loss: 2.953510046005249
training step: 52118, total_loss: 0.06310991942882538
training step: 52119, total_loss: 0.5451136827468872
training step: 52120, total_loss: 2.3688061237335205
training step: 52121, total_loss: 3.273145914077759
training step: 52122, total_loss: 0.03403611108660698
training step: 52123, total_loss: 3.1851961612701416
training step: 52124, total_loss: 0.4813191294670105
training step: 52125, total_loss: 3.951205015182495
training step: 52126, total_loss: 0.9995745420455933
training step: 52127, total_loss: 4.444613456726074
training step: 52128, total_loss: 0.11267437040805817
training step: 52129, total_loss: 2.920170307159424
training step: 52130, total_loss: 3.1560051441192627
training step: 52131, total_loss: 1.7009907960891724
training step: 52132, total_loss: 0.6942763328552246
training step: 52133, total_loss: 0.1392306238412857
training step: 52134, total_loss: 2.3612430095672607
training step: 52135, total_loss: 0.5732174515724182
training step: 52136, total_loss: 0.1036243885755539
training step: 52137, total_loss: 2.403531312942505
training step: 52138, total_loss: 1.8451659679412842
training step: 52139, total_loss: 2.942420721054077
training step: 52140, total_loss: 1.4546997547149658
training step: 52141, total_loss: 6.866178512573242
training step: 52142, total_loss: 1.804332971572876
training step: 52143, total_loss: 1.4721405506134033
training step: 52144, total_loss: 1.4365332126617432
training step: 52145, total_loss: 1.3311164379119873
training step: 52146, total_loss: 2.663814067840576
training step: 52147, total_loss: 0.38516491651535034
training step: 52148, total_loss: 0.00013213475176598877
training step: 52149, total_loss: 0.9873752593994141
training step: 52150, total_loss: 2.4275736808776855
training step: 52151, total_loss: 0.47591739892959595
training step: 52152, total_loss: 0.1986548751592636
training step: 52153, total_loss: 2.7582476139068604
training step: 52154, total_loss: 1.7059128284454346
training step: 52155, total_loss: 2.906552314758301
training step: 52156, total_loss: 2.6393449306488037
training step: 52157, total_loss: 1.254773497581482
training step: 52158, total_loss: 2.6277105808258057
training step: 52159, total_loss: 1.404956579208374
training step: 52160, total_loss: 1.7728221416473389
training step: 52161, total_loss: 6.029409408569336
training step: 52162, total_loss: 0.8695499897003174
training step: 52163, total_loss: 0.6658279299736023
training step: 52164, total_loss: 0.5544076561927795
training step: 52165, total_loss: 1.5960988998413086
training step: 52166, total_loss: 1.339479684829712
training step: 52167, total_loss: 0.02967005781829357
training step: 52168, total_loss: 0.7871444225311279
training step: 52169, total_loss: 1.251931071281433
training step: 52170, total_loss: 0.7138002514839172
training step: 52171, total_loss: 1.507752776145935
training step: 52172, total_loss: 2.8866710662841797
training step: 52173, total_loss: 2.042506694793701
training step: 52174, total_loss: 2.2150979042053223
training step: 52175, total_loss: 0.8496795296669006
training step: 52176, total_loss: 2.1156017780303955
training step: 52177, total_loss: 1.3893537521362305
training step: 52178, total_loss: 1.7488768100738525
training step: 52179, total_loss: 0.02982674539089203
training step: 52180, total_loss: 1.0360549688339233
training step: 52181, total_loss: 0.5617223381996155
training step: 52182, total_loss: 2.6795969009399414
training step: 52183, total_loss: 1.6349353790283203
training step: 52184, total_loss: 0.9956019520759583
training step: 52185, total_loss: 3.9708497524261475
training step: 52186, total_loss: 0.4086625576019287
training step: 52187, total_loss: 1.098807454109192
training step: 52188, total_loss: 1.1988499164581299
training step: 52189, total_loss: 1.6981866359710693
training step: 52190, total_loss: 1.521834135055542
training step: 52191, total_loss: 4.204446315765381
training step: 52192, total_loss: 0.4918535053730011
training step: 52193, total_loss: 1.526334524154663
training step: 52194, total_loss: 1.14931321144104
training step: 52195, total_loss: 2.4871394634246826
training step: 52196, total_loss: 1.8779783248901367
training step: 52197, total_loss: 1.310056447982788
training step: 52198, total_loss: 1.2363319396972656
training step: 52199, total_loss: 2.105375289916992
training step: 52200, total_loss: 1.9073631763458252
training step: 52201, total_loss: 1.1009583473205566
training step: 52202, total_loss: 1.9219248294830322
training step: 52203, total_loss: 0.318021297454834
training step: 52204, total_loss: 1.2198123931884766
training step: 52205, total_loss: 1.2322216033935547
training step: 52206, total_loss: 2.1326699256896973
training step: 52207, total_loss: 1.9968088865280151
training step: 52208, total_loss: 0.6637585163116455
training step: 52209, total_loss: 1.1297849416732788
training step: 52210, total_loss: 0.11022171378135681
training step: 52211, total_loss: 1.1458643674850464
training step: 52212, total_loss: 0.10650118440389633
training step: 52213, total_loss: 1.3208894729614258
training step: 52214, total_loss: 1.4954051971435547
training step: 52215, total_loss: 2.5689735412597656
training step: 52216, total_loss: 1.9076257944107056
training step: 52217, total_loss: 1.0348587036132812
training step: 52218, total_loss: 1.6665499210357666
training step: 52219, total_loss: 2.3789639472961426
training step: 52220, total_loss: 3.9207632541656494
training step: 52221, total_loss: 0.16817519068717957
training step: 52222, total_loss: 2.510556697845459
training step: 52223, total_loss: 0.8516870737075806
training step: 52224, total_loss: 1.4878519773483276
training step: 52225, total_loss: 1.433488130569458
training step: 52226, total_loss: 0.5824486017227173
training step: 52227, total_loss: 1.170074462890625
training step: 52228, total_loss: 0.06924913823604584
training step: 52229, total_loss: 2.427051067352295
training step: 52230, total_loss: 1.4837656021118164
training step: 52231, total_loss: 1.7543879747390747
training step: 52232, total_loss: 0.7681455016136169
training step: 52233, total_loss: 5.7249250411987305
training step: 52234, total_loss: 6.719745635986328
training step: 52235, total_loss: 4.048557281494141
training step: 52236, total_loss: 1.287327766418457
training step: 52237, total_loss: 0.7127947807312012
training step: 52238, total_loss: 0.8261637687683105
training step: 52239, total_loss: 2.458024024963379
training step: 52240, total_loss: 2.3245651721954346
training step: 52241, total_loss: 1.00010347366333
training step: 52242, total_loss: 1.2531938552856445
training step: 52243, total_loss: 1.0778815746307373
training step: 52244, total_loss: 0.5135003924369812
training step: 52245, total_loss: 0.4247100055217743
training step: 52246, total_loss: 0.014078866690397263
training step: 52247, total_loss: 2.044884204864502
training step: 52248, total_loss: 1.7163900136947632
training step: 52249, total_loss: 0.9447422027587891
training step: 52250, total_loss: 0.5515365600585938
training step: 52251, total_loss: 2.719829559326172
training step: 52252, total_loss: 3.244863986968994
training step: 52253, total_loss: 0.5756769180297852
training step: 52254, total_loss: 0.9917587637901306
training step: 52255, total_loss: 1.4611141681671143
training step: 52256, total_loss: 1.2793775796890259
training step: 52257, total_loss: 2.28902006149292
training step: 52258, total_loss: 2.840296983718872
training step: 52259, total_loss: 0.40601807832717896
training step: 52260, total_loss: 3.9346773624420166
training step: 52261, total_loss: 0.3757016360759735
training step: 52262, total_loss: 0.6213692426681519
training step: 52263, total_loss: 0.18343964219093323
training step: 52264, total_loss: 0.8846721649169922
training step: 52265, total_loss: 1.2458758354187012
training step: 52266, total_loss: 7.111263751983643
training step: 52267, total_loss: 0.9390887022018433
training step: 52268, total_loss: 1.6968035697937012
training step: 52269, total_loss: 3.0987462997436523
training step: 52270, total_loss: 0.395387202501297
training step: 52271, total_loss: 1.4671015739440918
training step: 52272, total_loss: 4.838524341583252
training step: 52273, total_loss: 0.4947715103626251
training step: 52274, total_loss: 1.6634935140609741
training step: 52275, total_loss: 0.02020251750946045
training step: 52276, total_loss: 0.003324432298541069
training step: 52277, total_loss: 4.1738810539245605
training step: 52278, total_loss: 0.5353092551231384
training step: 52279, total_loss: 4.105862617492676
training step: 52280, total_loss: 0.26196834444999695
training step: 52281, total_loss: 1.2540264129638672
training step: 52282, total_loss: 1.6146259307861328
training step: 52283, total_loss: 0.10548997670412064
training step: 52284, total_loss: 1.3011056184768677
training step: 52285, total_loss: 1.6870036125183105
training step: 52286, total_loss: 2.2254838943481445
training step: 52287, total_loss: 2.6534342765808105
training step: 52288, total_loss: 1.6223688125610352
training step: 52289, total_loss: 2.888871669769287
training step: 52290, total_loss: 2.057412624359131
training step: 52291, total_loss: 0.13447566330432892
training step: 52292, total_loss: 0.7818019390106201
training step: 52293, total_loss: 0.1358785182237625
training step: 52294, total_loss: 2.0425710678100586
training step: 52295, total_loss: 1.628955602645874
training step: 52296, total_loss: 0.8692052364349365
training step: 52297, total_loss: 1.6197395324707031
training step: 52298, total_loss: 0.43381384015083313
training step: 52299, total_loss: 1.8296680450439453
training step: 52300, total_loss: 1.4326987266540527
training step: 52301, total_loss: 0.5156006217002869
training step: 52302, total_loss: 2.1737165451049805
training step: 52303, total_loss: 3.068523406982422
training step: 52304, total_loss: 0.2490849494934082
training step: 52305, total_loss: 3.2245895862579346
training step: 52306, total_loss: 1.5180332660675049
training step: 52307, total_loss: 1.093480110168457
training step: 52308, total_loss: 1.8924556970596313
training step: 52309, total_loss: 0.4814607501029968
training step: 52310, total_loss: 0.9360558390617371
training step: 52311, total_loss: 1.2998484373092651
training step: 52312, total_loss: 2.3656396865844727
training step: 52313, total_loss: 0.0639750063419342
training step: 52314, total_loss: 3.5686070919036865
training step: 52315, total_loss: 0.3336912989616394
training step: 52316, total_loss: 1.2188725471496582
training step: 52317, total_loss: 1.1824254989624023
training step: 52318, total_loss: 1.0434058904647827
training step: 52319, total_loss: 0.22873960435390472
training step: 52320, total_loss: 0.989115834236145
training step: 52321, total_loss: 2.5230531692504883
training step: 52322, total_loss: 1.1403594017028809
training step: 52323, total_loss: 2.1763789653778076
training step: 52324, total_loss: 2.143749952316284
training step: 52325, total_loss: 2.5120463371276855
training step: 52326, total_loss: 1.231199860572815
training step: 52327, total_loss: 1.5138418674468994
training step: 52328, total_loss: 1.43678879737854
training step: 52329, total_loss: 0.039222538471221924
training step: 52330, total_loss: 2.5336081981658936
training step: 52331, total_loss: 2.1276206970214844
training step: 52332, total_loss: 1.4829232692718506
training step: 52333, total_loss: 0.12188849598169327
training step: 52334, total_loss: 0.7664380073547363
training step: 52335, total_loss: 0.46934908628463745
training step: 52336, total_loss: 0.021956423297524452
training step: 52337, total_loss: 1.6875720024108887
training step: 52338, total_loss: 1.0852124691009521
training step: 52339, total_loss: 2.6005518436431885
training step: 52340, total_loss: 1.5404340028762817
training step: 52341, total_loss: 1.437788963317871
training step: 52342, total_loss: 1.6690945625305176
training step: 52343, total_loss: 2.5828230381011963
training step: 52344, total_loss: 2.9648566246032715
training step: 52345, total_loss: 0.7851868867874146
training step: 52346, total_loss: 1.2301011085510254
training step: 52347, total_loss: 3.3777544498443604
training step: 52348, total_loss: 1.1862479448318481
training step: 52349, total_loss: 1.8583378791809082
training step: 52350, total_loss: 0.5468457937240601
training step: 52351, total_loss: 2.4616127014160156
training step: 52352, total_loss: 0.7194140553474426
training step: 52353, total_loss: 3.4666123390197754
training step: 52354, total_loss: 2.8646864891052246
training step: 52355, total_loss: 1.6982477903366089
training step: 52356, total_loss: 1.4560779333114624
training step: 52357, total_loss: 0.035086601972579956
training step: 52358, total_loss: 1.4794515371322632
training step: 52359, total_loss: 4.559943675994873
training step: 52360, total_loss: 0.4485199451446533
training step: 52361, total_loss: 2.144685745239258
training step: 52362, total_loss: 2.12453031539917
training step: 52363, total_loss: 1.4075419902801514
training step: 52364, total_loss: 1.5151602029800415
training step: 52365, total_loss: 1.131676197052002
training step: 52366, total_loss: 1.091603398323059
training step: 52367, total_loss: 1.7680227756500244
training step: 52368, total_loss: 1.0039583444595337
training step: 52369, total_loss: 2.943124294281006
training step: 52370, total_loss: 0.6735307574272156
training step: 52371, total_loss: 1.8823412656784058
training step: 52372, total_loss: 0.4228948652744293
training step: 52373, total_loss: 0.7229124307632446
training step: 52374, total_loss: 1.7007908821105957
training step: 52375, total_loss: 1.0730271339416504
training step: 52376, total_loss: 1.8338472843170166
training step: 52377, total_loss: 4.824048042297363
training step: 52378, total_loss: 2.8554301261901855
training step: 52379, total_loss: 0.9036235213279724
training step: 52380, total_loss: 0.7220778465270996
training step: 52381, total_loss: 1.9281843900680542
training step: 52382, total_loss: 0.10041879117488861
training step: 52383, total_loss: 1.6203620433807373
training step: 52384, total_loss: 1.2453879117965698
training step: 52385, total_loss: 1.0005220174789429
training step: 52386, total_loss: 0.9790617227554321
training step: 52387, total_loss: 3.895420789718628
training step: 52388, total_loss: 1.7755235433578491
training step: 52389, total_loss: 2.10280704498291
training step: 52390, total_loss: 3.4586477279663086
training step: 52391, total_loss: 1.2517285346984863
training step: 52392, total_loss: 2.4366633892059326
training step: 52393, total_loss: 1.6082127094268799
training step: 52394, total_loss: 1.1034234762191772
training step: 52395, total_loss: 1.9424562454223633
training step: 52396, total_loss: 0.5822842121124268
training step: 52397, total_loss: 2.3973138332366943
training step: 52398, total_loss: 2.3295319080352783
training step: 52399, total_loss: 1.2792963981628418
training step: 52400, total_loss: 1.177196979522705
training step: 52401, total_loss: 2.4689478874206543
training step: 52402, total_loss: 1.4353684186935425
training step: 52403, total_loss: 0.9396912455558777
training step: 52404, total_loss: 0.495598703622818
training step: 52405, total_loss: 1.9991559982299805
training step: 52406, total_loss: 0.5773608684539795
training step: 52407, total_loss: 0.2705061137676239
training step: 52408, total_loss: 1.1242256164550781
training step: 52409, total_loss: 2.4773688316345215
training step: 52410, total_loss: 1.3928296566009521
training step: 52411, total_loss: 0.09409435838460922
training step: 52412, total_loss: 5.089317321777344
training step: 52413, total_loss: 3.265678882598877
training step: 52414, total_loss: 0.8969815969467163
training step: 52415, total_loss: 0.9239472150802612
training step: 52416, total_loss: 1.710629940032959
training step: 52417, total_loss: 1.8687169551849365
training step: 52418, total_loss: 0.8054714798927307
training step: 52419, total_loss: 1.1284408569335938
training step: 52420, total_loss: 4.203365325927734
training step: 52421, total_loss: 0.06681077182292938
training step: 52422, total_loss: 0.18611671030521393
training step: 52423, total_loss: 1.539074420928955
training step: 52424, total_loss: 1.0858173370361328
training step: 52425, total_loss: 2.3055713176727295
training step: 52426, total_loss: 0.8403371572494507
training step: 52427, total_loss: 0.9064767956733704
training step: 52428, total_loss: 2.255431652069092
training step: 52429, total_loss: 0.5097425580024719
training step: 52430, total_loss: 0.018426498398184776
training step: 52431, total_loss: 2.0754470825195312
training step: 52432, total_loss: 1.2484875917434692
training step: 52433, total_loss: 4.438968658447266
training step: 52434, total_loss: 0.6667603254318237
training step: 52435, total_loss: 1.6793224811553955
training step: 52436, total_loss: 2.991661548614502
training step: 52437, total_loss: 0.9364187717437744
training step: 52438, total_loss: 0.7574746012687683
training step: 52439, total_loss: 0.1329752802848816
training step: 52440, total_loss: 1.5085383653640747
training step: 52441, total_loss: 0.45562639832496643
training step: 52442, total_loss: 2.376765727996826
training step: 52443, total_loss: 0.1724315583705902
training step: 52444, total_loss: 2.8625102043151855
training step: 52445, total_loss: 0.6725127100944519
training step: 52446, total_loss: 3.9421370029449463
training step: 52447, total_loss: 1.3327221870422363
training step: 52448, total_loss: 1.2878618240356445
training step: 52449, total_loss: 0.9635995030403137
training step: 52450, total_loss: 2.567417621612549
training step: 52451, total_loss: 1.987534523010254
training step: 52452, total_loss: 0.2497411072254181
training step: 52453, total_loss: 0.24630093574523926
training step: 52454, total_loss: 1.1075385808944702
training step: 52455, total_loss: 0.5588407516479492
training step: 52456, total_loss: 2.7590274810791016
training step: 52457, total_loss: 0.5779793858528137
training step: 52458, total_loss: 2.321993827819824
training step: 52459, total_loss: 0.2940185070037842
training step: 52460, total_loss: 1.7151143550872803
training step: 52461, total_loss: 0.9858207702636719
training step: 52462, total_loss: 0.9237527847290039
training step: 52463, total_loss: 1.3170231580734253
training step: 52464, total_loss: 3.574695110321045
training step: 52465, total_loss: 2.7397117614746094
training step: 52466, total_loss: 1.1786589622497559
training step: 52467, total_loss: 1.4379440546035767
training step: 52468, total_loss: 0.10128958523273468
training step: 52469, total_loss: 1.0754218101501465
training step: 52470, total_loss: 2.2323598861694336
training step: 52471, total_loss: 2.023223638534546
training step: 52472, total_loss: 3.893551826477051
training step: 52473, total_loss: 1.0875771045684814
training step: 52474, total_loss: 2.6439688205718994
training step: 52475, total_loss: 0.394330233335495
training step: 52476, total_loss: 1.9973092079162598
training step: 52477, total_loss: 2.550034999847412
training step: 52478, total_loss: 3.1989431381225586
training step: 52479, total_loss: 0.22821414470672607
training step: 52480, total_loss: 2.652742862701416
training step: 52481, total_loss: 1.0884536504745483
training step: 52482, total_loss: 0.32229024171829224
training step: 52483, total_loss: 2.962667226791382
training step: 52484, total_loss: 6.502381324768066
training step: 52485, total_loss: 1.0032867193222046
training step: 52486, total_loss: 5.117806434631348
training step: 52487, total_loss: 2.4498090744018555
training step: 52488, total_loss: 1.5762078762054443
training step: 52489, total_loss: 1.570708990097046
training step: 52490, total_loss: 1.4312362670898438
training step: 52491, total_loss: 5.078549385070801
training step: 52492, total_loss: 0.47656095027923584
training step: 52493, total_loss: 1.4759163856506348
training step: 52494, total_loss: 1.8439199924468994
training step: 52495, total_loss: 1.0860633850097656
training step: 52496, total_loss: 1.6917697191238403
training step: 52497, total_loss: 0.3949573040008545
training step: 52498, total_loss: 3.0098390579223633
training step: 52499, total_loss: 1.8158040046691895
training step: 52500, total_loss: 1.397291898727417
training step: 52501, total_loss: 2.166444778442383
training step: 52502, total_loss: 2.6048073768615723
training step: 52503, total_loss: 2.0647056102752686
training step: 52504, total_loss: 0.025287112221121788
training step: 52505, total_loss: 0.2831382751464844
training step: 52506, total_loss: 1.520068883895874
training step: 52507, total_loss: 1.3757489919662476
training step: 52508, total_loss: 0.3555215895175934
training step: 52509, total_loss: 0.8711694478988647
training step: 52510, total_loss: 2.8816871643066406
training step: 52511, total_loss: 0.0024533679243177176
training step: 52512, total_loss: 0.653430700302124
training step: 52513, total_loss: 1.4333057403564453
training step: 52514, total_loss: 1.6040946245193481
training step: 52515, total_loss: 0.1729559302330017
training step: 52516, total_loss: 1.6201425790786743
training step: 52517, total_loss: 0.6314188241958618
training step: 52518, total_loss: 1.4104151725769043
training step: 52519, total_loss: 0.31725427508354187
training step: 52520, total_loss: 0.8684338331222534
training step: 52521, total_loss: 1.4928934574127197
training step: 52522, total_loss: 1.5392251014709473
training step: 52523, total_loss: 0.7855901122093201
training step: 52524, total_loss: 3.658144474029541
training step: 52525, total_loss: 3.015275478363037
training step: 52526, total_loss: 2.6652650833129883
training step: 52527, total_loss: 3.629678964614868
training step: 52528, total_loss: 0.27271372079849243
training step: 52529, total_loss: 1.2456529140472412
training step: 52530, total_loss: 0.24767865240573883
training step: 52531, total_loss: 1.4804894924163818
training step: 52532, total_loss: 1.486619234085083
training step: 52533, total_loss: 2.8083484172821045
training step: 52534, total_loss: 4.058012008666992
training step: 52535, total_loss: 1.0691642761230469
training step: 52536, total_loss: 1.2319090366363525
training step: 52537, total_loss: 1.0384782552719116
training step: 52538, total_loss: 0.5956864953041077
training step: 52539, total_loss: 0.13795745372772217
training step: 52540, total_loss: 2.229109048843384
training step: 52541, total_loss: 1.5766147375106812
training step: 52542, total_loss: 3.8666632175445557
training step: 52543, total_loss: 0.4958053231239319
training step: 52544, total_loss: 1.1566791534423828
training step: 52545, total_loss: 3.5571656227111816
training step: 52546, total_loss: 0.08306144177913666
training step: 52547, total_loss: 0.7516482472419739
training step: 52548, total_loss: 2.8172802925109863
training step: 52549, total_loss: 1.620835542678833
training step: 52550, total_loss: 1.0795629024505615
training step: 52551, total_loss: 3.6988463401794434
training step: 52552, total_loss: 1.2649250030517578
training step: 52553, total_loss: 2.0541722774505615
training step: 52554, total_loss: 0.30467450618743896
training step: 52555, total_loss: 1.951137900352478
training step: 52556, total_loss: 1.0878931283950806
training step: 52557, total_loss: 4.3461737632751465
training step: 52558, total_loss: 0.8880634307861328
training step: 52559, total_loss: 3.622725248336792
training step: 52560, total_loss: 1.6185475587844849
training step: 52561, total_loss: 0.333896666765213
training step: 52562, total_loss: 1.0070443153381348
training step: 52563, total_loss: 0.6907867193222046
training step: 52564, total_loss: 0.7304071187973022
training step: 52565, total_loss: 1.38003408908844
training step: 52566, total_loss: 1.5965709686279297
training step: 52567, total_loss: 3.6509361267089844
training step: 52568, total_loss: 1.9943320751190186
training step: 52569, total_loss: 0.27617067098617554
training step: 52570, total_loss: 0.026185259222984314
training step: 52571, total_loss: 2.4599809646606445
training step: 52572, total_loss: 1.5840544700622559
training step: 52573, total_loss: 1.3985519409179688
training step: 52574, total_loss: 0.3611127734184265
training step: 52575, total_loss: 0.1934497058391571
training step: 52576, total_loss: 1.4318689107894897
training step: 52577, total_loss: 2.94346022605896
training step: 52578, total_loss: 4.6256914138793945
training step: 52579, total_loss: 0.7182395458221436
training step: 52580, total_loss: 0.42643702030181885
training step: 52581, total_loss: 2.3059346675872803
training step: 52582, total_loss: 1.715273380279541
training step: 52583, total_loss: 0.15869593620300293
training step: 52584, total_loss: 3.1859586238861084
training step: 52585, total_loss: 0.8021373748779297
training step: 52586, total_loss: 0.9191719889640808
training step: 52587, total_loss: 2.5524556636810303
training step: 52588, total_loss: 3.2324020862579346
training step: 52589, total_loss: 1.8615049123764038
training step: 52590, total_loss: 1.4013124704360962
training step: 52591, total_loss: 1.9754971265792847
training step: 52592, total_loss: 0.9008755683898926
training step: 52593, total_loss: 4.672127723693848
training step: 52594, total_loss: 1.0392770767211914
training step: 52595, total_loss: 0.4430578947067261
training step: 52596, total_loss: 2.226865291595459
training step: 52597, total_loss: 1.697641134262085
training step: 52598, total_loss: 2.114077568054199
training step: 52599, total_loss: 2.58699369430542
training step: 52600, total_loss: 0.38605162501335144
training step: 52601, total_loss: 2.116053581237793
training step: 52602, total_loss: 0.1842738837003708
training step: 52603, total_loss: 1.3319337368011475
training step: 52604, total_loss: 0.25861459970474243
training step: 52605, total_loss: 1.7538114786148071
training step: 52606, total_loss: 1.9019606113433838
training step: 52607, total_loss: 0.5293474793434143
training step: 52608, total_loss: 4.505832672119141
training step: 52609, total_loss: 1.6751725673675537
training step: 52610, total_loss: 2.2146148681640625
training step: 52611, total_loss: 0.6166637539863586
training step: 52612, total_loss: 0.3408789038658142
training step: 52613, total_loss: 0.44889336824417114
training step: 52614, total_loss: 2.3366875648498535
training step: 52615, total_loss: 0.510809063911438
training step: 52616, total_loss: 0.6163204908370972
training step: 52617, total_loss: 1.202239990234375
training step: 52618, total_loss: 2.061244249343872
training step: 52619, total_loss: 0.6771841645240784
training step: 52620, total_loss: 0.4723259210586548
training step: 52621, total_loss: 2.280318260192871
training step: 52622, total_loss: 1.1436166763305664
training step: 52623, total_loss: 3.400383472442627
training step: 52624, total_loss: 0.03508296608924866
training step: 52625, total_loss: 2.086341381072998
training step: 52626, total_loss: 1.6129310131072998
training step: 52627, total_loss: 2.7085468769073486
training step: 52628, total_loss: 3.343165397644043
training step: 52629, total_loss: 2.726926803588867
training step: 52630, total_loss: 0.060807570815086365
training step: 52631, total_loss: 1.9412403106689453
training step: 52632, total_loss: 3.0515975952148438
training step: 52633, total_loss: 0.48717424273490906
training step: 52634, total_loss: 1.7443922758102417
training step: 52635, total_loss: 0.0031450041569769382
training step: 52636, total_loss: 1.715303659439087
training step: 52637, total_loss: 1.7090868949890137
training step: 52638, total_loss: 0.039559151977300644
training step: 52639, total_loss: 0.010416536591947079
training step: 52640, total_loss: 0.7022287249565125
training step: 52641, total_loss: 0.12073680013418198
training step: 52642, total_loss: 1.2770353555679321
training step: 52643, total_loss: 2.414262533187866
training step: 52644, total_loss: 0.40369999408721924
training step: 52645, total_loss: 1.3312242031097412
training step: 52646, total_loss: 2.038088083267212
training step: 52647, total_loss: 4.568791389465332
training step: 52648, total_loss: 0.9232712984085083
training step: 52649, total_loss: 2.6469573974609375
training step: 52650, total_loss: 0.8855980634689331
training step: 52651, total_loss: 0.6239200234413147
training step: 52652, total_loss: 2.2691895961761475
training step: 52653, total_loss: 1.5284736156463623
training step: 52654, total_loss: 0.463505357503891
training step: 52655, total_loss: 1.961052656173706
training step: 52656, total_loss: 0.8922591209411621
training step: 52657, total_loss: 1.687437653541565
training step: 52658, total_loss: 1.3556797504425049
training step: 52659, total_loss: 0.030835257843136787
training step: 52660, total_loss: 2.7760088443756104
training step: 52661, total_loss: 0.47652700543403625
training step: 52662, total_loss: 0.330758661031723
training step: 52663, total_loss: 2.687350034713745
training step: 52664, total_loss: 2.9334769248962402
training step: 52665, total_loss: 0.42472341656684875
training step: 52666, total_loss: 1.7995173931121826
training step: 52667, total_loss: 2.900397777557373
training step: 52668, total_loss: 0.6958476901054382
training step: 52669, total_loss: 1.8865718841552734
training step: 52670, total_loss: 0.7978789806365967
training step: 52671, total_loss: 2.672787666320801
training step: 52672, total_loss: 0.4616156220436096
training step: 52673, total_loss: 3.5489871501922607
training step: 52674, total_loss: 0.3030768036842346
training step: 52675, total_loss: 4.599112510681152
training step: 52676, total_loss: 1.511704683303833
training step: 52677, total_loss: 1.910165786743164
training step: 52678, total_loss: 6.410098075866699
training step: 52679, total_loss: 1.7419222593307495
training step: 52680, total_loss: 4.4772233963012695
training step: 52681, total_loss: 1.000993251800537
training step: 52682, total_loss: 1.3765461444854736
training step: 52683, total_loss: 1.5270237922668457
training step: 52684, total_loss: 1.1022484302520752
training step: 52685, total_loss: 1.7636826038360596
training step: 52686, total_loss: 0.7476332187652588
training step: 52687, total_loss: 1.1449053287506104
training step: 52688, total_loss: 0.34722810983657837
training step: 52689, total_loss: 2.169527530670166
training step: 52690, total_loss: 1.4630074501037598
training step: 52691, total_loss: 1.440107822418213
training step: 52692, total_loss: 3.0480031967163086
training step: 52693, total_loss: 1.1070659160614014
training step: 52694, total_loss: 0.04465242847800255
training step: 52695, total_loss: 2.1969306468963623
training step: 52696, total_loss: 2.454899787902832
training step: 52697, total_loss: 0.36593738198280334
training step: 52698, total_loss: 2.071685314178467
training step: 52699, total_loss: 0.7981534600257874
training step: 52700, total_loss: 0.0046701813116669655
training step: 52701, total_loss: 1.8008010387420654
training step: 52702, total_loss: 0.0003980795736424625
training step: 52703, total_loss: 1.3060959577560425
training step: 52704, total_loss: 0.6279637813568115
training step: 52705, total_loss: 1.8797547817230225
training step: 52706, total_loss: 1.2793025970458984
training step: 52707, total_loss: 1.6762964725494385
training step: 52708, total_loss: 4.051948547363281
training step: 52709, total_loss: 0.310587614774704
training step: 52710, total_loss: 3.413980484008789
training step: 52711, total_loss: 0.3104085922241211
training step: 52712, total_loss: 1.5871648788452148
training step: 52713, total_loss: 0.9385952949523926
training step: 52714, total_loss: 1.53885018825531
training step: 52715, total_loss: 1.2463152408599854
training step: 52716, total_loss: 1.762091875076294
training step: 52717, total_loss: 1.0322136878967285
training step: 52718, total_loss: 1.973017930984497
training step: 52719, total_loss: 0.24037474393844604
training step: 52720, total_loss: 0.9011290669441223
training step: 52721, total_loss: 0.15231889486312866
training step: 52722, total_loss: 0.24382264912128448
training step: 52723, total_loss: 0.1584239900112152
training step: 52724, total_loss: 2.2608299255371094
training step: 52725, total_loss: 0.13177402317523956
training step: 52726, total_loss: 2.5847980976104736
training step: 52727, total_loss: 0.009843921288847923
training step: 52728, total_loss: 2.0145156383514404
training step: 52729, total_loss: 1.6952059268951416
training step: 52730, total_loss: 0.25971800088882446
training step: 52731, total_loss: 1.1276544332504272
training step: 52732, total_loss: 0.7474730610847473
training step: 52733, total_loss: 1.1632415056228638
training step: 52734, total_loss: 1.458268165588379
training step: 52735, total_loss: 1.7550386190414429
training step: 52736, total_loss: 1.8366236686706543
training step: 52737, total_loss: 0.2767122685909271
training step: 52738, total_loss: 0.4825475215911865
training step: 52739, total_loss: 0.7879135608673096
training step: 52740, total_loss: 1.217054843902588
training step: 52741, total_loss: 0.8734199404716492
training step: 52742, total_loss: 2.832778215408325
training step: 52743, total_loss: 0.7366204261779785
training step: 52744, total_loss: 1.2589097023010254
training step: 52745, total_loss: 1.7429698705673218
training step: 52746, total_loss: 1.1409223079681396
training step: 52747, total_loss: 1.1656053066253662
training step: 52748, total_loss: 1.020412802696228
training step: 52749, total_loss: 1.9046783447265625
training step: 52750, total_loss: 0.37920230627059937
training step: 52751, total_loss: 3.091588258743286
training step: 52752, total_loss: 2.632507801055908
training step: 52753, total_loss: 2.6336803436279297
training step: 52754, total_loss: 1.348598837852478
training step: 52755, total_loss: 0.18510670959949493
training step: 52756, total_loss: 0.004750994499772787
training step: 52757, total_loss: 1.3889464139938354
training step: 52758, total_loss: 1.3553552627563477
training step: 52759, total_loss: 2.703416109085083
training step: 52760, total_loss: 2.646745443344116
training step: 52761, total_loss: 5.75072717666626
training step: 52762, total_loss: 2.1103744506835938
training step: 52763, total_loss: 0.009502159431576729
training step: 52764, total_loss: 0.397710919380188
training step: 52765, total_loss: 0.04060683771967888
training step: 52766, total_loss: 0.6277689933776855
training step: 52767, total_loss: 2.060004711151123
training step: 52768, total_loss: 1.621113896369934
training step: 52769, total_loss: 1.4702799320220947
training step: 52770, total_loss: 0.5919177532196045
training step: 52771, total_loss: 0.17619448900222778
training step: 52772, total_loss: 1.0938080549240112
training step: 52773, total_loss: 0.5648565292358398
training step: 52774, total_loss: 4.604193210601807
training step: 52775, total_loss: 2.506385326385498
training step: 52776, total_loss: 7.388647556304932
training step: 52777, total_loss: 1.1625628471374512
training step: 52778, total_loss: 3.181212902069092
training step: 52779, total_loss: 1.5840351581573486
training step: 52780, total_loss: 1.1381415128707886
training step: 52781, total_loss: 0.4611945152282715
training step: 52782, total_loss: 0.016245566308498383
training step: 52783, total_loss: 0.9618914127349854
training step: 52784, total_loss: 0.16205503046512604
training step: 52785, total_loss: 0.8998140096664429
training step: 52786, total_loss: 0.4030996859073639
training step: 52787, total_loss: 1.576481819152832
training step: 52788, total_loss: 5.173722267150879
training step: 52789, total_loss: 0.07402738928794861
training step: 52790, total_loss: 5.734892845153809
training step: 52791, total_loss: 0.015552671626210213
training step: 52792, total_loss: 2.0858945846557617
training step: 52793, total_loss: 0.9343382120132446
training step: 52794, total_loss: 1.224609375
training step: 52795, total_loss: 1.230182409286499
training step: 52796, total_loss: 0.5743980407714844
training step: 52797, total_loss: 2.051547050476074
training step: 52798, total_loss: 1.0784633159637451
training step: 52799, total_loss: 0.4422423243522644
training step: 52800, total_loss: 1.7387566566467285
training step: 52801, total_loss: 1.4515305757522583
training step: 52802, total_loss: 0.07147122919559479
training step: 52803, total_loss: 2.594054937362671
training step: 52804, total_loss: 0.03245488926768303
training step: 52805, total_loss: 2.414517641067505
training step: 52806, total_loss: 0.013806079514324665
training step: 52807, total_loss: 1.0989789962768555
training step: 52808, total_loss: 3.1297099590301514
training step: 52809, total_loss: 2.0123751163482666
training step: 52810, total_loss: 2.684616804122925
training step: 52811, total_loss: 3.740687370300293
training step: 52812, total_loss: 2.699944019317627
training step: 52813, total_loss: 2.0489165782928467
training step: 52814, total_loss: 3.26782488822937
training step: 52815, total_loss: 1.935441017150879
training step: 52816, total_loss: 4.6372785568237305
training step: 52817, total_loss: 3.7628092765808105
training step: 52818, total_loss: 1.2864627838134766
training step: 52819, total_loss: 4.002306938171387
training step: 52820, total_loss: 2.0906825065612793
training step: 52821, total_loss: 0.1535218358039856
training step: 52822, total_loss: 0.6273004412651062
training step: 52823, total_loss: 1.4755136966705322
training step: 52824, total_loss: 2.499574661254883
training step: 52825, total_loss: 4.848597526550293
training step: 52826, total_loss: 0.31358659267425537
training step: 52827, total_loss: 0.578233003616333
training step: 52828, total_loss: 0.5148137211799622
training step: 52829, total_loss: 2.297490358352661
training step: 52830, total_loss: 0.6987215876579285
training step: 52831, total_loss: 0.059032417833805084
training step: 52832, total_loss: 4.28972053527832
training step: 52833, total_loss: 2.266871929168701
training step: 52834, total_loss: 1.0450619459152222
training step: 52835, total_loss: 3.660167694091797
training step: 52836, total_loss: 0.06343361735343933
training step: 52837, total_loss: 1.893792748451233
training step: 52838, total_loss: 0.5564119815826416
training step: 52839, total_loss: 0.4090839624404907
training step: 52840, total_loss: 2.8644399642944336
training step: 52841, total_loss: 1.353349208831787
training step: 52842, total_loss: 0.6905999779701233
training step: 52843, total_loss: 0.5574103593826294
training step: 52844, total_loss: 0.0849921852350235
training step: 52845, total_loss: 1.1069399118423462
training step: 52846, total_loss: 0.4755951762199402
training step: 52847, total_loss: 0.27594345808029175
training step: 52848, total_loss: 1.0691970586776733
training step: 52849, total_loss: 0.823267936706543
training step: 52850, total_loss: 1.241186499595642
INFO:tensorflow:Writing predictions to: test_output/predictions_53000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_53000.json
training step: 52851, total_loss: 3.833292007446289
training step: 52852, total_loss: 1.3522989749908447
training step: 52853, total_loss: 1.0818432569503784
training step: 52854, total_loss: 1.192754864692688
training step: 52855, total_loss: 2.4011363983154297
training step: 52856, total_loss: 0.8552239537239075
training step: 52857, total_loss: 2.2309987545013428
training step: 52858, total_loss: 3.4896936416625977
training step: 52859, total_loss: 0.030609281733632088
training step: 52860, total_loss: 0.9892591238021851
training step: 52861, total_loss: 2.46533465385437
training step: 52862, total_loss: 1.4944117069244385
training step: 52863, total_loss: 0.02708396688103676
training step: 52864, total_loss: 0.5059105753898621
training step: 52865, total_loss: 2.5009658336639404
training step: 52866, total_loss: 4.017855644226074
training step: 52867, total_loss: 2.46817684173584
training step: 52868, total_loss: 1.1430981159210205
training step: 52869, total_loss: 0.3908694088459015
training step: 52870, total_loss: 2.302616596221924
training step: 52871, total_loss: 3.4020066261291504
training step: 52872, total_loss: 2.472531795501709
training step: 52873, total_loss: 3.214787244796753
training step: 52874, total_loss: 0.7394481301307678
training step: 52875, total_loss: 0.003138484200462699
training step: 52876, total_loss: 0.3874061703681946
training step: 52877, total_loss: 2.6622719764709473
training step: 52878, total_loss: 1.3132244348526
training step: 52879, total_loss: 0.6535889506340027
training step: 52880, total_loss: 0.8858616948127747
training step: 52881, total_loss: 0.44119951128959656
training step: 52882, total_loss: 0.4833361506462097
training step: 52883, total_loss: 2.0769190788269043
training step: 52884, total_loss: 1.543810486793518
training step: 52885, total_loss: 0.8465849161148071
training step: 52886, total_loss: 0.45358139276504517
training step: 52887, total_loss: 2.635106086730957
training step: 52888, total_loss: 1.9892535209655762
training step: 52889, total_loss: 2.5993285179138184
training step: 52890, total_loss: 0.45876777172088623
training step: 52891, total_loss: 0.2731054127216339
training step: 52892, total_loss: 1.6461100578308105
training step: 52893, total_loss: 7.06289429217577e-05
training step: 52894, total_loss: 4.2415971755981445
training step: 52895, total_loss: 2.5254008769989014
training step: 52896, total_loss: 1.2052288055419922
training step: 52897, total_loss: 0.84578937292099
training step: 52898, total_loss: 1.5625858306884766
training step: 52899, total_loss: 2.867239236831665
training step: 52900, total_loss: 0.7491132020950317
training step: 52901, total_loss: 0.4787505865097046
training step: 52902, total_loss: 0.5177857279777527
training step: 52903, total_loss: 3.7665863037109375
training step: 52904, total_loss: 1.383408784866333
training step: 52905, total_loss: 0.9154269695281982
training step: 52906, total_loss: 0.1570306420326233
training step: 52907, total_loss: 0.11477215588092804
training step: 52908, total_loss: 2.0617165565490723
training step: 52909, total_loss: 0.3170761466026306
training step: 52910, total_loss: 1.6751246452331543
training step: 52911, total_loss: 1.6549471616744995
training step: 52912, total_loss: 2.8522086143493652
training step: 52913, total_loss: 0.12764549255371094
training step: 52914, total_loss: 2.5890350341796875
training step: 52915, total_loss: 0.2774646282196045
training step: 52916, total_loss: 0.8625914454460144
training step: 52917, total_loss: 0.17290356755256653
training step: 52918, total_loss: 0.06541691720485687
training step: 52919, total_loss: 0.7443836331367493
training step: 52920, total_loss: 0.27019065618515015
training step: 52921, total_loss: 0.010551488026976585
training step: 52922, total_loss: 1.799660563468933
training step: 52923, total_loss: 2.246227502822876
training step: 52924, total_loss: 5.065761566162109
training step: 52925, total_loss: 2.870666027069092
training step: 52926, total_loss: 0.8038160800933838
training step: 52927, total_loss: 2.711958885192871
training step: 52928, total_loss: 1.231101155281067
training step: 52929, total_loss: 0.04266734793782234
training step: 52930, total_loss: 0.1872905194759369
training step: 52931, total_loss: 3.3709588050842285
training step: 52932, total_loss: 0.009980041533708572
training step: 52933, total_loss: 3.7402172088623047
training step: 52934, total_loss: 0.4465620815753937
training step: 52935, total_loss: 3.554603099822998
training step: 52936, total_loss: 0.08899480104446411
training step: 52937, total_loss: 2.1116766929626465
training step: 52938, total_loss: 1.5932888984680176
training step: 52939, total_loss: 4.205448150634766
training step: 52940, total_loss: 2.85198712348938
training step: 52941, total_loss: 2.1606674194335938
training step: 52942, total_loss: 1.9421582221984863
training step: 52943, total_loss: 2.846479654312134
training step: 52944, total_loss: 1.729702115058899
training step: 52945, total_loss: 0.3613179326057434
training step: 52946, total_loss: 2.71073842048645
training step: 52947, total_loss: 1.7170493602752686
training step: 52948, total_loss: 1.8838194608688354
training step: 52949, total_loss: 0.6066746711730957
training step: 52950, total_loss: 1.6652675867080688
training step: 52951, total_loss: 2.405205249786377
training step: 52952, total_loss: 1.9643988609313965
training step: 52953, total_loss: 3.0579380989074707
training step: 52954, total_loss: 0.6318932175636292
training step: 52955, total_loss: 2.523324489593506
training step: 52956, total_loss: 0.28229308128356934
training step: 52957, total_loss: 0.7423244714736938
training step: 52958, total_loss: 3.4364359378814697
training step: 52959, total_loss: 0.9026200175285339
training step: 52960, total_loss: 2.1232051849365234
training step: 52961, total_loss: 1.8443241119384766
training step: 52962, total_loss: 1.522221565246582
training step: 52963, total_loss: 1.7179841995239258
training step: 52964, total_loss: 1.956481695175171
training step: 52965, total_loss: 0.5798329710960388
training step: 52966, total_loss: 2.2274296283721924
training step: 52967, total_loss: 0.45697396993637085
training step: 52968, total_loss: 2.0737481117248535
training step: 52969, total_loss: 1.5411300659179688
training step: 52970, total_loss: 3.867033004760742
training step: 52971, total_loss: 2.9335200786590576
training step: 52972, total_loss: 0.29626917839050293
training step: 52973, total_loss: 0.5237246155738831
training step: 52974, total_loss: 0.7443305850028992
training step: 52975, total_loss: 0.1358291357755661
training step: 52976, total_loss: 3.386373281478882
training step: 52977, total_loss: 2.0863614082336426
training step: 52978, total_loss: 1.7244393825531006
training step: 52979, total_loss: 0.47069692611694336
training step: 52980, total_loss: 1.5920794010162354
training step: 52981, total_loss: 2.1434402465820312
training step: 52982, total_loss: 0.006007085554301739
training step: 52983, total_loss: 0.15694469213485718
training step: 52984, total_loss: 2.0337257385253906
training step: 52985, total_loss: 0.43820738792419434
training step: 52986, total_loss: 2.262298583984375
training step: 52987, total_loss: 2.451653003692627
training step: 52988, total_loss: 3.8272476196289062
training step: 52989, total_loss: 5.6548967361450195
training step: 52990, total_loss: 0.6709787249565125
training step: 52991, total_loss: 1.0331605672836304
training step: 52992, total_loss: 1.7863136529922485
training step: 52993, total_loss: 1.9334276914596558
training step: 52994, total_loss: 0.3272574245929718
training step: 52995, total_loss: 1.2995848655700684
training step: 52996, total_loss: 1.169975757598877
training step: 52997, total_loss: 2.178272247314453
training step: 52998, total_loss: 0.8877052664756775
training step: 52999, total_loss: 0.7878758311271667
training step: 53000, total_loss: 1.5462285280227661
epoch finished! shuffle=False
evaluation: 17000, total_loss: 1.9298008680343628, f1: 55.75817491772159, followup: 27.749885896850753, yesno: 57.66012475277651, heq: 50.920432070591815, dheq: 3.6

Model saved in path test_output//model_53000.ckpt
training step: 53001, total_loss: 1.3215852975845337
training step: 53002, total_loss: 2.074928045272827
training step: 53003, total_loss: 2.370872974395752
training step: 53004, total_loss: 3.203544855117798
training step: 53005, total_loss: 1.9511768817901611
training step: 53006, total_loss: 0.9924206137657166
training step: 53007, total_loss: 2.442288875579834
training step: 53008, total_loss: 0.8573761582374573
training step: 53009, total_loss: 0.4942360520362854
training step: 53010, total_loss: 1.9089651107788086
training step: 53011, total_loss: 0.4599952697753906
training step: 53012, total_loss: 5.596846580505371
training step: 53013, total_loss: 4.284115791320801
training step: 53014, total_loss: 1.3883564472198486
training step: 53015, total_loss: 0.8821247816085815
training step: 53016, total_loss: 0.6978937387466431
training step: 53017, total_loss: 1.085876226425171
training step: 53018, total_loss: 1.6408705711364746
training step: 53019, total_loss: 1.1877632141113281
training step: 53020, total_loss: 1.0252708196640015
training step: 53021, total_loss: 1.402704119682312
training step: 53022, total_loss: 0.6391245722770691
training step: 53023, total_loss: 4.804997444152832
training step: 53024, total_loss: 0.08054030686616898
training step: 53025, total_loss: 1.4529372453689575
training step: 53026, total_loss: 2.736372470855713
training step: 53027, total_loss: 1.4828020334243774
training step: 53028, total_loss: 5.756673812866211
training step: 53029, total_loss: 0.7871285080909729
training step: 53030, total_loss: 1.5377845764160156
training step: 53031, total_loss: 0.3459058105945587
training step: 53032, total_loss: 0.0031279653776437044
training step: 53033, total_loss: 0.23361502587795258
training step: 53034, total_loss: 1.083065152168274
training step: 53035, total_loss: 1.4504632949829102
training step: 53036, total_loss: 1.1855065822601318
training step: 53037, total_loss: 2.4571480751037598
training step: 53038, total_loss: 2.139673948287964
training step: 53039, total_loss: 4.925296783447266
training step: 53040, total_loss: 0.013086301274597645
training step: 53041, total_loss: 2.898289680480957
training step: 53042, total_loss: 0.6040758490562439
training step: 53043, total_loss: 0.00031841619056649506
training step: 53044, total_loss: 2.331084728240967
training step: 53045, total_loss: 0.24400372803211212
training step: 53046, total_loss: 0.1364431381225586
training step: 53047, total_loss: 1.471500277519226
training step: 53048, total_loss: 2.6210339069366455
training step: 53049, total_loss: 2.137298583984375
training step: 53050, total_loss: 0.6578296422958374
training step: 53051, total_loss: 1.0057824850082397
training step: 53052, total_loss: 1.7818984985351562
training step: 53053, total_loss: 0.910151481628418
training step: 53054, total_loss: 3.0327463150024414
training step: 53055, total_loss: 2.5856943130493164
training step: 53056, total_loss: 0.42299699783325195
training step: 53057, total_loss: 1.5942494869232178
training step: 53058, total_loss: 0.6918227672576904
training step: 53059, total_loss: 1.186030626296997
training step: 53060, total_loss: 0.1746857464313507
training step: 53061, total_loss: 1.8133217096328735
training step: 53062, total_loss: 0.9673792123794556
training step: 53063, total_loss: 3.540731430053711
training step: 53064, total_loss: 0.662092924118042
training step: 53065, total_loss: 0.00885354820638895
training step: 53066, total_loss: 0.07439246028661728
training step: 53067, total_loss: 0.16421958804130554
training step: 53068, total_loss: 0.6444441080093384
training step: 53069, total_loss: 1.0247406959533691
training step: 53070, total_loss: 4.053830146789551
training step: 53071, total_loss: 0.25177374482154846
training step: 53072, total_loss: 1.9551570415496826
training step: 53073, total_loss: 1.5476347208023071
training step: 53074, total_loss: 1.2525023221969604
training step: 53075, total_loss: 2.973123073577881
training step: 53076, total_loss: 3.4410924911499023
training step: 53077, total_loss: 1.3266197443008423
training step: 53078, total_loss: 0.8335994482040405
training step: 53079, total_loss: 1.2201356887817383
training step: 53080, total_loss: 1.7498692274093628
training step: 53081, total_loss: 3.994063138961792
training step: 53082, total_loss: 0.7483577728271484
training step: 53083, total_loss: 0.6063982844352722
training step: 53084, total_loss: 1.7196959257125854
training step: 53085, total_loss: 1.267423391342163
training step: 53086, total_loss: 1.1489601135253906
training step: 53087, total_loss: 2.364034652709961
training step: 53088, total_loss: 1.37125825881958
training step: 53089, total_loss: 2.9960122108459473
training step: 53090, total_loss: 1.430375337600708
training step: 53091, total_loss: 0.3155139684677124
training step: 53092, total_loss: 2.7064642906188965
training step: 53093, total_loss: 0.5849971175193787
training step: 53094, total_loss: 1.7851576805114746
training step: 53095, total_loss: 1.2431260347366333
training step: 53096, total_loss: 1.4227113723754883
training step: 53097, total_loss: 0.43147826194763184
training step: 53098, total_loss: 2.259598731994629
training step: 53099, total_loss: 4.204197406768799
training step: 53100, total_loss: 0.22127780318260193
training step: 53101, total_loss: 4.124770641326904
training step: 53102, total_loss: 0.7283686399459839
training step: 53103, total_loss: 0.836998701095581
training step: 53104, total_loss: 2.7484989166259766
training step: 53105, total_loss: 2.1002352237701416
training step: 53106, total_loss: 1.0972020626068115
training step: 53107, total_loss: 0.38852888345718384
training step: 53108, total_loss: 2.516638994216919
training step: 53109, total_loss: 0.8953346014022827
training step: 53110, total_loss: 1.5194754600524902
training step: 53111, total_loss: 0.056667592376470566
training step: 53112, total_loss: 1.8920180797576904
training step: 53113, total_loss: 1.9088752269744873
training step: 53114, total_loss: 0.4961932897567749
training step: 53115, total_loss: 0.09027784317731857
training step: 53116, total_loss: 1.0495237112045288
training step: 53117, total_loss: 1.5719287395477295
training step: 53118, total_loss: 1.0461293458938599
training step: 53119, total_loss: 0.19972708821296692
training step: 53120, total_loss: 1.7246532440185547
training step: 53121, total_loss: 4.616621017456055
training step: 53122, total_loss: 0.018536504358053207
training step: 53123, total_loss: 0.32375138998031616
training step: 53124, total_loss: 1.192507266998291
training step: 53125, total_loss: 2.058126449584961
training step: 53126, total_loss: 2.6099729537963867
training step: 53127, total_loss: 1.2139819860458374
training step: 53128, total_loss: 1.0071759223937988
training step: 53129, total_loss: 3.0461480617523193
training step: 53130, total_loss: 2.0810275077819824
training step: 53131, total_loss: 1.22354257106781
training step: 53132, total_loss: 0.38475966453552246
training step: 53133, total_loss: 3.057262659072876
training step: 53134, total_loss: 2.2796711921691895
training step: 53135, total_loss: 1.1689832210540771
training step: 53136, total_loss: 0.10784374177455902
training step: 53137, total_loss: 1.451550006866455
training step: 53138, total_loss: 1.4935286045074463
training step: 53139, total_loss: 0.7945184111595154
training step: 53140, total_loss: 3.3365397453308105
training step: 53141, total_loss: 3.2320706844329834
training step: 53142, total_loss: 0.19378557801246643
training step: 53143, total_loss: 0.040993597358465195
training step: 53144, total_loss: 0.9977462291717529
training step: 53145, total_loss: 1.5892817974090576
training step: 53146, total_loss: 0.2624075412750244
training step: 53147, total_loss: 0.007551423739641905
training step: 53148, total_loss: 2.033554792404175
training step: 53149, total_loss: 4.094773769378662
training step: 53150, total_loss: 0.4825022220611572
training step: 53151, total_loss: 1.923987865447998
training step: 53152, total_loss: 1.032125473022461
training step: 53153, total_loss: 1.9775826930999756
training step: 53154, total_loss: 1.2144643068313599
training step: 53155, total_loss: 2.3523848056793213
training step: 53156, total_loss: 3.540369987487793
training step: 53157, total_loss: 1.5352917909622192
training step: 53158, total_loss: 0.3479873836040497
training step: 53159, total_loss: 1.6715775728225708
training step: 53160, total_loss: 1.5269534587860107
training step: 53161, total_loss: 2.5051116943359375
training step: 53162, total_loss: 1.0310280323028564
training step: 53163, total_loss: 1.3949806690216064
training step: 53164, total_loss: 0.8669780492782593
training step: 53165, total_loss: 1.192425012588501
training step: 53166, total_loss: 1.318100929260254
training step: 53167, total_loss: 1.5392086505889893
training step: 53168, total_loss: 1.7800161838531494
training step: 53169, total_loss: 2.6970462799072266
training step: 53170, total_loss: 0.9147660732269287
training step: 53171, total_loss: 2.2525718212127686
training step: 53172, total_loss: 1.008732795715332
training step: 53173, total_loss: 1.5759060382843018
training step: 53174, total_loss: 0.9724379777908325
training step: 53175, total_loss: 2.3700485229492188
training step: 53176, total_loss: 2.540064573287964
training step: 53177, total_loss: 3.6128783226013184
training step: 53178, total_loss: 1.0826705694198608
training step: 53179, total_loss: 1.1609010696411133
training step: 53180, total_loss: 4.781625747680664
training step: 53181, total_loss: 1.54233980178833
training step: 53182, total_loss: 1.7050288915634155
training step: 53183, total_loss: 1.709118127822876
training step: 53184, total_loss: 1.547126293182373
training step: 53185, total_loss: 2.229586362838745
training step: 53186, total_loss: 4.800363540649414
training step: 53187, total_loss: 0.8398100137710571
training step: 53188, total_loss: 0.37174496054649353
training step: 53189, total_loss: 1.170137643814087
training step: 53190, total_loss: 0.09000205993652344
training step: 53191, total_loss: 0.8144491314888
training step: 53192, total_loss: 1.1623204946517944
training step: 53193, total_loss: 1.57248854637146
training step: 53194, total_loss: 1.3788893222808838
training step: 53195, total_loss: 2.265136241912842
training step: 53196, total_loss: 2.0024914741516113
training step: 53197, total_loss: 1.5008106231689453
training step: 53198, total_loss: 2.6998486518859863
training step: 53199, total_loss: 0.1803281456232071
training step: 53200, total_loss: 1.7671473026275635
training step: 53201, total_loss: 0.5954936146736145
training step: 53202, total_loss: 1.5219018459320068
training step: 53203, total_loss: 1.2226686477661133
training step: 53204, total_loss: 2.6969785690307617
training step: 53205, total_loss: 1.5224690437316895
training step: 53206, total_loss: 2.11136531829834
training step: 53207, total_loss: 2.057097911834717
training step: 53208, total_loss: 3.199882984161377
training step: 53209, total_loss: 0.9269194602966309
training step: 53210, total_loss: 2.9925827980041504
training step: 53211, total_loss: 1.8937814235687256
training step: 53212, total_loss: 2.781919002532959
training step: 53213, total_loss: 0.5124035477638245
training step: 53214, total_loss: 2.413832187652588
training step: 53215, total_loss: 0.4518948495388031
training step: 53216, total_loss: 0.28948402404785156
training step: 53217, total_loss: 1.9832803010940552
training step: 53218, total_loss: 4.499725341796875
training step: 53219, total_loss: 0.15372443199157715
training step: 53220, total_loss: 2.3477940559387207
training step: 53221, total_loss: 1.5856270790100098
training step: 53222, total_loss: 1.602757453918457
training step: 53223, total_loss: 2.0698819160461426
training step: 53224, total_loss: 0.4959013760089874
training step: 53225, total_loss: 1.7076618671417236
training step: 53226, total_loss: 0.1243588775396347
training step: 53227, total_loss: 2.498894691467285
training step: 53228, total_loss: 2.5173287391662598
training step: 53229, total_loss: 1.6726267337799072
training step: 53230, total_loss: 1.1774938106536865
training step: 53231, total_loss: 3.862154483795166
training step: 53232, total_loss: 1.0932307243347168
training step: 53233, total_loss: 2.1771206855773926
training step: 53234, total_loss: 2.6418681144714355
training step: 53235, total_loss: 0.060250479727983475
training step: 53236, total_loss: 2.0029172897338867
training step: 53237, total_loss: 0.8388001322746277
training step: 53238, total_loss: 0.444080650806427
training step: 53239, total_loss: 2.5821609497070312
training step: 53240, total_loss: 0.0001904757518786937
training step: 53241, total_loss: 1.2161884307861328
training step: 53242, total_loss: 1.7222613096237183
training step: 53243, total_loss: 2.3619160652160645
training step: 53244, total_loss: 1.3810808658599854
training step: 53245, total_loss: 3.9273972511291504
training step: 53246, total_loss: 1.9319111108779907
training step: 53247, total_loss: 1.5264748334884644
training step: 53248, total_loss: 1.0736818313598633
training step: 53249, total_loss: 1.9228557348251343
training step: 53250, total_loss: 1.0035431385040283
training step: 53251, total_loss: 0.9283682703971863
training step: 53252, total_loss: 1.0000087022781372
training step: 53253, total_loss: 2.401137113571167
training step: 53254, total_loss: 1.3834069967269897
training step: 53255, total_loss: 2.4012441635131836
training step: 53256, total_loss: 0.023757897317409515
training step: 53257, total_loss: 4.286731719970703
training step: 53258, total_loss: 0.9091852903366089
training step: 53259, total_loss: 0.6342325806617737
training step: 53260, total_loss: 0.8239732384681702
training step: 53261, total_loss: 2.7293405532836914
training step: 53262, total_loss: 0.6343741416931152
training step: 53263, total_loss: 0.0904485359787941
training step: 53264, total_loss: 2.454531669616699
training step: 53265, total_loss: 0.8155176639556885
training step: 53266, total_loss: 1.7669577598571777
training step: 53267, total_loss: 0.5666906237602234
training step: 53268, total_loss: 1.316814661026001
training step: 53269, total_loss: 0.8579044938087463
training step: 53270, total_loss: 1.1513086557388306
training step: 53271, total_loss: 0.006472473964095116
training step: 53272, total_loss: 1.185318946838379
training step: 53273, total_loss: 3.5727367401123047
training step: 53274, total_loss: 1.702441692352295
training step: 53275, total_loss: 0.6724315881729126
training step: 53276, total_loss: 0.6861995458602905
training step: 53277, total_loss: 1.848236322402954
training step: 53278, total_loss: 1.123531460762024
training step: 53279, total_loss: 2.0910255908966064
training step: 53280, total_loss: 0.2556876540184021
training step: 53281, total_loss: 1.1963926553726196
training step: 53282, total_loss: 2.3713812828063965
training step: 53283, total_loss: 0.5881590247154236
training step: 53284, total_loss: 1.1994829177856445
training step: 53285, total_loss: 2.239055871963501
training step: 53286, total_loss: 0.6131129264831543
training step: 53287, total_loss: 0.6850816011428833
training step: 53288, total_loss: 0.00018559145973995328
training step: 53289, total_loss: 4.149692058563232
training step: 53290, total_loss: 3.1024744510650635
training step: 53291, total_loss: 2.5494422912597656
training step: 53292, total_loss: 1.084336519241333
training step: 53293, total_loss: 3.563755512237549
training step: 53294, total_loss: 1.2263092994689941
training step: 53295, total_loss: 3.1461284160614014
training step: 53296, total_loss: 0.8169887065887451
training step: 53297, total_loss: 1.3230717182159424
training step: 53298, total_loss: 2.952791213989258
training step: 53299, total_loss: 1.7544395923614502
training step: 53300, total_loss: 1.2669464349746704
training step: 53301, total_loss: 1.5123200416564941
training step: 53302, total_loss: 0.9059546589851379
training step: 53303, total_loss: 1.8955063819885254
training step: 53304, total_loss: 0.2928326427936554
training step: 53305, total_loss: 0.9009898900985718
training step: 53306, total_loss: 0.629092812538147
training step: 53307, total_loss: 2.113567590713501
training step: 53308, total_loss: 2.1668500900268555
training step: 53309, total_loss: 1.8351339101791382
training step: 53310, total_loss: 0.9871346950531006
training step: 53311, total_loss: 3.6542067527770996
training step: 53312, total_loss: 0.38308873772621155
training step: 53313, total_loss: 0.9654281139373779
training step: 53314, total_loss: 3.1934447288513184
training step: 53315, total_loss: 0.15666088461875916
training step: 53316, total_loss: 0.024439582601189613
training step: 53317, total_loss: 2.9808602333068848
training step: 53318, total_loss: 0.5031862258911133
training step: 53319, total_loss: 2.6592931747436523
training step: 53320, total_loss: 0.7821793556213379
training step: 53321, total_loss: 0.37495601177215576
training step: 53322, total_loss: 3.387901544570923
training step: 53323, total_loss: 4.406598091125488
training step: 53324, total_loss: 1.1009924411773682
training step: 53325, total_loss: 2.0647387504577637
training step: 53326, total_loss: 2.4332518577575684
training step: 53327, total_loss: 4.783447265625
training step: 53328, total_loss: 0.7164621353149414
training step: 53329, total_loss: 1.6202900409698486
training step: 53330, total_loss: 0.008561711758375168
training step: 53331, total_loss: 1.8765301704406738
training step: 53332, total_loss: 1.8006293773651123
training step: 53333, total_loss: 1.7077823877334595
training step: 53334, total_loss: 3.518991470336914
training step: 53335, total_loss: 1.482121229171753
training step: 53336, total_loss: 0.8624684810638428
training step: 53337, total_loss: 4.244444370269775
training step: 53338, total_loss: 1.99745774269104
training step: 53339, total_loss: 1.393782615661621
training step: 53340, total_loss: 1.4038747549057007
training step: 53341, total_loss: 2.8701677322387695
training step: 53342, total_loss: 1.7293665409088135
training step: 53343, total_loss: 0.46434980630874634
training step: 53344, total_loss: 1.6891651153564453
training step: 53345, total_loss: 0.5307086706161499
training step: 53346, total_loss: 0.17035949230194092
training step: 53347, total_loss: 3.992957592010498
training step: 53348, total_loss: 1.5697190761566162
training step: 53349, total_loss: 0.061345454305410385
training step: 53350, total_loss: 0.16226565837860107
training step: 53351, total_loss: 0.9953978061676025
training step: 53352, total_loss: 0.24520641565322876
training step: 53353, total_loss: 3.786111354827881
training step: 53354, total_loss: 3.776615619659424
training step: 53355, total_loss: 0.34709620475769043
training step: 53356, total_loss: 0.8975537419319153
training step: 53357, total_loss: 1.9371923208236694
training step: 53358, total_loss: 2.0565993785858154
training step: 53359, total_loss: 2.8515539169311523
training step: 53360, total_loss: 1.4240944385528564
training step: 53361, total_loss: 1.4571170806884766
training step: 53362, total_loss: 1.367274284362793
training step: 53363, total_loss: 3.1882240772247314
training step: 53364, total_loss: 3.239025115966797
training step: 53365, total_loss: 1.3811442852020264
training step: 53366, total_loss: 2.7970316410064697
training step: 53367, total_loss: 4.6319427490234375
training step: 53368, total_loss: 0.5618782639503479
training step: 53369, total_loss: 0.1016567125916481
training step: 53370, total_loss: 1.334885597229004
training step: 53371, total_loss: 1.4032151699066162
training step: 53372, total_loss: 2.973320245742798
training step: 53373, total_loss: 4.5233612060546875
training step: 53374, total_loss: 2.84371018409729
training step: 53375, total_loss: 2.4818484783172607
training step: 53376, total_loss: 0.0007546056294813752
training step: 53377, total_loss: 2.7767856121063232
training step: 53378, total_loss: 1.7184098958969116
training step: 53379, total_loss: 1.6530523300170898
training step: 53380, total_loss: 0.9136545658111572
training step: 53381, total_loss: 0.17359426617622375
training step: 53382, total_loss: 1.1476517915725708
training step: 53383, total_loss: 2.349754810333252
training step: 53384, total_loss: 1.831864356994629
training step: 53385, total_loss: 0.000752176740206778
training step: 53386, total_loss: 0.07424015551805496
training step: 53387, total_loss: 0.22071906924247742
training step: 53388, total_loss: 2.100856304168701
training step: 53389, total_loss: 2.2351326942443848
training step: 53390, total_loss: 0.9389660358428955
training step: 53391, total_loss: 4.326039791107178
training step: 53392, total_loss: 0.5018983483314514
training step: 53393, total_loss: 1.3886134624481201
training step: 53394, total_loss: 1.221101999282837
training step: 53395, total_loss: 0.4036322236061096
training step: 53396, total_loss: 1.38673996925354
training step: 53397, total_loss: 1.715205430984497
training step: 53398, total_loss: 0.0884101465344429
training step: 53399, total_loss: 0.7092351317405701
training step: 53400, total_loss: 0.8379746675491333
training step: 53401, total_loss: 1.0366915464401245
training step: 53402, total_loss: 2.4781501293182373
training step: 53403, total_loss: 1.3189103603363037
training step: 53404, total_loss: 1.8371262550354004
training step: 53405, total_loss: 1.1177079677581787
training step: 53406, total_loss: 2.0794713497161865
training step: 53407, total_loss: 0.048927243798971176
training step: 53408, total_loss: 1.2363250255584717
training step: 53409, total_loss: 2.486891508102417
training step: 53410, total_loss: 0.021901261061429977
training step: 53411, total_loss: 0.9061436653137207
training step: 53412, total_loss: 2.7717506885528564
training step: 53413, total_loss: 0.643683671951294
training step: 53414, total_loss: 0.000466756580863148
training step: 53415, total_loss: 1.5253114700317383
training step: 53416, total_loss: 2.97440505027771
training step: 53417, total_loss: 0.39278462529182434
training step: 53418, total_loss: 2.5345633029937744
training step: 53419, total_loss: 0.36640578508377075
training step: 53420, total_loss: 0.6021376848220825
training step: 53421, total_loss: 1.5167676210403442
training step: 53422, total_loss: 1.806942343711853
training step: 53423, total_loss: 2.047013282775879
training step: 53424, total_loss: 2.349853038787842
training step: 53425, total_loss: 2.240105152130127
training step: 53426, total_loss: 0.7600331902503967
training step: 53427, total_loss: 2.208096504211426
training step: 53428, total_loss: 6.4932708740234375
training step: 53429, total_loss: 2.6253652572631836
training step: 53430, total_loss: 2.15580677986145
training step: 53431, total_loss: 0.909796953201294
training step: 53432, total_loss: 2.7272305488586426
training step: 53433, total_loss: 0.018453313037753105
training step: 53434, total_loss: 1.2305493354797363
training step: 53435, total_loss: 1.834434986114502
training step: 53436, total_loss: 0.30106326937675476
training step: 53437, total_loss: 1.5971591472625732
training step: 53438, total_loss: 1.156538963317871
training step: 53439, total_loss: 3.7900888919830322
training step: 53440, total_loss: 1.847076654434204
training step: 53441, total_loss: 2.0767436027526855
training step: 53442, total_loss: 2.4139809608459473
training step: 53443, total_loss: 2.4709126949310303
training step: 53444, total_loss: 2.8410258293151855
training step: 53445, total_loss: 0.20505335927009583
training step: 53446, total_loss: 1.298356294631958
training step: 53447, total_loss: 0.24857287108898163
training step: 53448, total_loss: 2.258122444152832
training step: 53449, total_loss: 0.2892451286315918
training step: 53450, total_loss: 1.3593670129776
training step: 53451, total_loss: 1.2497665882110596
training step: 53452, total_loss: 1.824021816253662
training step: 53453, total_loss: 2.8450448513031006
training step: 53454, total_loss: 0.5092018842697144
training step: 53455, total_loss: 1.1030633449554443
training step: 53456, total_loss: 0.40922990441322327
training step: 53457, total_loss: 0.05714767798781395
training step: 53458, total_loss: 0.0024205842055380344
training step: 53459, total_loss: 2.9131112098693848
training step: 53460, total_loss: 1.3251776695251465
training step: 53461, total_loss: 0.7562240362167358
training step: 53462, total_loss: 3.3829197883605957
training step: 53463, total_loss: 1.9987530708312988
training step: 53464, total_loss: 1.2767064571380615
training step: 53465, total_loss: 2.830349922180176
training step: 53466, total_loss: 0.7626073956489563
training step: 53467, total_loss: 1.149111032485962
training step: 53468, total_loss: 2.601062297821045
training step: 53469, total_loss: 0.23428450524806976
training step: 53470, total_loss: 0.47037962079048157
training step: 53471, total_loss: 0.9266791343688965
training step: 53472, total_loss: 2.112358570098877
training step: 53473, total_loss: 1.0329086780548096
training step: 53474, total_loss: 0.003362992312759161
training step: 53475, total_loss: 2.6243715286254883
training step: 53476, total_loss: 4.406073093414307
training step: 53477, total_loss: 1.202582597732544
training step: 53478, total_loss: 0.8919656276702881
training step: 53479, total_loss: 0.8373312950134277
training step: 53480, total_loss: 1.7217414379119873
training step: 53481, total_loss: 0.0013273012591525912
training step: 53482, total_loss: 0.9794850945472717
training step: 53483, total_loss: 0.31675854325294495
training step: 53484, total_loss: 3.0164737701416016
training step: 53485, total_loss: 1.2856247425079346
training step: 53486, total_loss: 0.8699315786361694
training step: 53487, total_loss: 1.779911756515503
training step: 53488, total_loss: 2.5265328884124756
training step: 53489, total_loss: 1.695361852645874
training step: 53490, total_loss: 0.19145527482032776
training step: 53491, total_loss: 2.9091365337371826
training step: 53492, total_loss: 0.9010343551635742
training step: 53493, total_loss: 1.4616994857788086
training step: 53494, total_loss: 2.0040781497955322
training step: 53495, total_loss: 0.18724240362644196
training step: 53496, total_loss: 2.255765676498413
training step: 53497, total_loss: 0.8001237511634827
training step: 53498, total_loss: 1.2973287105560303
training step: 53499, total_loss: 0.6232139468193054
training step: 53500, total_loss: 1.724759578704834
training step: 53501, total_loss: 2.7108004093170166
training step: 53502, total_loss: 2.4716086387634277
training step: 53503, total_loss: 0.9464863538742065
training step: 53504, total_loss: 1.0188937187194824
training step: 53505, total_loss: 2.083951234817505
training step: 53506, total_loss: 0.0008202874450944364
training step: 53507, total_loss: 0.21722914278507233
training step: 53508, total_loss: 1.8795253038406372
training step: 53509, total_loss: 0.6325123906135559
training step: 53510, total_loss: 0.34301549196243286
training step: 53511, total_loss: 0.4420360326766968
training step: 53512, total_loss: 1.9691369533538818
training step: 53513, total_loss: 0.17338255047798157
training step: 53514, total_loss: 0.10910798609256744
training step: 53515, total_loss: 4.341584205627441
training step: 53516, total_loss: 0.14455702900886536
training step: 53517, total_loss: 0.010363860055804253
training step: 53518, total_loss: 1.7643914222717285
training step: 53519, total_loss: 1.9599871635437012
training step: 53520, total_loss: 2.598907947540283
training step: 53521, total_loss: 3.345038414001465
training step: 53522, total_loss: 0.05865102633833885
training step: 53523, total_loss: 0.1586909294128418
training step: 53524, total_loss: 3.497321605682373
training step: 53525, total_loss: 1.9042266607284546
training step: 53526, total_loss: 0.8749982118606567
training step: 53527, total_loss: 0.3602536916732788
training step: 53528, total_loss: 1.3586033582687378
training step: 53529, total_loss: 1.2755405902862549
training step: 53530, total_loss: 1.6062289476394653
training step: 53531, total_loss: 1.681067705154419
training step: 53532, total_loss: 0.5594192743301392
training step: 53533, total_loss: 0.12248463928699493
training step: 53534, total_loss: 0.08044535666704178
training step: 53535, total_loss: 1.237321376800537
training step: 53536, total_loss: 1.3252607583999634
training step: 53537, total_loss: 2.966355562210083
training step: 53538, total_loss: 0.5552890300750732
training step: 53539, total_loss: 0.6280468702316284
training step: 53540, total_loss: 1.9310200214385986
training step: 53541, total_loss: 1.499696969985962
training step: 53542, total_loss: 1.347614049911499
training step: 53543, total_loss: 0.054803527891635895
training step: 53544, total_loss: 0.39018237590789795
training step: 53545, total_loss: 2.746333122253418
training step: 53546, total_loss: 1.3977755308151245
training step: 53547, total_loss: 0.4476545751094818
training step: 53548, total_loss: 1.441965937614441
training step: 53549, total_loss: 2.483654499053955
training step: 53550, total_loss: 1.3978126049041748
training step: 53551, total_loss: 1.7776670455932617
training step: 53552, total_loss: 0.5533353090286255
training step: 53553, total_loss: 4.350420951843262
training step: 53554, total_loss: 0.1016819179058075
training step: 53555, total_loss: 0.00846236664801836
training step: 53556, total_loss: 2.3340795040130615
training step: 53557, total_loss: 4.856441497802734
training step: 53558, total_loss: 1.3199933767318726
training step: 53559, total_loss: 3.3725757598876953
training step: 53560, total_loss: 1.0639210939407349
training step: 53561, total_loss: 1.7567384243011475
training step: 53562, total_loss: 2.9499311447143555
training step: 53563, total_loss: 1.802005410194397
training step: 53564, total_loss: 0.25513628125190735
training step: 53565, total_loss: 3.348385810852051
training step: 53566, total_loss: 2.228940010070801
training step: 53567, total_loss: 1.0900241136550903
training step: 53568, total_loss: 3.2624454498291016
training step: 53569, total_loss: 3.4176039695739746
training step: 53570, total_loss: 0.15811756253242493
training step: 53571, total_loss: 1.0902950763702393
training step: 53572, total_loss: 0.9258613586425781
training step: 53573, total_loss: 1.1505391597747803
training step: 53574, total_loss: 1.8074450492858887
training step: 53575, total_loss: 0.8653615713119507
training step: 53576, total_loss: 0.9212130308151245
training step: 53577, total_loss: 1.045346975326538
training step: 53578, total_loss: 1.4048585891723633
training step: 53579, total_loss: 1.182793378829956
training step: 53580, total_loss: 1.1666635274887085
training step: 53581, total_loss: 1.206018090248108
training step: 53582, total_loss: 0.1566809117794037
training step: 53583, total_loss: 0.04977846145629883
training step: 53584, total_loss: 1.9935309886932373
training step: 53585, total_loss: 0.3292088508605957
training step: 53586, total_loss: 1.635105013847351
training step: 53587, total_loss: 2.313370704650879
training step: 53588, total_loss: 1.254237413406372
training step: 53589, total_loss: 0.7685585021972656
training step: 53590, total_loss: 3.523681163787842
training step: 53591, total_loss: 1.2372483015060425
training step: 53592, total_loss: 0.5262198448181152
training step: 53593, total_loss: 0.7063990831375122
training step: 53594, total_loss: 0.37993305921554565
training step: 53595, total_loss: 0.03129129856824875
training step: 53596, total_loss: 0.00015817792154848576
training step: 53597, total_loss: 0.0020779715850949287
training step: 53598, total_loss: 0.3196662366390228
training step: 53599, total_loss: 4.079737186431885
training step: 53600, total_loss: 1.5865668058395386
training step: 53601, total_loss: 0.005726554896682501
training step: 53602, total_loss: 1.8347595930099487
training step: 53603, total_loss: 0.4141842722892761
training step: 53604, total_loss: 1.2025569677352905
training step: 53605, total_loss: 0.6070619821548462
training step: 53606, total_loss: 1.043480396270752
training step: 53607, total_loss: 6.7184648513793945
training step: 53608, total_loss: 0.015133244916796684
training step: 53609, total_loss: 2.7591967582702637
training step: 53610, total_loss: 1.4830414056777954
training step: 53611, total_loss: 0.557031512260437
training step: 53612, total_loss: 2.4813101291656494
training step: 53613, total_loss: 1.2860037088394165
training step: 53614, total_loss: 0.37985312938690186
training step: 53615, total_loss: 2.3356237411499023
training step: 53616, total_loss: 3.8404414653778076
training step: 53617, total_loss: 1.2448523044586182
training step: 53618, total_loss: 0.024502677842974663
training step: 53619, total_loss: 3.3042221069335938
training step: 53620, total_loss: 1.8007745742797852
training step: 53621, total_loss: 1.6566734313964844
training step: 53622, total_loss: 0.8521477580070496
training step: 53623, total_loss: 0.08287938684225082
training step: 53624, total_loss: 2.2623186111450195
training step: 53625, total_loss: 1.0915586948394775
training step: 53626, total_loss: 2.029356002807617
training step: 53627, total_loss: 2.9973506927490234
training step: 53628, total_loss: 2.556743621826172
training step: 53629, total_loss: 0.9963425993919373
training step: 53630, total_loss: 2.7734928131103516
training step: 53631, total_loss: 1.153767704963684
training step: 53632, total_loss: 0.7842099666595459
training step: 53633, total_loss: 0.786715567111969
training step: 53634, total_loss: 4.932001113891602
training step: 53635, total_loss: 0.31248220801353455
training step: 53636, total_loss: 0.5324917435646057
training step: 53637, total_loss: 2.7063684463500977
training step: 53638, total_loss: 1.0919724702835083
training step: 53639, total_loss: 0.8296369314193726
training step: 53640, total_loss: 3.10518217086792
training step: 53641, total_loss: 3.4114644527435303
training step: 53642, total_loss: 1.6089134216308594
training step: 53643, total_loss: 3.345853328704834
training step: 53644, total_loss: 1.6690969467163086
training step: 53645, total_loss: 0.8586245775222778
training step: 53646, total_loss: 0.5573364496231079
training step: 53647, total_loss: 1.8484175205230713
training step: 53648, total_loss: 3.509551525115967
training step: 53649, total_loss: 3.3378024101257324
training step: 53650, total_loss: 2.562390089035034
training step: 53651, total_loss: 1.2433037757873535
training step: 53652, total_loss: 0.4699414074420929
training step: 53653, total_loss: 2.198472738265991
training step: 53654, total_loss: 3.5402374267578125
training step: 53655, total_loss: 0.42394500970840454
training step: 53656, total_loss: 1.89497971534729
training step: 53657, total_loss: 0.7958462238311768
training step: 53658, total_loss: 2.4913554191589355
training step: 53659, total_loss: 1.1116645336151123
training step: 53660, total_loss: 3.283517837524414
training step: 53661, total_loss: 1.822986364364624
training step: 53662, total_loss: 0.6037347912788391
training step: 53663, total_loss: 0.9080227613449097
training step: 53664, total_loss: 0.0001929778081830591
training step: 53665, total_loss: 0.07979561388492584
training step: 53666, total_loss: 0.7577796578407288
training step: 53667, total_loss: 1.7134852409362793
training step: 53668, total_loss: 0.9837468862533569
training step: 53669, total_loss: 1.2569153308868408
training step: 53670, total_loss: 0.08906438946723938
training step: 53671, total_loss: 1.3915042877197266
training step: 53672, total_loss: 1.937572717666626
training step: 53673, total_loss: 2.840121269226074
training step: 53674, total_loss: 0.9267610311508179
training step: 53675, total_loss: 3.7419800758361816
training step: 53676, total_loss: 1.4455664157867432
training step: 53677, total_loss: 2.7498884201049805
training step: 53678, total_loss: 1.6315503120422363
training step: 53679, total_loss: 0.2016734629869461
training step: 53680, total_loss: 3.163090229034424
training step: 53681, total_loss: 2.157141923904419
training step: 53682, total_loss: 3.6424615383148193
training step: 53683, total_loss: 2.586693286895752
training step: 53684, total_loss: 1.1563937664031982
training step: 53685, total_loss: 0.000501565111335367
training step: 53686, total_loss: 0.9650150537490845
training step: 53687, total_loss: 3.408938407897949
training step: 53688, total_loss: 1.1357755661010742
training step: 53689, total_loss: 0.7818401455879211
training step: 53690, total_loss: 5.12888765335083
training step: 53691, total_loss: 1.7784128189086914
training step: 53692, total_loss: 2.9398791790008545
training step: 53693, total_loss: 0.556327760219574
training step: 53694, total_loss: 0.8049077987670898
training step: 53695, total_loss: 2.074373960494995
training step: 53696, total_loss: 0.9373199343681335
training step: 53697, total_loss: 0.6496251821517944
training step: 53698, total_loss: 1.294273853302002
training step: 53699, total_loss: 2.8804941177368164
training step: 53700, total_loss: 1.2073709964752197
training step: 53701, total_loss: 1.5939301252365112
training step: 53702, total_loss: 1.6774694919586182
training step: 53703, total_loss: 3.938232421875
training step: 53704, total_loss: 2.2052080631256104
training step: 53705, total_loss: 0.5890192985534668
training step: 53706, total_loss: 2.169475555419922
training step: 53707, total_loss: 1.0720868110656738
training step: 53708, total_loss: 1.4026739597320557
training step: 53709, total_loss: 1.1883890628814697
training step: 53710, total_loss: 2.4577736854553223
training step: 53711, total_loss: 0.36916622519493103
training step: 53712, total_loss: 2.357668161392212
training step: 53713, total_loss: 2.33840274810791
training step: 53714, total_loss: 2.5836181640625
training step: 53715, total_loss: 1.7710679769515991
training step: 53716, total_loss: 1.6184360980987549
training step: 53717, total_loss: 2.1124000549316406
training step: 53718, total_loss: 0.6357423663139343
training step: 53719, total_loss: 0.7106673717498779
training step: 53720, total_loss: 3.0298523902893066
training step: 53721, total_loss: 2.7107603549957275
training step: 53722, total_loss: 0.27331286668777466
training step: 53723, total_loss: 1.2318813800811768
training step: 53724, total_loss: 1.9696433544158936
training step: 53725, total_loss: 1.1376445293426514
training step: 53726, total_loss: 1.8401885032653809
training step: 53727, total_loss: 3.0659494400024414
training step: 53728, total_loss: 1.2159208059310913
training step: 53729, total_loss: 0.8086110353469849
training step: 53730, total_loss: 2.894951343536377
training step: 53731, total_loss: 4.495973110198975
training step: 53732, total_loss: 1.1550116539001465
training step: 53733, total_loss: 1.7033002376556396
training step: 53734, total_loss: 1.4248977899551392
training step: 53735, total_loss: 1.317474126815796
training step: 53736, total_loss: 1.0949921607971191
training step: 53737, total_loss: 1.3023409843444824
training step: 53738, total_loss: 3.447558879852295
training step: 53739, total_loss: 1.4179584980010986
training step: 53740, total_loss: 1.8757209777832031
training step: 53741, total_loss: 0.6966882944107056
training step: 53742, total_loss: 1.8331339359283447
training step: 53743, total_loss: 2.0059382915496826
training step: 53744, total_loss: 0.013408930972218513
training step: 53745, total_loss: 2.034700870513916
training step: 53746, total_loss: 0.8110878467559814
training step: 53747, total_loss: 0.00743821868672967
training step: 53748, total_loss: 0.770652174949646
training step: 53749, total_loss: 1.5195374488830566
training step: 53750, total_loss: 0.04263001307845116
training step: 53751, total_loss: 0.7597931623458862
training step: 53752, total_loss: 1.647059440612793
training step: 53753, total_loss: 4.411170959472656
training step: 53754, total_loss: 1.200553059577942
training step: 53755, total_loss: 1.4227720499038696
training step: 53756, total_loss: 2.09030818939209
training step: 53757, total_loss: 1.3186572790145874
training step: 53758, total_loss: 0.0008472507470287383
training step: 53759, total_loss: 2.950273036956787
training step: 53760, total_loss: 0.8806988000869751
training step: 53761, total_loss: 1.792680025100708
training step: 53762, total_loss: 1.7716883420944214
training step: 53763, total_loss: 2.4884519577026367
training step: 53764, total_loss: 2.460430145263672
training step: 53765, total_loss: 0.8970481157302856
training step: 53766, total_loss: 0.47184526920318604
training step: 53767, total_loss: 1.3037909269332886
training step: 53768, total_loss: 3.17264986038208
training step: 53769, total_loss: 1.5975970029830933
training step: 53770, total_loss: 5.3711700439453125
training step: 53771, total_loss: 1.3021173477172852
training step: 53772, total_loss: 1.2794880867004395
training step: 53773, total_loss: 1.8118655681610107
training step: 53774, total_loss: 1.071677803993225
training step: 53775, total_loss: 5.641942024230957
training step: 53776, total_loss: 1.5043388605117798
training step: 53777, total_loss: 1.180410623550415
training step: 53778, total_loss: 1.4631189107894897
training step: 53779, total_loss: 0.04889052361249924
training step: 53780, total_loss: 2.0873186588287354
training step: 53781, total_loss: 0.21856243908405304
training step: 53782, total_loss: 1.4406195878982544
training step: 53783, total_loss: 3.1121773719787598
training step: 53784, total_loss: 1.1468287706375122
training step: 53785, total_loss: 1.587268590927124
training step: 53786, total_loss: 3.4801530838012695
training step: 53787, total_loss: 2.0298049449920654
training step: 53788, total_loss: 2.2481274604797363
training step: 53789, total_loss: 4.417642593383789
training step: 53790, total_loss: 0.4291330575942993
training step: 53791, total_loss: 1.65706205368042
training step: 53792, total_loss: 0.24536409974098206
training step: 53793, total_loss: 0.2555083930492401
training step: 53794, total_loss: 0.521661639213562
training step: 53795, total_loss: 2.472752332687378
training step: 53796, total_loss: 1.9364168643951416
training step: 53797, total_loss: 0.045863229781389236
training step: 53798, total_loss: 1.6888766288757324
training step: 53799, total_loss: 1.4208221435546875
training step: 53800, total_loss: 2.6595101356506348
training step: 53801, total_loss: 1.4943506717681885
training step: 53802, total_loss: 0.00930542778223753
training step: 53803, total_loss: 1.6237218379974365
training step: 53804, total_loss: 6.152874946594238
training step: 53805, total_loss: 1.7318551540374756
training step: 53806, total_loss: 1.7212262153625488
training step: 53807, total_loss: 2.378059148788452
training step: 53808, total_loss: 3.7991766929626465
training step: 53809, total_loss: 1.082344651222229
training step: 53810, total_loss: 0.7382527589797974
training step: 53811, total_loss: 0.14779962599277496
training step: 53812, total_loss: 1.6807283163070679
training step: 53813, total_loss: 0.7170172333717346
training step: 53814, total_loss: 0.2753903567790985
training step: 53815, total_loss: 0.9930346608161926
training step: 53816, total_loss: 2.945833921432495
training step: 53817, total_loss: 1.3743109703063965
training step: 53818, total_loss: 0.0019285236485302448
training step: 53819, total_loss: 2.441622018814087
training step: 53820, total_loss: 1.0263075828552246
training step: 53821, total_loss: 0.17181414365768433
training step: 53822, total_loss: 3.3764216899871826
training step: 53823, total_loss: 0.011842576786875725
training step: 53824, total_loss: 0.6882385611534119
training step: 53825, total_loss: 1.4264335632324219
training step: 53826, total_loss: 1.8253241777420044
training step: 53827, total_loss: 1.8536070585250854
training step: 53828, total_loss: 1.6384608745574951
training step: 53829, total_loss: 0.01730314828455448
training step: 53830, total_loss: 0.08395881205797195
training step: 53831, total_loss: 4.093117713928223
training step: 53832, total_loss: 7.549232482910156
training step: 53833, total_loss: 0.536586344242096
training step: 53834, total_loss: 0.9795494675636292
training step: 53835, total_loss: 1.6168205738067627
training step: 53836, total_loss: 0.3969115614891052
training step: 53837, total_loss: 0.9240798950195312
training step: 53838, total_loss: 3.061685562133789
training step: 53839, total_loss: 4.977130889892578
training step: 53840, total_loss: 2.030395269393921
training step: 53841, total_loss: 0.2775269150733948
training step: 53842, total_loss: 1.270070195198059
training step: 53843, total_loss: 2.986135244369507
training step: 53844, total_loss: 3.605283498764038
training step: 53845, total_loss: 0.031671978533267975
training step: 53846, total_loss: 0.9204146862030029
training step: 53847, total_loss: 0.4796029329299927
training step: 53848, total_loss: 3.785567045211792
training step: 53849, total_loss: 2.1055026054382324
training step: 53850, total_loss: 1.099890112876892
training step: 53851, total_loss: 1.5576115846633911
training step: 53852, total_loss: 0.7240743637084961
training step: 53853, total_loss: 1.7420127391815186
training step: 53854, total_loss: 0.6774897575378418
training step: 53855, total_loss: 2.350648880004883
training step: 53856, total_loss: 4.316756248474121
training step: 53857, total_loss: 3.4803404808044434
training step: 53858, total_loss: 0.9389881491661072
training step: 53859, total_loss: 0.0013651642948389053
training step: 53860, total_loss: 1.0210784673690796
training step: 53861, total_loss: 2.063187837600708
training step: 53862, total_loss: 2.7771248817443848
training step: 53863, total_loss: 2.4165115356445312
training step: 53864, total_loss: 1.2505155801773071
training step: 53865, total_loss: 0.9148588180541992
training step: 53866, total_loss: 3.995068073272705
training step: 53867, total_loss: 1.0079355239868164
training step: 53868, total_loss: 0.1772332787513733
training step: 53869, total_loss: 1.4123996496200562
training step: 53870, total_loss: 0.40883439779281616
training step: 53871, total_loss: 0.1020117998123169
training step: 53872, total_loss: 2.1337404251098633
training step: 53873, total_loss: 1.099868655204773
training step: 53874, total_loss: 0.2636445164680481
training step: 53875, total_loss: 3.181090831756592
training step: 53876, total_loss: 1.2350527048110962
training step: 53877, total_loss: 1.3696479797363281
training step: 53878, total_loss: 2.0623579025268555
training step: 53879, total_loss: 0.043252505362033844
training step: 53880, total_loss: 1.0228126049041748
training step: 53881, total_loss: 3.392948627471924
training step: 53882, total_loss: 1.2980327606201172
training step: 53883, total_loss: 0.1764039397239685
training step: 53884, total_loss: 0.525436520576477
training step: 53885, total_loss: 0.006459908094257116
training step: 53886, total_loss: 2.066227912902832
training step: 53887, total_loss: 4.254423141479492
training step: 53888, total_loss: 2.8896899223327637
training step: 53889, total_loss: 0.8246385455131531
training step: 53890, total_loss: 1.322920322418213
training step: 53891, total_loss: 0.442089706659317
training step: 53892, total_loss: 3.325241804122925
training step: 53893, total_loss: 1.2998936176300049
training step: 53894, total_loss: 2.113938331604004
training step: 53895, total_loss: 0.07372159510850906
training step: 53896, total_loss: 2.5838205814361572
training step: 53897, total_loss: 2.530888557434082
training step: 53898, total_loss: 1.3554283380508423
training step: 53899, total_loss: 0.9913500547409058
training step: 53900, total_loss: 1.8970708847045898
training step: 53901, total_loss: 1.07296884059906
training step: 53902, total_loss: 0.9969555139541626
training step: 53903, total_loss: 0.24262648820877075
training step: 53904, total_loss: 0.45031821727752686
training step: 53905, total_loss: 1.8015186786651611
training step: 53906, total_loss: 1.9544918537139893
training step: 53907, total_loss: 0.06376707553863525
training step: 53908, total_loss: 0.7261637449264526
training step: 53909, total_loss: 0.3812906742095947
training step: 53910, total_loss: 0.7252850532531738
training step: 53911, total_loss: 0.38589099049568176
training step: 53912, total_loss: 1.7611103057861328
training step: 53913, total_loss: 2.0061633586883545
training step: 53914, total_loss: 2.681499481201172
training step: 53915, total_loss: 5.471173286437988
training step: 53916, total_loss: 1.84935462474823
training step: 53917, total_loss: 0.6648930311203003
training step: 53918, total_loss: 1.306128740310669
training step: 53919, total_loss: 2.662653684616089
training step: 53920, total_loss: 0.009078789502382278
training step: 53921, total_loss: 0.5074924826622009
training step: 53922, total_loss: 5.055708885192871
training step: 53923, total_loss: 0.7789934873580933
training step: 53924, total_loss: 0.20468883216381073
training step: 53925, total_loss: 2.2616312503814697
training step: 53926, total_loss: 0.07066194713115692
training step: 53927, total_loss: 2.5963077545166016
training step: 53928, total_loss: 1.9712845087051392
training step: 53929, total_loss: 2.059922218322754
training step: 53930, total_loss: 1.6195180416107178
training step: 53931, total_loss: 0.6479922533035278
training step: 53932, total_loss: 0.9919439554214478
training step: 53933, total_loss: 2.6343770027160645INFO:tensorflow:Writing predictions to: test_output/predictions_54000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_54000.json

training step: 53934, total_loss: 0.2336396872997284
training step: 53935, total_loss: 0.5169627666473389
training step: 53936, total_loss: 0.03977087512612343
training step: 53937, total_loss: 0.3359057307243347
training step: 53938, total_loss: 0.6236855387687683
training step: 53939, total_loss: 3.1460986137390137
training step: 53940, total_loss: 1.8341734409332275
training step: 53941, total_loss: 2.7816264629364014
training step: 53942, total_loss: 2.1017954349517822
training step: 53943, total_loss: 3.280764579772949
training step: 53944, total_loss: 4.170279502868652
training step: 53945, total_loss: 3.8545114994049072
training step: 53946, total_loss: 5.523159027099609
training step: 53947, total_loss: 0.7182615995407104
training step: 53948, total_loss: 0.49440765380859375
training step: 53949, total_loss: 0.9627782702445984
training step: 53950, total_loss: 0.03489724546670914
training step: 53951, total_loss: 2.165346384048462
training step: 53952, total_loss: 1.518263816833496
training step: 53953, total_loss: 1.5940847396850586
training step: 53954, total_loss: 3.1487550735473633
training step: 53955, total_loss: 0.5655632019042969
training step: 53956, total_loss: 1.0609205961227417
training step: 53957, total_loss: 5.716647148132324
training step: 53958, total_loss: 0.8523616790771484
training step: 53959, total_loss: 0.1781974583864212
training step: 53960, total_loss: 1.7376446723937988
training step: 53961, total_loss: 1.5485327243804932
training step: 53962, total_loss: 1.096038818359375
training step: 53963, total_loss: 1.1627203226089478
training step: 53964, total_loss: 1.8953357934951782
training step: 53965, total_loss: 1.5381931066513062
training step: 53966, total_loss: 2.334773540496826
training step: 53967, total_loss: 0.4310622215270996
training step: 53968, total_loss: 3.867196559906006
training step: 53969, total_loss: 1.3660322427749634
training step: 53970, total_loss: 1.6229249238967896
training step: 53971, total_loss: 0.809610903263092
training step: 53972, total_loss: 3.451352596282959
training step: 53973, total_loss: 1.2015661001205444
training step: 53974, total_loss: 1.1906285285949707
training step: 53975, total_loss: 1.786010980606079
training step: 53976, total_loss: 0.29391685128211975
training step: 53977, total_loss: 0.480387806892395
training step: 53978, total_loss: 2.987244129180908
training step: 53979, total_loss: 1.9584050178527832
training step: 53980, total_loss: 3.715909481048584
training step: 53981, total_loss: 2.4973464012145996
training step: 53982, total_loss: 0.03363333269953728
training step: 53983, total_loss: 1.6939996480941772
training step: 53984, total_loss: 0.4130532443523407
training step: 53985, total_loss: 2.5220303535461426
training step: 53986, total_loss: 1.8992189168930054
training step: 53987, total_loss: 1.665144681930542
training step: 53988, total_loss: 0.673974335193634
training step: 53989, total_loss: 3.8010339736938477
training step: 53990, total_loss: 0.04662493243813515
training step: 53991, total_loss: 3.2797250747680664
training step: 53992, total_loss: 0.9755150079727173
training step: 53993, total_loss: 4.169549942016602
training step: 53994, total_loss: 0.9185378551483154
training step: 53995, total_loss: 0.43685734272003174
training step: 53996, total_loss: 0.002821987494826317
training step: 53997, total_loss: 0.3088729679584503
training step: 53998, total_loss: 0.2531111240386963
training step: 53999, total_loss: 1.2274008989334106
training step: 54000, total_loss: 1.6763396263122559
epoch finished! shuffle=False
evaluation: 18000, total_loss: 1.8674488067626953, f1: 56.88697186407615, followup: 28.845276129621176, yesno: 69.08565343070136, heq: 51.57462345960749, dheq: 2.8

Model saved in path test_output//model_54000.ckpt
training step: 54001, total_loss: 0.31145843863487244
training step: 54002, total_loss: 0.9020453095436096
training step: 54003, total_loss: 1.6607069969177246
training step: 54004, total_loss: 3.0383858680725098
training step: 54005, total_loss: 1.941481113433838
training step: 54006, total_loss: 1.8675673007965088
training step: 54007, total_loss: 0.879424512386322
training step: 54008, total_loss: 0.07771124690771103
training step: 54009, total_loss: 0.8487779498100281
training step: 54010, total_loss: 2.4499993324279785
training step: 54011, total_loss: 0.1760721504688263
training step: 54012, total_loss: 1.9195700883865356
training step: 54013, total_loss: 4.081326961517334
training step: 54014, total_loss: 1.6311304569244385
training step: 54015, total_loss: 1.4616780281066895
training step: 54016, total_loss: 0.13772670924663544
training step: 54017, total_loss: 1.4758853912353516
training step: 54018, total_loss: 1.3488260507583618
training step: 54019, total_loss: 1.2934268712997437
training step: 54020, total_loss: 4.794443130493164
training step: 54021, total_loss: 0.887723445892334
training step: 54022, total_loss: 1.1353434324264526
training step: 54023, total_loss: 1.761904239654541
training step: 54024, total_loss: 2.0289721488952637
training step: 54025, total_loss: 0.5938998460769653
training step: 54026, total_loss: 1.954641342163086
training step: 54027, total_loss: 1.0515097379684448
training step: 54028, total_loss: 2.075546979904175
training step: 54029, total_loss: 1.2594573497772217
training step: 54030, total_loss: 1.2797921895980835
training step: 54031, total_loss: 4.286429405212402
training step: 54032, total_loss: 2.2550008296966553
training step: 54033, total_loss: 6.649082660675049
training step: 54034, total_loss: 1.9139103889465332
training step: 54035, total_loss: 2.5164554119110107
training step: 54036, total_loss: 1.9365310668945312
training step: 54037, total_loss: 1.7583953142166138
training step: 54038, total_loss: 2.5938682556152344
training step: 54039, total_loss: 2.8758440017700195
training step: 54040, total_loss: 0.2887635827064514
training step: 54041, total_loss: 0.3760100305080414
training step: 54042, total_loss: 1.416939616203308
training step: 54043, total_loss: 0.45223569869995117
training step: 54044, total_loss: 0.2656739354133606
training step: 54045, total_loss: 1.24515962600708
training step: 54046, total_loss: 1.1180354356765747
training step: 54047, total_loss: 1.4300962686538696
training step: 54048, total_loss: 0.26273104548454285
training step: 54049, total_loss: 2.195209264755249
training step: 54050, total_loss: 0.6241313219070435
training step: 54051, total_loss: 1.82471764087677
training step: 54052, total_loss: 0.88856041431427
training step: 54053, total_loss: 1.2358616590499878
training step: 54054, total_loss: 1.3782082796096802
training step: 54055, total_loss: 2.133631944656372
training step: 54056, total_loss: 0.7158663868904114
training step: 54057, total_loss: 2.7617745399475098
training step: 54058, total_loss: 4.056702613830566
training step: 54059, total_loss: 0.8942798376083374
training step: 54060, total_loss: 1.0445605516433716
training step: 54061, total_loss: 1.9107780456542969
training step: 54062, total_loss: 0.8061361312866211
training step: 54063, total_loss: 1.730292797088623
training step: 54064, total_loss: 3.5414576530456543
training step: 54065, total_loss: 7.193370819091797
training step: 54066, total_loss: 0.29175665974617004
training step: 54067, total_loss: 4.052915573120117
training step: 54068, total_loss: 2.221343755722046
training step: 54069, total_loss: 1.0882500410079956
training step: 54070, total_loss: 0.7514439821243286
training step: 54071, total_loss: 0.959824800491333
training step: 54072, total_loss: 3.9733753204345703
training step: 54073, total_loss: 1.6330009698867798
training step: 54074, total_loss: 1.0101404190063477
training step: 54075, total_loss: 0.35802221298217773
training step: 54076, total_loss: 1.5956213474273682
training step: 54077, total_loss: 1.6210099458694458
training step: 54078, total_loss: 1.5659936666488647
training step: 54079, total_loss: 3.9441306591033936
training step: 54080, total_loss: 0.8472860455513
training step: 54081, total_loss: 1.311044692993164
training step: 54082, total_loss: 0.3534775972366333
training step: 54083, total_loss: 0.8476315140724182
training step: 54084, total_loss: 5.057165145874023
training step: 54085, total_loss: 2.8461928367614746
training step: 54086, total_loss: 1.3040330410003662
training step: 54087, total_loss: 0.32905998826026917
training step: 54088, total_loss: 0.011196795850992203
training step: 54089, total_loss: 0.850993275642395
training step: 54090, total_loss: 1.08426833152771
training step: 54091, total_loss: 1.5258138179779053
training step: 54092, total_loss: 1.737806797027588
training step: 54093, total_loss: 1.393287181854248
training step: 54094, total_loss: 1.7072293758392334
training step: 54095, total_loss: 1.4310312271118164
training step: 54096, total_loss: 0.7644977569580078
training step: 54097, total_loss: 0.7494282126426697
training step: 54098, total_loss: 0.3106914162635803
training step: 54099, total_loss: 1.7224284410476685
training step: 54100, total_loss: 0.3524511456489563
training step: 54101, total_loss: 1.948082685470581
training step: 54102, total_loss: 1.1315422058105469
training step: 54103, total_loss: 0.06666722893714905
training step: 54104, total_loss: 1.3923712968826294
training step: 54105, total_loss: 3.380527973175049
training step: 54106, total_loss: 2.7115159034729004
training step: 54107, total_loss: 0.019844073802232742
training step: 54108, total_loss: 1.6424403190612793
training step: 54109, total_loss: 2.6349844932556152
training step: 54110, total_loss: 1.9876136779785156
training step: 54111, total_loss: 2.758676052093506
training step: 54112, total_loss: 3.1242637634277344
training step: 54113, total_loss: 3.543053150177002
training step: 54114, total_loss: 1.1836011409759521
training step: 54115, total_loss: 0.2871021032333374
training step: 54116, total_loss: 0.5075207948684692
training step: 54117, total_loss: 1.8571388721466064
training step: 54118, total_loss: 0.4500409960746765
training step: 54119, total_loss: 2.865668773651123
training step: 54120, total_loss: 0.7829128503799438
training step: 54121, total_loss: 3.299684762954712
training step: 54122, total_loss: 2.262547492980957
training step: 54123, total_loss: 1.7075183391571045
training step: 54124, total_loss: 0.28397101163864136
training step: 54125, total_loss: 2.0369653701782227
training step: 54126, total_loss: 2.7666516304016113
training step: 54127, total_loss: 1.540686011314392
training step: 54128, total_loss: 0.19875934720039368
training step: 54129, total_loss: 1.1901168823242188
training step: 54130, total_loss: 0.493375301361084
training step: 54131, total_loss: 0.502677321434021
training step: 54132, total_loss: 1.3209502696990967
training step: 54133, total_loss: 3.175130844116211
training step: 54134, total_loss: 1.1596200466156006
training step: 54135, total_loss: 0.055843766778707504
training step: 54136, total_loss: 0.6305087208747864
training step: 54137, total_loss: 1.0911259651184082
training step: 54138, total_loss: 2.7886877059936523
training step: 54139, total_loss: 2.1198668479919434
training step: 54140, total_loss: 0.9458984136581421
training step: 54141, total_loss: 2.6553268432617188
training step: 54142, total_loss: 1.6989824771881104
training step: 54143, total_loss: 1.2448819875717163
training step: 54144, total_loss: 1.6958496570587158
training step: 54145, total_loss: 0.4650724530220032
training step: 54146, total_loss: 0.8922518491744995
training step: 54147, total_loss: 0.23388825356960297
training step: 54148, total_loss: 1.8979895114898682
training step: 54149, total_loss: 0.017591824755072594
training step: 54150, total_loss: 2.83744215965271
training step: 54151, total_loss: 2.581277847290039
training step: 54152, total_loss: 1.7800672054290771
training step: 54153, total_loss: 1.1322773694992065
training step: 54154, total_loss: 4.163419723510742
training step: 54155, total_loss: 2.5603883266448975
training step: 54156, total_loss: 1.2044117450714111
training step: 54157, total_loss: 1.6812989711761475
training step: 54158, total_loss: 1.6601850986480713
training step: 54159, total_loss: 0.32349878549575806
training step: 54160, total_loss: 0.006956042721867561
training step: 54161, total_loss: 1.1283226013183594
training step: 54162, total_loss: 1.1120927333831787
training step: 54163, total_loss: 0.029278095811605453
training step: 54164, total_loss: 2.708341360092163
training step: 54165, total_loss: 1.9923243522644043
training step: 54166, total_loss: 1.4278279542922974
training step: 54167, total_loss: 0.5122683048248291
training step: 54168, total_loss: 1.088847279548645
training step: 54169, total_loss: 1.089719295501709
training step: 54170, total_loss: 1.2969261407852173
training step: 54171, total_loss: 1.0382418632507324
training step: 54172, total_loss: 0.012620972469449043
training step: 54173, total_loss: 0.19817978143692017
training step: 54174, total_loss: 0.43406903743743896
training step: 54175, total_loss: 1.3816349506378174
training step: 54176, total_loss: 2.817997694015503
training step: 54177, total_loss: 0.12860111892223358
training step: 54178, total_loss: 1.2757172584533691
training step: 54179, total_loss: 0.09804973751306534
training step: 54180, total_loss: 0.9807658791542053
training step: 54181, total_loss: 0.18062371015548706
training step: 54182, total_loss: 3.1007776260375977
training step: 54183, total_loss: 0.037583813071250916
training step: 54184, total_loss: 0.49441856145858765
training step: 54185, total_loss: 0.6558231711387634
training step: 54186, total_loss: 1.5999112129211426
training step: 54187, total_loss: 1.222929835319519
training step: 54188, total_loss: 2.1359596252441406
training step: 54189, total_loss: 4.112782001495361
training step: 54190, total_loss: 1.6323997974395752
training step: 54191, total_loss: 3.19807505607605
training step: 54192, total_loss: 3.153453826904297
training step: 54193, total_loss: 1.4580492973327637
training step: 54194, total_loss: 4.09273624420166
training step: 54195, total_loss: 1.5306894779205322
training step: 54196, total_loss: 2.8379199504852295
training step: 54197, total_loss: 0.0003252637689001858
training step: 54198, total_loss: 0.12233024835586548
training step: 54199, total_loss: 3.953366756439209
training step: 54200, total_loss: 2.184884548187256
training step: 54201, total_loss: 3.5195813179016113
training step: 54202, total_loss: 3.7292559146881104
training step: 54203, total_loss: 2.326409339904785
training step: 54204, total_loss: 0.7014597058296204
training step: 54205, total_loss: 1.3405938148498535
training step: 54206, total_loss: 1.2516376972198486
training step: 54207, total_loss: 2.1368556022644043
training step: 54208, total_loss: 0.7928644418716431
training step: 54209, total_loss: 5.433629989624023
training step: 54210, total_loss: 0.1803494095802307
training step: 54211, total_loss: 1.0166276693344116
training step: 54212, total_loss: 3.191448211669922
training step: 54213, total_loss: 3.168200969696045
training step: 54214, total_loss: 2.6876602172851562
training step: 54215, total_loss: 1.88694167137146
training step: 54216, total_loss: 1.081526517868042
training step: 54217, total_loss: 3.6340508460998535
training step: 54218, total_loss: 0.25939467549324036
training step: 54219, total_loss: 3.247032880783081
training step: 54220, total_loss: 0.01290893740952015
training step: 54221, total_loss: 3.277434825897217
training step: 54222, total_loss: 1.977644681930542
training step: 54223, total_loss: 0.02670314535498619
training step: 54224, total_loss: 3.1329569816589355
training step: 54225, total_loss: 1.537528157234192
training step: 54226, total_loss: 0.9777546525001526
training step: 54227, total_loss: 3.812614679336548
training step: 54228, total_loss: 1.1506379842758179
training step: 54229, total_loss: 3.242647647857666
training step: 54230, total_loss: 0.6201664209365845
training step: 54231, total_loss: 2.5742273330688477
training step: 54232, total_loss: 1.7166972160339355
training step: 54233, total_loss: 0.03640316054224968
training step: 54234, total_loss: 0.3203819692134857
training step: 54235, total_loss: 1.941084623336792
training step: 54236, total_loss: 1.2126362323760986
training step: 54237, total_loss: 1.3485043048858643
training step: 54238, total_loss: 0.4235529601573944
training step: 54239, total_loss: 0.8699214458465576
training step: 54240, total_loss: 3.1382155418395996
training step: 54241, total_loss: 2.375128984451294
training step: 54242, total_loss: 5.165426731109619
training step: 54243, total_loss: 0.22211885452270508
training step: 54244, total_loss: 2.6464953422546387
training step: 54245, total_loss: 0.7455697059631348
training step: 54246, total_loss: 2.8716022968292236
training step: 54247, total_loss: 0.15223346650600433
training step: 54248, total_loss: 0.1414804905653
training step: 54249, total_loss: 1.1710700988769531
training step: 54250, total_loss: 0.14472900331020355
training step: 54251, total_loss: 2.4405808448791504
training step: 54252, total_loss: 1.5854058265686035
training step: 54253, total_loss: 1.202881097793579
training step: 54254, total_loss: 0.9591977596282959
training step: 54255, total_loss: 1.9199178218841553
training step: 54256, total_loss: 1.1189799308776855
training step: 54257, total_loss: 0.1358528584241867
training step: 54258, total_loss: 1.6571887731552124
training step: 54259, total_loss: 2.066662311553955
training step: 54260, total_loss: 0.9257352948188782
training step: 54261, total_loss: 2.541130542755127
training step: 54262, total_loss: 0.49007558822631836
training step: 54263, total_loss: 1.1622322797775269
training step: 54264, total_loss: 3.471184015274048
training step: 54265, total_loss: 2.043900489807129
training step: 54266, total_loss: 0.025058936327695847
training step: 54267, total_loss: 1.2681045532226562
training step: 54268, total_loss: 3.9246983528137207
training step: 54269, total_loss: 0.7953957319259644
training step: 54270, total_loss: 1.8619201183319092
training step: 54271, total_loss: 1.0299063920974731
training step: 54272, total_loss: 0.7518783211708069
training step: 54273, total_loss: 3.3488192558288574
training step: 54274, total_loss: 0.046087801456451416
training step: 54275, total_loss: 2.409543514251709
training step: 54276, total_loss: 1.0425200462341309
training step: 54277, total_loss: 0.44593173265457153
training step: 54278, total_loss: 2.814680337905884
training step: 54279, total_loss: 1.41883385181427
training step: 54280, total_loss: 1.573854684829712
training step: 54281, total_loss: 3.1154959201812744
training step: 54282, total_loss: 3.926910877227783
training step: 54283, total_loss: 2.8490829467773438
training step: 54284, total_loss: 1.9816596508026123
training step: 54285, total_loss: 0.6467307806015015
training step: 54286, total_loss: 0.7987369298934937
training step: 54287, total_loss: 1.7301297187805176
training step: 54288, total_loss: 0.49435657262802124
training step: 54289, total_loss: 1.852361798286438
training step: 54290, total_loss: 0.6369643211364746
training step: 54291, total_loss: 2.925555467605591
training step: 54292, total_loss: 3.1777093410491943
training step: 54293, total_loss: 1.2499949932098389
training step: 54294, total_loss: 0.7618305087089539
training step: 54295, total_loss: 0.02524152398109436
training step: 54296, total_loss: 0.03473745658993721
training step: 54297, total_loss: 1.6953518390655518
training step: 54298, total_loss: 3.4335012435913086
training step: 54299, total_loss: 0.8665107488632202
training step: 54300, total_loss: 3.6863112449645996
training step: 54301, total_loss: 0.7451843023300171
training step: 54302, total_loss: 0.7976560592651367
training step: 54303, total_loss: 1.012364387512207
training step: 54304, total_loss: 2.027376890182495
training step: 54305, total_loss: 0.015480849891901016
training step: 54306, total_loss: 3.6851248741149902
training step: 54307, total_loss: 1.593246340751648
training step: 54308, total_loss: 1.8546391725540161
training step: 54309, total_loss: 1.6404786109924316
training step: 54310, total_loss: 0.2481498420238495
training step: 54311, total_loss: 0.7053846120834351
training step: 54312, total_loss: 1.6657836437225342
training step: 54313, total_loss: 1.6100871562957764
training step: 54314, total_loss: 0.7342773675918579
training step: 54315, total_loss: 2.7601823806762695
training step: 54316, total_loss: 2.544631004333496
training step: 54317, total_loss: 0.6720802783966064
training step: 54318, total_loss: 1.8453717231750488
training step: 54319, total_loss: 2.2514126300811768
training step: 54320, total_loss: 0.31013357639312744
training step: 54321, total_loss: 0.762413740158081
training step: 54322, total_loss: 2.015974521636963
training step: 54323, total_loss: 1.2240058183670044
training step: 54324, total_loss: 0.042460355907678604
training step: 54325, total_loss: 1.9194494485855103
training step: 54326, total_loss: 1.5092601776123047
training step: 54327, total_loss: 2.540802478790283
training step: 54328, total_loss: 1.387556552886963
training step: 54329, total_loss: 3.1381025314331055
training step: 54330, total_loss: 1.5330545902252197
training step: 54331, total_loss: 3.202775716781616
training step: 54332, total_loss: 0.474523663520813
training step: 54333, total_loss: 1.2983216047286987
training step: 54334, total_loss: 0.047831661999225616
training step: 54335, total_loss: 2.4694015979766846
training step: 54336, total_loss: 2.89077091217041
training step: 54337, total_loss: 2.2851343154907227
training step: 54338, total_loss: 0.8191909790039062
training step: 54339, total_loss: 1.5145750045776367
training step: 54340, total_loss: 1.1658601760864258
training step: 54341, total_loss: 1.3048352003097534
training step: 54342, total_loss: 2.062727689743042
training step: 54343, total_loss: 0.6724298000335693
training step: 54344, total_loss: 3.2323408126831055
training step: 54345, total_loss: 0.7677451372146606
training step: 54346, total_loss: 2.3694264888763428
training step: 54347, total_loss: 3.346905469894409
training step: 54348, total_loss: 3.1034650802612305
training step: 54349, total_loss: 3.1414968967437744
training step: 54350, total_loss: 2.592374563217163
training step: 54351, total_loss: 0.6326167583465576
training step: 54352, total_loss: 1.0215331315994263
training step: 54353, total_loss: 1.5496835708618164
training step: 54354, total_loss: 2.107731819152832
training step: 54355, total_loss: 2.428945541381836
training step: 54356, total_loss: 1.4320062398910522
training step: 54357, total_loss: 1.2859697341918945
training step: 54358, total_loss: 0.34454426169395447
training step: 54359, total_loss: 1.3498737812042236
training step: 54360, total_loss: 0.06988933682441711
training step: 54361, total_loss: 0.7940201163291931
training step: 54362, total_loss: 1.5563455820083618
training step: 54363, total_loss: 2.767798900604248
training step: 54364, total_loss: 0.7648226022720337
training step: 54365, total_loss: 1.2866425514221191
training step: 54366, total_loss: 1.657470941543579
training step: 54367, total_loss: 0.39353758096694946
training step: 54368, total_loss: 3.1270267963409424
training step: 54369, total_loss: 2.2470107078552246
training step: 54370, total_loss: 0.8699105978012085
training step: 54371, total_loss: 0.5756048560142517
training step: 54372, total_loss: 0.8026350736618042
training step: 54373, total_loss: 0.9328162670135498
training step: 54374, total_loss: 3.5887155532836914
training step: 54375, total_loss: 3.024466037750244
training step: 54376, total_loss: 1.333197832107544
training step: 54377, total_loss: 1.3062739372253418
training step: 54378, total_loss: 2.093276262283325
training step: 54379, total_loss: 1.9965590238571167
training step: 54380, total_loss: 0.9275058507919312
training step: 54381, total_loss: 1.087514877319336
training step: 54382, total_loss: 0.12057699263095856
training step: 54383, total_loss: 0.943134069442749
training step: 54384, total_loss: 1.4764314889907837
training step: 54385, total_loss: 1.0155471563339233
training step: 54386, total_loss: 0.016445284709334373
training step: 54387, total_loss: 1.7534512281417847
training step: 54388, total_loss: 2.176058530807495
training step: 54389, total_loss: 1.3903815746307373
training step: 54390, total_loss: 1.447604775428772
training step: 54391, total_loss: 2.948652744293213
training step: 54392, total_loss: 4.459683418273926
training step: 54393, total_loss: 0.02784283459186554
training step: 54394, total_loss: 0.5511980056762695
training step: 54395, total_loss: 2.2711386680603027
training step: 54396, total_loss: 6.229074478149414
training step: 54397, total_loss: 2.396261692047119
training step: 54398, total_loss: 3.2959465980529785
training step: 54399, total_loss: 2.085947036743164
training step: 54400, total_loss: 0.8330330848693848
training step: 54401, total_loss: 0.736401379108429
training step: 54402, total_loss: 0.9910099506378174
training step: 54403, total_loss: 0.5897140502929688
training step: 54404, total_loss: 0.004279918037354946
training step: 54405, total_loss: 2.320194721221924
training step: 54406, total_loss: 1.2066738605499268
training step: 54407, total_loss: 0.4925488233566284
training step: 54408, total_loss: 2.6693944931030273
training step: 54409, total_loss: 1.151792287826538
training step: 54410, total_loss: 3.3809523582458496
training step: 54411, total_loss: 0.6362229585647583
training step: 54412, total_loss: 1.6525025367736816
training step: 54413, total_loss: 3.056445598602295
training step: 54414, total_loss: 1.6894254684448242
training step: 54415, total_loss: 1.1690483093261719
training step: 54416, total_loss: 0.014368526637554169
training step: 54417, total_loss: 1.7381300926208496
training step: 54418, total_loss: 3.947648048400879
training step: 54419, total_loss: 1.3443083763122559
training step: 54420, total_loss: 2.1963906288146973
training step: 54421, total_loss: 0.6390554904937744
training step: 54422, total_loss: 3.7209115028381348
training step: 54423, total_loss: 0.064435213804245
training step: 54424, total_loss: 1.6946336030960083
training step: 54425, total_loss: 0.26038241386413574
training step: 54426, total_loss: 2.6445326805114746
training step: 54427, total_loss: 2.2029385566711426
training step: 54428, total_loss: 2.5083940029144287
training step: 54429, total_loss: 1.2576401233673096
training step: 54430, total_loss: 1.8905363082885742
training step: 54431, total_loss: 3.464874744415283
training step: 54432, total_loss: 2.2851080894470215
training step: 54433, total_loss: 0.520002007484436
training step: 54434, total_loss: 3.0051732063293457
training step: 54435, total_loss: 0.001840775366872549
training step: 54436, total_loss: 1.0152091979980469
training step: 54437, total_loss: 1.8222944736480713
training step: 54438, total_loss: 1.1660796403884888
training step: 54439, total_loss: 2.8030238151550293
training step: 54440, total_loss: 0.29644256830215454
training step: 54441, total_loss: 0.8107022047042847
training step: 54442, total_loss: 1.2228741645812988
training step: 54443, total_loss: 2.6777544021606445
training step: 54444, total_loss: 2.072315216064453
training step: 54445, total_loss: 0.34378305077552795
training step: 54446, total_loss: 0.39277005195617676
training step: 54447, total_loss: 0.764327883720398
training step: 54448, total_loss: 2.186551094055176
training step: 54449, total_loss: 1.7049049139022827
training step: 54450, total_loss: 2.191474676132202
training step: 54451, total_loss: 2.1059377193450928
training step: 54452, total_loss: 0.9951858520507812
training step: 54453, total_loss: 3.420112133026123
training step: 54454, total_loss: 0.8508694171905518
training step: 54455, total_loss: 0.09198100864887238
training step: 54456, total_loss: 4.053793907165527
training step: 54457, total_loss: 0.37258613109588623
training step: 54458, total_loss: 0.46598100662231445
training step: 54459, total_loss: 5.271347999572754
training step: 54460, total_loss: 3.075193405151367
training step: 54461, total_loss: 1.799118995666504
training step: 54462, total_loss: 2.2497398853302
training step: 54463, total_loss: 3.33957576751709
training step: 54464, total_loss: 1.6080396175384521
training step: 54465, total_loss: 0.19584405422210693
training step: 54466, total_loss: 1.713057279586792
training step: 54467, total_loss: 1.7672297954559326
training step: 54468, total_loss: 2.6270599365234375
training step: 54469, total_loss: 1.34397554397583
training step: 54470, total_loss: 0.1964646428823471
training step: 54471, total_loss: 3.127774238586426
training step: 54472, total_loss: 2.281231164932251
training step: 54473, total_loss: 1.1428332328796387
training step: 54474, total_loss: 4.306887149810791
training step: 54475, total_loss: 0.8225974440574646
training step: 54476, total_loss: 1.4157869815826416
training step: 54477, total_loss: 0.4793369472026825
training step: 54478, total_loss: 0.8454891443252563
training step: 54479, total_loss: 0.4404812455177307
training step: 54480, total_loss: 0.9525916576385498
training step: 54481, total_loss: 2.7912380695343018
training step: 54482, total_loss: 2.3819751739501953
training step: 54483, total_loss: 1.8496501445770264
training step: 54484, total_loss: 0.7539858222007751
training step: 54485, total_loss: 1.6059231758117676
training step: 54486, total_loss: 3.668919086456299
training step: 54487, total_loss: 0.23504063487052917
training step: 54488, total_loss: 1.8349335193634033
training step: 54489, total_loss: 2.0024425983428955
training step: 54490, total_loss: 0.9436614513397217
training step: 54491, total_loss: 1.671347737312317
training step: 54492, total_loss: 1.6236432790756226
training step: 54493, total_loss: 0.8998210430145264
training step: 54494, total_loss: 4.3308610916137695
training step: 54495, total_loss: 1.7536325454711914
training step: 54496, total_loss: 2.0187649726867676
training step: 54497, total_loss: 0.18754228949546814
training step: 54498, total_loss: 1.2160403728485107
training step: 54499, total_loss: 2.3533411026000977
training step: 54500, total_loss: 1.4086999893188477
training step: 54501, total_loss: 2.101034641265869
training step: 54502, total_loss: 0.9588598012924194
training step: 54503, total_loss: 2.255913257598877
training step: 54504, total_loss: 1.6167762279510498
training step: 54505, total_loss: 0.509610652923584
training step: 54506, total_loss: 2.1585960388183594
training step: 54507, total_loss: 1.896097183227539
training step: 54508, total_loss: 2.3318276405334473
training step: 54509, total_loss: 1.5622265338897705
training step: 54510, total_loss: 0.5110458135604858
training step: 54511, total_loss: 0.026672888547182083
training step: 54512, total_loss: 1.8966937065124512
training step: 54513, total_loss: 1.7758513689041138
training step: 54514, total_loss: 3.335249423980713
training step: 54515, total_loss: 0.02275528945028782
training step: 54516, total_loss: 1.7980966567993164
training step: 54517, total_loss: 2.176602840423584
training step: 54518, total_loss: 1.390836477279663
training step: 54519, total_loss: 1.6786612272262573
training step: 54520, total_loss: 2.4033408164978027
training step: 54521, total_loss: 0.9789794683456421
training step: 54522, total_loss: 1.0337172746658325
training step: 54523, total_loss: 0.3503802716732025
training step: 54524, total_loss: 2.11677622795105
training step: 54525, total_loss: 0.04841545969247818
training step: 54526, total_loss: 0.21331146359443665
training step: 54527, total_loss: 1.1740853786468506
training step: 54528, total_loss: 0.20309752225875854
training step: 54529, total_loss: 0.9930415153503418
training step: 54530, total_loss: 0.25921568274497986
training step: 54531, total_loss: 1.4513123035430908
training step: 54532, total_loss: 0.5461259484291077
training step: 54533, total_loss: 0.12236727774143219
training step: 54534, total_loss: 0.4490492641925812
training step: 54535, total_loss: 2.120331048965454
training step: 54536, total_loss: 0.0022769325878471136
training step: 54537, total_loss: 0.2358740270137787
training step: 54538, total_loss: 2.223918914794922
training step: 54539, total_loss: 2.1845078468322754
training step: 54540, total_loss: 0.7428328990936279
training step: 54541, total_loss: 4.9642791748046875
training step: 54542, total_loss: 0.45349836349487305
training step: 54543, total_loss: 0.6826888918876648
training step: 54544, total_loss: 2.0578994750976562
training step: 54545, total_loss: 3.4250688552856445
training step: 54546, total_loss: 4.088518142700195
training step: 54547, total_loss: 0.7333359718322754
training step: 54548, total_loss: 1.6513080596923828
training step: 54549, total_loss: 0.5228568315505981
training step: 54550, total_loss: 1.4075193405151367
training step: 54551, total_loss: 1.6613892316818237
training step: 54552, total_loss: 2.4107298851013184
training step: 54553, total_loss: 1.6552987098693848
training step: 54554, total_loss: 8.188310623168945
training step: 54555, total_loss: 3.165740489959717
training step: 54556, total_loss: 2.794487953186035
training step: 54557, total_loss: 2.092719078063965
training step: 54558, total_loss: 0.005773106589913368
training step: 54559, total_loss: 0.6029881238937378
training step: 54560, total_loss: 1.1676654815673828
training step: 54561, total_loss: 0.3134399652481079
training step: 54562, total_loss: 4.112085342407227
training step: 54563, total_loss: 1.54286527633667
training step: 54564, total_loss: 1.2923450469970703
training step: 54565, total_loss: 0.19602245092391968
training step: 54566, total_loss: 0.47227102518081665
training step: 54567, total_loss: 2.9782919883728027
training step: 54568, total_loss: 0.13588999211788177
training step: 54569, total_loss: 0.3812645971775055
training step: 54570, total_loss: 2.7766213417053223
training step: 54571, total_loss: 1.3748245239257812
training step: 54572, total_loss: 1.1870636940002441
training step: 54573, total_loss: 1.4474200010299683
training step: 54574, total_loss: 2.165980815887451
training step: 54575, total_loss: 3.6329257488250732
training step: 54576, total_loss: 2.0082762241363525
training step: 54577, total_loss: 1.9403973817825317
training step: 54578, total_loss: 0.19652864336967468
training step: 54579, total_loss: 2.1581292152404785
training step: 54580, total_loss: 0.3502921164035797
training step: 54581, total_loss: 3.5762972831726074
training step: 54582, total_loss: 1.6798081398010254
training step: 54583, total_loss: 0.9424830079078674
training step: 54584, total_loss: 0.8838070034980774
training step: 54585, total_loss: 0.8970122337341309
training step: 54586, total_loss: 1.6472326517105103
training step: 54587, total_loss: 3.26937198638916
training step: 54588, total_loss: 2.0573208332061768
training step: 54589, total_loss: 1.227165699005127
training step: 54590, total_loss: 2.2024335861206055
training step: 54591, total_loss: 1.2973158359527588
training step: 54592, total_loss: 0.7830836772918701
training step: 54593, total_loss: 2.556086540222168
training step: 54594, total_loss: 1.719850778579712
training step: 54595, total_loss: 0.8552858829498291
training step: 54596, total_loss: 2.617455005645752
training step: 54597, total_loss: 0.052048470824956894
training step: 54598, total_loss: 0.47236430644989014
training step: 54599, total_loss: 0.7949253916740417
training step: 54600, total_loss: 0.5394657850265503
training step: 54601, total_loss: 1.3617644309997559
training step: 54602, total_loss: 0.39003899693489075
training step: 54603, total_loss: 0.25279501080513
training step: 54604, total_loss: 4.903326034545898
training step: 54605, total_loss: 3.780778408050537
training step: 54606, total_loss: 2.945833683013916
training step: 54607, total_loss: 4.1059417724609375
training step: 54608, total_loss: 0.18550437688827515
training step: 54609, total_loss: 3.7639970779418945
training step: 54610, total_loss: 1.2089115381240845
training step: 54611, total_loss: 3.6538021564483643
training step: 54612, total_loss: 0.6994563341140747
training step: 54613, total_loss: 1.6737418174743652
training step: 54614, total_loss: 1.257988691329956
training step: 54615, total_loss: 1.2503948211669922
training step: 54616, total_loss: 0.7303476333618164
training step: 54617, total_loss: 0.4959970712661743
training step: 54618, total_loss: 0.9512524604797363
training step: 54619, total_loss: 1.4640722274780273
training step: 54620, total_loss: 1.4455941915512085
training step: 54621, total_loss: 2.447244644165039
training step: 54622, total_loss: 1.4873268604278564
training step: 54623, total_loss: 2.2595715522766113
training step: 54624, total_loss: 1.4071640968322754
training step: 54625, total_loss: 3.051730155944824
training step: 54626, total_loss: 2.83073353767395
training step: 54627, total_loss: 1.4884915351867676
training step: 54628, total_loss: 2.1457114219665527
training step: 54629, total_loss: 1.2328789234161377
training step: 54630, total_loss: 0.3218696415424347
training step: 54631, total_loss: 0.8422576785087585
training step: 54632, total_loss: 0.90008944272995
training step: 54633, total_loss: 3.78698468208313
training step: 54634, total_loss: 2.974952459335327
training step: 54635, total_loss: 1.1335055828094482
training step: 54636, total_loss: 0.49908339977264404
training step: 54637, total_loss: 1.2930622100830078
training step: 54638, total_loss: 0.9847099781036377
training step: 54639, total_loss: 1.4831528663635254
training step: 54640, total_loss: 1.0897011756896973
training step: 54641, total_loss: 3.2860305309295654
training step: 54642, total_loss: 0.05832735449075699
training step: 54643, total_loss: 0.8618242740631104
training step: 54644, total_loss: 1.489904761314392
training step: 54645, total_loss: 2.107024669647217
training step: 54646, total_loss: 2.2198078632354736
training step: 54647, total_loss: 2.0557074546813965
training step: 54648, total_loss: 0.48094409704208374
training step: 54649, total_loss: 0.07194143533706665
training step: 54650, total_loss: 0.24133969843387604
training step: 54651, total_loss: 0.6421859860420227
training step: 54652, total_loss: 0.9631186127662659
training step: 54653, total_loss: 0.5335422158241272
training step: 54654, total_loss: 1.8096262216567993
training step: 54655, total_loss: 0.3050638735294342
training step: 54656, total_loss: 2.807168483734131
training step: 54657, total_loss: 0.6744247674942017
training step: 54658, total_loss: 0.8121814727783203
training step: 54659, total_loss: 4.952636241912842
training step: 54660, total_loss: 2.5515968799591064
training step: 54661, total_loss: 3.0313544273376465
training step: 54662, total_loss: 2.3385403156280518
training step: 54663, total_loss: 0.4983386993408203
training step: 54664, total_loss: 1.0358999967575073
training step: 54665, total_loss: 1.3448634147644043
training step: 54666, total_loss: 2.5211191177368164
training step: 54667, total_loss: 1.8063123226165771
training step: 54668, total_loss: 1.155676007270813
training step: 54669, total_loss: 2.7119617462158203
training step: 54670, total_loss: 0.06142512708902359
training step: 54671, total_loss: 0.6362205743789673
training step: 54672, total_loss: 1.9050486087799072
training step: 54673, total_loss: 0.7834295034408569
training step: 54674, total_loss: 0.4989262819290161
training step: 54675, total_loss: 0.6087230443954468
training step: 54676, total_loss: 3.3220982551574707
training step: 54677, total_loss: 1.6777024269104004
training step: 54678, total_loss: 0.31932294368743896
training step: 54679, total_loss: 0.9753326177597046
training step: 54680, total_loss: 1.7644131183624268
training step: 54681, total_loss: 0.032017141580581665
training step: 54682, total_loss: 1.8323400020599365
training step: 54683, total_loss: 0.36020615696907043
training step: 54684, total_loss: 0.6125271320343018
training step: 54685, total_loss: 1.852229118347168
training step: 54686, total_loss: 1.4599533081054688
training step: 54687, total_loss: 3.0706753730773926
training step: 54688, total_loss: 0.15016750991344452
training step: 54689, total_loss: 2.4427309036254883
training step: 54690, total_loss: 2.661916494369507
training step: 54691, total_loss: 0.8773398399353027
training step: 54692, total_loss: 6.236973762512207
training step: 54693, total_loss: 0.828362226486206
training step: 54694, total_loss: 0.7178782224655151
training step: 54695, total_loss: 7.443619728088379
training step: 54696, total_loss: 0.9245930314064026
training step: 54697, total_loss: 0.03492830693721771
training step: 54698, total_loss: 0.9553108215332031
training step: 54699, total_loss: 0.6857701539993286
training step: 54700, total_loss: 0.9624192714691162
training step: 54701, total_loss: 5.485530853271484
training step: 54702, total_loss: 2.116367816925049
training step: 54703, total_loss: 0.5224748849868774
training step: 54704, total_loss: 0.5043563842773438
training step: 54705, total_loss: 0.5988849997520447
training step: 54706, total_loss: 1.8938686847686768
training step: 54707, total_loss: 0.33481931686401367
training step: 54708, total_loss: 2.397747039794922
training step: 54709, total_loss: 1.1881359815597534
training step: 54710, total_loss: 0.5016406774520874
training step: 54711, total_loss: 1.6748312711715698
training step: 54712, total_loss: 0.9418926239013672
training step: 54713, total_loss: 0.5039989948272705
training step: 54714, total_loss: 3.2298362255096436
training step: 54715, total_loss: 2.1179349422454834
training step: 54716, total_loss: 1.2067404985427856
training step: 54717, total_loss: 1.6354992389678955
training step: 54718, total_loss: 0.8560571670532227
training step: 54719, total_loss: 0.15708079934120178
training step: 54720, total_loss: 0.748263955116272
training step: 54721, total_loss: 0.16266635060310364
training step: 54722, total_loss: 0.7041547298431396
training step: 54723, total_loss: 1.579898715019226
training step: 54724, total_loss: 1.2411774396896362
training step: 54725, total_loss: 0.003375519532710314
training step: 54726, total_loss: 0.1570010632276535
training step: 54727, total_loss: 0.6995059847831726
training step: 54728, total_loss: 1.6151806116104126
training step: 54729, total_loss: 3.5304689407348633
training step: 54730, total_loss: 0.7224037647247314
training step: 54731, total_loss: 0.00656473683193326
training step: 54732, total_loss: 0.06541258841753006
training step: 54733, total_loss: 1.4716167449951172
training step: 54734, total_loss: 1.9088928699493408
training step: 54735, total_loss: 0.5959259271621704
training step: 54736, total_loss: 2.309474468231201
training step: 54737, total_loss: 5.986451148986816
training step: 54738, total_loss: 1.4047702550888062
training step: 54739, total_loss: 4.910934925079346
training step: 54740, total_loss: 0.1521839201450348
training step: 54741, total_loss: 0.4660734534263611
training step: 54742, total_loss: 1.9140444993972778
training step: 54743, total_loss: 0.6753676533699036
training step: 54744, total_loss: 5.617947578430176
training step: 54745, total_loss: 0.21757085621356964
training step: 54746, total_loss: 1.8165849447250366
training step: 54747, total_loss: 1.257075309753418
training step: 54748, total_loss: 6.016102313995361
training step: 54749, total_loss: 2.028717517852783
training step: 54750, total_loss: 0.06244223564863205
training step: 54751, total_loss: 1.110247254371643
training step: 54752, total_loss: 1.5177375078201294
training step: 54753, total_loss: 2.431623697280884
training step: 54754, total_loss: 2.4232187271118164
training step: 54755, total_loss: 2.8410873413085938
training step: 54756, total_loss: 1.298537254333496
training step: 54757, total_loss: 0.5870339274406433
training step: 54758, total_loss: 1.1053118705749512
training step: 54759, total_loss: 0.7260053753852844
training step: 54760, total_loss: 0.5285000801086426
training step: 54761, total_loss: 0.7237699031829834
training step: 54762, total_loss: 0.5075361728668213
training step: 54763, total_loss: 0.08385033160448074
training step: 54764, total_loss: 0.9311619400978088
training step: 54765, total_loss: 0.35234615206718445
training step: 54766, total_loss: 1.4936426877975464
training step: 54767, total_loss: 1.812744379043579
training step: 54768, total_loss: 0.5643272995948792
training step: 54769, total_loss: 4.539093494415283
training step: 54770, total_loss: 0.6977417469024658
training step: 54771, total_loss: 0.8991177678108215
training step: 54772, total_loss: 2.030005693435669
training step: 54773, total_loss: 0.7081852555274963
training step: 54774, total_loss: 0.6083108186721802
training step: 54775, total_loss: 1.0222676992416382
training step: 54776, total_loss: 2.166889190673828
training step: 54777, total_loss: 1.0769908428192139
training step: 54778, total_loss: 3.3335392475128174
training step: 54779, total_loss: 0.7007147073745728
training step: 54780, total_loss: 0.046843018382787704
training step: 54781, total_loss: 0.618595540523529
training step: 54782, total_loss: 2.0312280654907227
training step: 54783, total_loss: 1.9644572734832764
training step: 54784, total_loss: 2.813197374343872
training step: 54785, total_loss: 1.353298544883728
training step: 54786, total_loss: 1.0434825420379639
training step: 54787, total_loss: 5.543872356414795
training step: 54788, total_loss: 0.022919129580259323
training step: 54789, total_loss: 0.02678910456597805
training step: 54790, total_loss: 1.8691422939300537
training step: 54791, total_loss: 0.4706614315509796
training step: 54792, total_loss: 5.153839111328125
training step: 54793, total_loss: 2.332526206970215
training step: 54794, total_loss: 0.22927716374397278
training step: 54795, total_loss: 1.7198927402496338
training step: 54796, total_loss: 0.0010311020305380225
training step: 54797, total_loss: 2.3292994499206543
training step: 54798, total_loss: 1.7827832698822021
training step: 54799, total_loss: 0.7103801369667053
training step: 54800, total_loss: 2.5335254669189453
training step: 54801, total_loss: 0.9772238731384277
training step: 54802, total_loss: 1.7484581470489502
training step: 54803, total_loss: 1.836775302886963
training step: 54804, total_loss: 2.6069159507751465
training step: 54805, total_loss: 0.7996730804443359
training step: 54806, total_loss: 0.28913259506225586
training step: 54807, total_loss: 1.6511791944503784
training step: 54808, total_loss: 4.273151874542236
training step: 54809, total_loss: 3.444150447845459
training step: 54810, total_loss: 3.8033411502838135
training step: 54811, total_loss: 2.9087657928466797
training step: 54812, total_loss: 0.09900522232055664
training step: 54813, total_loss: 1.2420544624328613
training step: 54814, total_loss: 1.9010803699493408
training step: 54815, total_loss: 1.9505960941314697
training step: 54816, total_loss: 0.9512169361114502
training step: 54817, total_loss: 1.742771863937378
training step: 54818, total_loss: 2.419788360595703
training step: 54819, total_loss: 0.2191750705242157
training step: 54820, total_loss: 0.7681485414505005
training step: 54821, total_loss: 1.3197680711746216
training step: 54822, total_loss: 0.022361161187291145
training step: 54823, total_loss: 1.4709641933441162
training step: 54824, total_loss: 1.7180906534194946
training step: 54825, total_loss: 1.4343044757843018
training step: 54826, total_loss: 1.19573974609375
training step: 54827, total_loss: 1.6001384258270264
training step: 54828, total_loss: 0.7493616938591003
training step: 54829, total_loss: 3.9557533264160156
training step: 54830, total_loss: 0.4756249785423279
training step: 54831, total_loss: 2.2457098960876465
training step: 54832, total_loss: 1.6950953006744385
training step: 54833, total_loss: 1.047090768814087
training step: 54834, total_loss: 1.2584056854248047
training step: 54835, total_loss: 0.7597823143005371
training step: 54836, total_loss: 0.12060266733169556
training step: 54837, total_loss: 2.288508892059326
training step: 54838, total_loss: 0.018129490315914154
training step: 54839, total_loss: 0.5692602396011353
training step: 54840, total_loss: 1.996570110321045
training step: 54841, total_loss: 1.1661441326141357
training step: 54842, total_loss: 3.2321181297302246
training step: 54843, total_loss: 2.6201796531677246
training step: 54844, total_loss: 0.9631599187850952
training step: 54845, total_loss: 3.411301374435425
training step: 54846, total_loss: 0.00033609094680286944
training step: 54847, total_loss: 1.2817436456680298
training step: 54848, total_loss: 1.3626421689987183
training step: 54849, total_loss: 1.4825352430343628
training step: 54850, total_loss: 0.6576228141784668
training step: 54851, total_loss: 1.322405219078064
training step: 54852, total_loss: 0.6842364072799683
training step: 54853, total_loss: 2.7724051475524902
training step: 54854, total_loss: 4.591161727905273
training step: 54855, total_loss: 1.8102566003799438
training step: 54856, total_loss: 0.0011236376594752073
training step: 54857, total_loss: 0.8454774022102356
training step: 54858, total_loss: 1.3719160556793213
training step: 54859, total_loss: 3.5314712524414062INFO:tensorflow:Writing predictions to: test_output/predictions_55000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_55000.json

training step: 54860, total_loss: 0.9826338291168213
training step: 54861, total_loss: 0.6254839897155762
training step: 54862, total_loss: 3.4620673656463623
training step: 54863, total_loss: 3.2135181427001953
training step: 54864, total_loss: 0.12568582594394684
training step: 54865, total_loss: 0.8299059867858887
training step: 54866, total_loss: 0.001510970643721521
training step: 54867, total_loss: 3.1104140281677246
training step: 54868, total_loss: 1.7192609310150146
training step: 54869, total_loss: 0.08796142041683197
training step: 54870, total_loss: 4.028576374053955
training step: 54871, total_loss: 0.876496434211731
training step: 54872, total_loss: 2.721540689468384
training step: 54873, total_loss: 2.4986283779144287
training step: 54874, total_loss: 4.010238170623779
training step: 54875, total_loss: 0.0010804850608110428
training step: 54876, total_loss: 0.37750810384750366
training step: 54877, total_loss: 0.3816394805908203
training step: 54878, total_loss: 1.905910611152649
training step: 54879, total_loss: 0.424152135848999
training step: 54880, total_loss: 2.07371187210083
training step: 54881, total_loss: 5.783947944641113
training step: 54882, total_loss: 0.4912688732147217
training step: 54883, total_loss: 0.10287228226661682
training step: 54884, total_loss: 1.427210807800293
training step: 54885, total_loss: 2.511380434036255
training step: 54886, total_loss: 4.284162998199463
training step: 54887, total_loss: 0.641539454460144
training step: 54888, total_loss: 1.5282065868377686
training step: 54889, total_loss: 3.9774796962738037
training step: 54890, total_loss: 2.7410945892333984
training step: 54891, total_loss: 0.41372811794281006
training step: 54892, total_loss: 1.7129286527633667
training step: 54893, total_loss: 0.5118066072463989
training step: 54894, total_loss: 4.79713249206543
training step: 54895, total_loss: 2.105870008468628
training step: 54896, total_loss: 3.191488027572632
training step: 54897, total_loss: 2.017793655395508
training step: 54898, total_loss: 2.866429328918457
training step: 54899, total_loss: 6.17771053314209
training step: 54900, total_loss: 0.223519429564476
training step: 54901, total_loss: 2.5313029289245605
training step: 54902, total_loss: 0.9294490814208984
training step: 54903, total_loss: 1.763096570968628
training step: 54904, total_loss: 4.169305324554443
training step: 54905, total_loss: 0.5826759338378906
training step: 54906, total_loss: 1.935124397277832
training step: 54907, total_loss: 2.714677572250366
training step: 54908, total_loss: 1.3772450685501099
training step: 54909, total_loss: 5.398680210113525
training step: 54910, total_loss: 0.7317454218864441
training step: 54911, total_loss: 0.4517330527305603
training step: 54912, total_loss: 1.3410204648971558
training step: 54913, total_loss: 0.25915080308914185
training step: 54914, total_loss: 0.09756003320217133
training step: 54915, total_loss: 0.6933692097663879
training step: 54916, total_loss: 0.6499089002609253
training step: 54917, total_loss: 1.9021350145339966
training step: 54918, total_loss: 0.13881224393844604
training step: 54919, total_loss: 0.9330275058746338
training step: 54920, total_loss: 0.5958908796310425
training step: 54921, total_loss: 0.867775559425354
training step: 54922, total_loss: 0.21645420789718628
training step: 54923, total_loss: 1.449845314025879
training step: 54924, total_loss: 0.7517780065536499
training step: 54925, total_loss: 0.006766078062355518
training step: 54926, total_loss: 0.064579539000988
training step: 54927, total_loss: 1.1118525266647339
training step: 54928, total_loss: 2.580247163772583
training step: 54929, total_loss: 1.2991098165512085
training step: 54930, total_loss: 0.03189675882458687
training step: 54931, total_loss: 2.627199649810791
training step: 54932, total_loss: 4.008066654205322
training step: 54933, total_loss: 0.5448931455612183
training step: 54934, total_loss: 2.118302345275879
training step: 54935, total_loss: 3.974961757659912
training step: 54936, total_loss: 1.9255890846252441
training step: 54937, total_loss: 0.014517983421683311
training step: 54938, total_loss: 0.9799456000328064
training step: 54939, total_loss: 1.7741689682006836
training step: 54940, total_loss: 3.513969659805298
training step: 54941, total_loss: 0.00904512032866478
training step: 54942, total_loss: 1.0572094917297363
training step: 54943, total_loss: 1.5151950120925903
training step: 54944, total_loss: 0.7872745990753174
training step: 54945, total_loss: 1.8361083269119263
training step: 54946, total_loss: 1.6928162574768066
training step: 54947, total_loss: 0.511743426322937
training step: 54948, total_loss: 2.156597375869751
training step: 54949, total_loss: 1.9809311628341675
training step: 54950, total_loss: 1.375556230545044
training step: 54951, total_loss: 3.0718324184417725
training step: 54952, total_loss: 1.2966279983520508
training step: 54953, total_loss: 3.117208480834961
training step: 54954, total_loss: 4.2079949378967285
training step: 54955, total_loss: 5.15186882019043
training step: 54956, total_loss: 2.4869723320007324
training step: 54957, total_loss: 1.2895946502685547
training step: 54958, total_loss: 2.200955867767334
training step: 54959, total_loss: 0.11147503554821014
training step: 54960, total_loss: 1.0382684469223022
training step: 54961, total_loss: 1.1066923141479492
training step: 54962, total_loss: 1.9807233810424805
training step: 54963, total_loss: 0.9067016839981079
training step: 54964, total_loss: 0.042294327169656754
training step: 54965, total_loss: 2.9397218227386475
training step: 54966, total_loss: 1.2490794658660889
training step: 54967, total_loss: 0.5360013842582703
training step: 54968, total_loss: 1.4499626159667969
training step: 54969, total_loss: 1.2743144035339355
training step: 54970, total_loss: 5.381837844848633
training step: 54971, total_loss: 0.17073050141334534
training step: 54972, total_loss: 1.7482647895812988
training step: 54973, total_loss: 2.7241551876068115
training step: 54974, total_loss: 0.3360767960548401
training step: 54975, total_loss: 0.7459622621536255
training step: 54976, total_loss: 0.3584836423397064
training step: 54977, total_loss: 0.5727292895317078
training step: 54978, total_loss: 2.818206787109375
training step: 54979, total_loss: 0.2870546579360962
training step: 54980, total_loss: 1.184770941734314
training step: 54981, total_loss: 1.626112937927246
training step: 54982, total_loss: 0.3549784719944
training step: 54983, total_loss: 2.174345016479492
training step: 54984, total_loss: 0.7780827283859253
training step: 54985, total_loss: 2.672795295715332
training step: 54986, total_loss: 2.03875732421875
training step: 54987, total_loss: 0.01201477088034153
training step: 54988, total_loss: 1.2081186771392822
training step: 54989, total_loss: 0.1642187237739563
training step: 54990, total_loss: 2.9372692108154297
training step: 54991, total_loss: 0.5581891536712646
training step: 54992, total_loss: 0.9362015724182129
training step: 54993, total_loss: 1.9080116748809814
training step: 54994, total_loss: 1.5950219631195068
training step: 54995, total_loss: 0.047778598964214325
training step: 54996, total_loss: 1.0017189979553223
training step: 54997, total_loss: 0.0002939261612482369
training step: 54998, total_loss: 3.305851936340332
training step: 54999, total_loss: 0.14210465550422668
training step: 55000, total_loss: 4.759185314178467
epoch finished! shuffle=False
evaluation: 19000, total_loss: 1.90946364402771, f1: 56.26076466009614, followup: 39.099345808610984, yesno: 65.93640651148638, heq: 51.45291343374411, dheq: 3.5

Model saved in path test_output//model_55000.ckpt
training step: 55001, total_loss: 2.181614637374878
training step: 55002, total_loss: 1.5828957557678223
training step: 55003, total_loss: 2.621450662612915
training step: 55004, total_loss: 0.6976163983345032
training step: 55005, total_loss: 3.785796880722046
training step: 55006, total_loss: 3.204774856567383
training step: 55007, total_loss: 1.2772878408432007
training step: 55008, total_loss: 0.8556199669837952
training step: 55009, total_loss: 1.643146276473999
training step: 55010, total_loss: 0.9288109540939331
training step: 55011, total_loss: 0.8569054007530212
training step: 55012, total_loss: 2.21494722366333
training step: 55013, total_loss: 0.16108523309230804
training step: 55014, total_loss: 1.7220947742462158
training step: 55015, total_loss: 2.1542716026306152
training step: 55016, total_loss: 0.5562217235565186
training step: 55017, total_loss: 2.4575705528259277
training step: 55018, total_loss: 1.7969954013824463
training step: 55019, total_loss: 1.7951865196228027
training step: 55020, total_loss: 0.3910291790962219
training step: 55021, total_loss: 0.5365415811538696
training step: 55022, total_loss: 1.9226990938186646
training step: 55023, total_loss: 2.4084248542785645
training step: 55024, total_loss: 0.7519843578338623
training step: 55025, total_loss: 5.241480827331543
training step: 55026, total_loss: 2.857079029083252
training step: 55027, total_loss: 0.13130195438861847
training step: 55028, total_loss: 4.062638282775879
training step: 55029, total_loss: 3.727576732635498
training step: 55030, total_loss: 0.21156379580497742
training step: 55031, total_loss: 2.813734769821167
training step: 55032, total_loss: 0.4844904839992523
training step: 55033, total_loss: 0.470691978931427
training step: 55034, total_loss: 2.7569644451141357
training step: 55035, total_loss: 1.9867440462112427
training step: 55036, total_loss: 0.7794584035873413
training step: 55037, total_loss: 1.7444548606872559
training step: 55038, total_loss: 3.112734317779541
training step: 55039, total_loss: 3.0486369132995605
training step: 55040, total_loss: 0.6487062573432922
training step: 55041, total_loss: 1.659632682800293
training step: 55042, total_loss: 3.366567611694336
training step: 55043, total_loss: 0.0003922408213838935
training step: 55044, total_loss: 2.2186715602874756
training step: 55045, total_loss: 1.1921424865722656
training step: 55046, total_loss: 1.0215791463851929
training step: 55047, total_loss: 0.5870634317398071
training step: 55048, total_loss: 2.0597898960113525
training step: 55049, total_loss: 3.5750699043273926
training step: 55050, total_loss: 1.534851312637329
training step: 55051, total_loss: 0.6331362724304199
training step: 55052, total_loss: 1.8214991092681885
training step: 55053, total_loss: 2.626400947570801
training step: 55054, total_loss: 1.0466761589050293
training step: 55055, total_loss: 2.0216894149780273
training step: 55056, total_loss: 4.464064121246338
training step: 55057, total_loss: 0.11467674374580383
training step: 55058, total_loss: 2.2691614627838135
training step: 55059, total_loss: 0.8186770081520081
training step: 55060, total_loss: 3.654043674468994
training step: 55061, total_loss: 0.12726782262325287
training step: 55062, total_loss: 4.952236652374268
training step: 55063, total_loss: 0.805601179599762
training step: 55064, total_loss: 1.334314227104187
training step: 55065, total_loss: 1.6351011991500854
training step: 55066, total_loss: 1.6534967422485352
training step: 55067, total_loss: 0.623832106590271
training step: 55068, total_loss: 1.4018712043762207
training step: 55069, total_loss: 0.9124705791473389
training step: 55070, total_loss: 1.4695284366607666
training step: 55071, total_loss: 1.5132579803466797
training step: 55072, total_loss: 0.57529217004776
training step: 55073, total_loss: 0.04597989469766617
training step: 55074, total_loss: 5.047590255737305
training step: 55075, total_loss: 0.6190686225891113
training step: 55076, total_loss: 0.10248390585184097
training step: 55077, total_loss: 2.818269729614258
training step: 55078, total_loss: 0.023115811869502068
training step: 55079, total_loss: 2.6062264442443848
training step: 55080, total_loss: 0.4024497866630554
training step: 55081, total_loss: 0.8395171165466309
training step: 55082, total_loss: 0.2858237624168396
training step: 55083, total_loss: 0.3767073154449463
training step: 55084, total_loss: 1.2648109197616577
training step: 55085, total_loss: 2.1999807357788086
training step: 55086, total_loss: 0.7043653726577759
training step: 55087, total_loss: 2.112809181213379
training step: 55088, total_loss: 0.23447299003601074
training step: 55089, total_loss: 0.9855176210403442
training step: 55090, total_loss: 0.5423890352249146
training step: 55091, total_loss: 2.6838631629943848
training step: 55092, total_loss: 1.6367334127426147
training step: 55093, total_loss: 2.1633007526397705
training step: 55094, total_loss: 0.5442572832107544
training step: 55095, total_loss: 1.4637601375579834
training step: 55096, total_loss: 5.197839260101318
training step: 55097, total_loss: 1.5139765739440918
training step: 55098, total_loss: 2.7646474838256836
training step: 55099, total_loss: 1.4818944931030273
training step: 55100, total_loss: 1.8871097564697266
training step: 55101, total_loss: 0.179609477519989
training step: 55102, total_loss: 0.8587483167648315
training step: 55103, total_loss: 1.2391917705535889
training step: 55104, total_loss: 0.06072928383946419
training step: 55105, total_loss: 1.3912943601608276
training step: 55106, total_loss: 3.270296573638916
training step: 55107, total_loss: 3.7610220909118652
training step: 55108, total_loss: 4.293381690979004
training step: 55109, total_loss: 2.8356893062591553
training step: 55110, total_loss: 1.4027533531188965
training step: 55111, total_loss: 1.7616702318191528
training step: 55112, total_loss: 0.03939247131347656
training step: 55113, total_loss: 3.630450963973999
training step: 55114, total_loss: 1.3380274772644043
training step: 55115, total_loss: 1.2226676940917969
training step: 55116, total_loss: 0.30696356296539307
training step: 55117, total_loss: 1.1862514019012451
training step: 55118, total_loss: 1.1249909400939941
training step: 55119, total_loss: 0.26020747423171997
training step: 55120, total_loss: 3.593045711517334
training step: 55121, total_loss: 0.25589078664779663
training step: 55122, total_loss: 1.0732342004776
training step: 55123, total_loss: 1.0991253852844238
training step: 55124, total_loss: 1.8011679649353027
training step: 55125, total_loss: 2.2095208168029785
training step: 55126, total_loss: 2.428750991821289
training step: 55127, total_loss: 4.496698379516602
training step: 55128, total_loss: 0.6154069900512695
training step: 55129, total_loss: 1.2024891376495361
training step: 55130, total_loss: 1.7583141326904297
training step: 55131, total_loss: 1.443854570388794
training step: 55132, total_loss: 0.34466129541397095
training step: 55133, total_loss: 1.6761257648468018
training step: 55134, total_loss: 2.3132879734039307
training step: 55135, total_loss: 0.700537919998169
training step: 55136, total_loss: 0.6456141471862793
training step: 55137, total_loss: 2.1077213287353516
training step: 55138, total_loss: 1.0140994787216187
training step: 55139, total_loss: 2.3539414405822754
training step: 55140, total_loss: 0.10859160125255585
training step: 55141, total_loss: 1.4697284698486328
training step: 55142, total_loss: 3.4401612281799316
training step: 55143, total_loss: 1.4651403427124023
training step: 55144, total_loss: 0.5298494100570679
training step: 55145, total_loss: 0.6593696475028992
training step: 55146, total_loss: 4.241999626159668
training step: 55147, total_loss: 4.1232757568359375
training step: 55148, total_loss: 2.8189053535461426
training step: 55149, total_loss: 3.0667002201080322
training step: 55150, total_loss: 0.9721093773841858
training step: 55151, total_loss: 1.4697728157043457
training step: 55152, total_loss: 0.24845819175243378
training step: 55153, total_loss: 0.7965494990348816
training step: 55154, total_loss: 1.0498591661453247
training step: 55155, total_loss: 0.8208614587783813
training step: 55156, total_loss: 2.569206476211548
training step: 55157, total_loss: 0.09945189952850342
training step: 55158, total_loss: 3.20736026763916
training step: 55159, total_loss: 2.758673667907715
training step: 55160, total_loss: 3.5404112339019775
training step: 55161, total_loss: 1.774524211883545
training step: 55162, total_loss: 2.3949923515319824
training step: 55163, total_loss: 2.254181385040283
training step: 55164, total_loss: 0.7689871788024902
training step: 55165, total_loss: 1.9823235273361206
training step: 55166, total_loss: 1.468329906463623
training step: 55167, total_loss: 1.0281740427017212
training step: 55168, total_loss: 1.419946312904358
training step: 55169, total_loss: 3.9969398975372314
training step: 55170, total_loss: 0.7074194550514221
training step: 55171, total_loss: 4.480677604675293
training step: 55172, total_loss: 2.3204431533813477
training step: 55173, total_loss: 3.536288261413574
training step: 55174, total_loss: 0.2785487473011017
training step: 55175, total_loss: 2.2046830654144287
training step: 55176, total_loss: 2.980184555053711
training step: 55177, total_loss: 0.4002644717693329
training step: 55178, total_loss: 1.4546442031860352
training step: 55179, total_loss: 0.7851921916007996
training step: 55180, total_loss: 3.249178409576416
training step: 55181, total_loss: 0.003769609611481428
training step: 55182, total_loss: 0.9305776357650757
training step: 55183, total_loss: 2.805901527404785
training step: 55184, total_loss: 1.8772993087768555
training step: 55185, total_loss: 1.3353850841522217
training step: 55186, total_loss: 0.21387259662151337
training step: 55187, total_loss: 0.10470926016569138
training step: 55188, total_loss: 1.5186965465545654
training step: 55189, total_loss: 0.3608349859714508
training step: 55190, total_loss: 2.8740930557250977
training step: 55191, total_loss: 0.8286994695663452
training step: 55192, total_loss: 1.0375250577926636
training step: 55193, total_loss: 3.615614175796509
training step: 55194, total_loss: 2.0143237113952637
training step: 55195, total_loss: 1.3388174772262573
training step: 55196, total_loss: 2.12333083152771
training step: 55197, total_loss: 2.535531520843506
training step: 55198, total_loss: 3.3812222480773926
training step: 55199, total_loss: 1.5633162260055542
training step: 55200, total_loss: 2.391712188720703
training step: 55201, total_loss: 0.4258376359939575
training step: 55202, total_loss: 0.234298437833786
training step: 55203, total_loss: 1.5609331130981445
training step: 55204, total_loss: 2.5258500576019287
training step: 55205, total_loss: 2.0752687454223633
training step: 55206, total_loss: 2.1496315002441406
training step: 55207, total_loss: 0.2206541746854782
training step: 55208, total_loss: 1.062838077545166
training step: 55209, total_loss: 0.18791083991527557
training step: 55210, total_loss: 0.18784987926483154
training step: 55211, total_loss: 0.8686901330947876
training step: 55212, total_loss: 1.2442528009414673
training step: 55213, total_loss: 1.7440330982208252
training step: 55214, total_loss: 0.024452969431877136
training step: 55215, total_loss: 2.0700249671936035
training step: 55216, total_loss: 1.57528817653656
training step: 55217, total_loss: 0.6085198521614075
training step: 55218, total_loss: 3.2957682609558105
training step: 55219, total_loss: 2.4186594486236572
training step: 55220, total_loss: 1.6752766370773315
training step: 55221, total_loss: 2.356341600418091
training step: 55222, total_loss: 1.0804165601730347
training step: 55223, total_loss: 3.3757243156433105
training step: 55224, total_loss: 1.373070240020752
training step: 55225, total_loss: 0.01143585704267025
training step: 55226, total_loss: 2.2665038108825684
training step: 55227, total_loss: 0.9653872847557068
training step: 55228, total_loss: 0.7522441744804382
training step: 55229, total_loss: 3.6062326431274414
training step: 55230, total_loss: 0.17055796086788177
training step: 55231, total_loss: 1.049406886100769
training step: 55232, total_loss: 3.4041709899902344
training step: 55233, total_loss: 1.8176560401916504
training step: 55234, total_loss: 2.1018481254577637
training step: 55235, total_loss: 0.29185545444488525
training step: 55236, total_loss: 2.8049874305725098
training step: 55237, total_loss: 1.636970043182373
training step: 55238, total_loss: 1.123049020767212
training step: 55239, total_loss: 1.0826646089553833
training step: 55240, total_loss: 2.453613519668579
training step: 55241, total_loss: 1.2076365947723389
training step: 55242, total_loss: 2.5123233795166016
training step: 55243, total_loss: 2.3639328479766846
training step: 55244, total_loss: 2.68977689743042
training step: 55245, total_loss: 1.4260634183883667
training step: 55246, total_loss: 2.222947597503662
training step: 55247, total_loss: 0.16252955794334412
training step: 55248, total_loss: 1.0107569694519043
training step: 55249, total_loss: 0.021009471267461777
training step: 55250, total_loss: 2.368823528289795
training step: 55251, total_loss: 2.6973605155944824
training step: 55252, total_loss: 1.4197723865509033
training step: 55253, total_loss: 0.545002818107605
training step: 55254, total_loss: 0.5853903293609619
training step: 55255, total_loss: 3.6269917488098145
training step: 55256, total_loss: 2.103776693344116
training step: 55257, total_loss: 0.04379768669605255
training step: 55258, total_loss: 0.2739943861961365
training step: 55259, total_loss: 1.8393803834915161
training step: 55260, total_loss: 0.5351136326789856
training step: 55261, total_loss: 1.1727790832519531
training step: 55262, total_loss: 3.4350838661193848
training step: 55263, total_loss: 2.3708953857421875
training step: 55264, total_loss: 1.051028847694397
training step: 55265, total_loss: 0.7543929815292358
training step: 55266, total_loss: 0.6863230466842651
training step: 55267, total_loss: 1.533495545387268
training step: 55268, total_loss: 0.6684454083442688
training step: 55269, total_loss: 0.1619952768087387
training step: 55270, total_loss: 3.786332130432129
training step: 55271, total_loss: 0.20923835039138794
training step: 55272, total_loss: 1.5461868047714233
training step: 55273, total_loss: 1.0132311582565308
training step: 55274, total_loss: 1.7810702323913574
training step: 55275, total_loss: 1.2170004844665527
training step: 55276, total_loss: 1.9791975021362305
training step: 55277, total_loss: 0.15298667550086975
training step: 55278, total_loss: 2.9650607109069824
training step: 55279, total_loss: 2.7900876998901367
training step: 55280, total_loss: 2.927196502685547
training step: 55281, total_loss: 0.9630970358848572
training step: 55282, total_loss: 0.46916237473487854
training step: 55283, total_loss: 2.157912015914917
training step: 55284, total_loss: 1.4265947341918945
training step: 55285, total_loss: 2.3786275386810303
training step: 55286, total_loss: 3.2396433353424072
training step: 55287, total_loss: 0.7209737300872803
training step: 55288, total_loss: 1.0443027019500732
training step: 55289, total_loss: 1.7904385328292847
training step: 55290, total_loss: 0.42493513226509094
training step: 55291, total_loss: 0.8847639560699463
training step: 55292, total_loss: 1.5977790355682373
training step: 55293, total_loss: 0.023623760789632797
training step: 55294, total_loss: 2.327812433242798
training step: 55295, total_loss: 0.8813521862030029
training step: 55296, total_loss: 0.19364935159683228
training step: 55297, total_loss: 3.257640838623047
training step: 55298, total_loss: 0.8534748554229736
training step: 55299, total_loss: 0.5531653761863708
training step: 55300, total_loss: 3.2261931896209717
training step: 55301, total_loss: 0.8052070140838623
training step: 55302, total_loss: 0.10375397652387619
training step: 55303, total_loss: 5.342720985412598
training step: 55304, total_loss: 0.016161292791366577
training step: 55305, total_loss: 1.383852481842041
training step: 55306, total_loss: 0.8714935779571533
training step: 55307, total_loss: 1.835425853729248
training step: 55308, total_loss: 0.8865648508071899
training step: 55309, total_loss: 2.7216637134552
training step: 55310, total_loss: 0.9749906659126282
training step: 55311, total_loss: 1.2852067947387695
training step: 55312, total_loss: 1.9054653644561768
training step: 55313, total_loss: 0.008255340158939362
training step: 55314, total_loss: 1.9197070598602295
training step: 55315, total_loss: 3.008094310760498
training step: 55316, total_loss: 1.7845757007598877
training step: 55317, total_loss: 2.201084852218628
training step: 55318, total_loss: 5.736930847167969
training step: 55319, total_loss: 0.3573419451713562
training step: 55320, total_loss: 3.97632098197937
training step: 55321, total_loss: 2.924387216567993
training step: 55322, total_loss: 0.001971479970961809
training step: 55323, total_loss: 2.892000675201416
training step: 55324, total_loss: 0.41527462005615234
training step: 55325, total_loss: 2.3891735076904297
training step: 55326, total_loss: 3.4285192489624023
training step: 55327, total_loss: 0.2707330882549286
training step: 55328, total_loss: 1.2850289344787598
training step: 55329, total_loss: 6.5909247398376465
training step: 55330, total_loss: 1.2832039594650269
training step: 55331, total_loss: 1.709240198135376
training step: 55332, total_loss: 0.7843087911605835
training step: 55333, total_loss: 1.7202653884887695
training step: 55334, total_loss: 0.34196937084198
training step: 55335, total_loss: 1.250810980796814
training step: 55336, total_loss: 4.543296813964844
training step: 55337, total_loss: 0.9792377948760986
training step: 55338, total_loss: 1.2828607559204102
training step: 55339, total_loss: 0.1416153609752655
training step: 55340, total_loss: 0.342986524105072
training step: 55341, total_loss: 0.9257925748825073
training step: 55342, total_loss: 1.3339362144470215
training step: 55343, total_loss: 1.8142225742340088
training step: 55344, total_loss: 1.6866662502288818
training step: 55345, total_loss: 0.39119037985801697
training step: 55346, total_loss: 3.4508862495422363
training step: 55347, total_loss: 2.25453782081604
training step: 55348, total_loss: 2.1744887828826904
training step: 55349, total_loss: 1.4835824966430664
training step: 55350, total_loss: 0.4884464144706726
training step: 55351, total_loss: 1.2620677947998047
training step: 55352, total_loss: 1.3617286682128906
training step: 55353, total_loss: 3.0673792362213135
training step: 55354, total_loss: 2.4339659214019775
training step: 55355, total_loss: 1.3100082874298096
training step: 55356, total_loss: 1.273594856262207
training step: 55357, total_loss: 1.643049716949463
training step: 55358, total_loss: 0.4174851179122925
training step: 55359, total_loss: 0.32757997512817383
training step: 55360, total_loss: 0.10618657618761063
training step: 55361, total_loss: 3.659698009490967
training step: 55362, total_loss: 0.8727845549583435
training step: 55363, total_loss: 4.0853424072265625
training step: 55364, total_loss: 1.572754144668579
training step: 55365, total_loss: 7.518980026245117
training step: 55366, total_loss: 0.8726360201835632
training step: 55367, total_loss: 3.9060707092285156
training step: 55368, total_loss: 0.46760764718055725
training step: 55369, total_loss: 0.6042097806930542
training step: 55370, total_loss: 2.171682596206665
training step: 55371, total_loss: 2.1672563552856445
training step: 55372, total_loss: 2.5484416484832764
training step: 55373, total_loss: 2.1238503456115723
training step: 55374, total_loss: 0.3235471844673157
training step: 55375, total_loss: 0.6124753355979919
training step: 55376, total_loss: 3.706083297729492
training step: 55377, total_loss: 3.9063687324523926
training step: 55378, total_loss: 1.172455906867981
training step: 55379, total_loss: 2.1462152004241943
training step: 55380, total_loss: 1.765028476715088
training step: 55381, total_loss: 1.9542951583862305
training step: 55382, total_loss: 0.6531016826629639
training step: 55383, total_loss: 0.46426188945770264
training step: 55384, total_loss: 0.499173104763031
training step: 55385, total_loss: 3.4936516284942627
training step: 55386, total_loss: 0.5283177495002747
training step: 55387, total_loss: 1.6214532852172852
training step: 55388, total_loss: 1.3712208271026611
training step: 55389, total_loss: 0.22872522473335266
training step: 55390, total_loss: 0.6740139722824097
training step: 55391, total_loss: 3.234290599822998
training step: 55392, total_loss: 1.8077508211135864
training step: 55393, total_loss: 0.5617417693138123
training step: 55394, total_loss: 0.03681745380163193
training step: 55395, total_loss: 0.2675892412662506
training step: 55396, total_loss: 0.9191030263900757
training step: 55397, total_loss: 0.39902082085609436
training step: 55398, total_loss: 0.1451493203639984
training step: 55399, total_loss: 3.8446741104125977
training step: 55400, total_loss: 1.4552124738693237
training step: 55401, total_loss: 1.583038330078125
training step: 55402, total_loss: 1.5138760805130005
training step: 55403, total_loss: 0.8148136138916016
training step: 55404, total_loss: 1.708322286605835
training step: 55405, total_loss: 1.1957710981369019
training step: 55406, total_loss: 1.0273925065994263
training step: 55407, total_loss: 2.977694272994995
training step: 55408, total_loss: 3.7593390941619873
training step: 55409, total_loss: 1.4513564109802246
training step: 55410, total_loss: 1.7295842170715332
training step: 55411, total_loss: 1.132819652557373
training step: 55412, total_loss: 0.21681803464889526
training step: 55413, total_loss: 3.762967109680176
training step: 55414, total_loss: 0.8911826610565186
training step: 55415, total_loss: 0.35599106550216675
training step: 55416, total_loss: 1.209855318069458
training step: 55417, total_loss: 1.7526907920837402
training step: 55418, total_loss: 2.95154070854187
training step: 55419, total_loss: 2.220658302307129
training step: 55420, total_loss: 1.524951457977295
training step: 55421, total_loss: 1.0014508962631226
training step: 55422, total_loss: 1.2899627685546875
training step: 55423, total_loss: 0.5025216341018677
training step: 55424, total_loss: 0.22106453776359558
training step: 55425, total_loss: 1.9279003143310547
training step: 55426, total_loss: 1.7421122789382935
training step: 55427, total_loss: 2.5987606048583984
training step: 55428, total_loss: 3.3439157009124756
training step: 55429, total_loss: 0.6689661741256714
training step: 55430, total_loss: 2.503190517425537
training step: 55431, total_loss: 3.0755326747894287
training step: 55432, total_loss: 0.8470326662063599
training step: 55433, total_loss: 2.2220473289489746
training step: 55434, total_loss: 3.0944161415100098
training step: 55435, total_loss: 1.9627063274383545
training step: 55436, total_loss: 0.8420920372009277
training step: 55437, total_loss: 2.067660331726074
training step: 55438, total_loss: 0.10480879247188568
training step: 55439, total_loss: 1.6820507049560547
training step: 55440, total_loss: 0.43276625871658325
training step: 55441, total_loss: 1.3721193075180054
training step: 55442, total_loss: 1.3014354705810547
training step: 55443, total_loss: 1.0268301963806152
training step: 55444, total_loss: 1.3350410461425781
training step: 55445, total_loss: 4.468557357788086
training step: 55446, total_loss: 1.2536479234695435
training step: 55447, total_loss: 0.36034464836120605
training step: 55448, total_loss: 0.2540873885154724
training step: 55449, total_loss: 0.4113612174987793
training step: 55450, total_loss: 0.24912062287330627
training step: 55451, total_loss: 3.0092415809631348
training step: 55452, total_loss: 0.3702802062034607
training step: 55453, total_loss: 0.4155084490776062
training step: 55454, total_loss: 0.20424950122833252
training step: 55455, total_loss: 0.7970343828201294
training step: 55456, total_loss: 3.583195209503174
training step: 55457, total_loss: 6.531854152679443
training step: 55458, total_loss: 2.9911956787109375
training step: 55459, total_loss: 0.0015646670944988728
training step: 55460, total_loss: 0.9804757833480835
training step: 55461, total_loss: 3.169205665588379
training step: 55462, total_loss: 0.4970620572566986
training step: 55463, total_loss: 2.081418037414551
training step: 55464, total_loss: 2.215806007385254
training step: 55465, total_loss: 2.573582172393799
training step: 55466, total_loss: 1.06844162940979
training step: 55467, total_loss: 0.3205096125602722
training step: 55468, total_loss: 1.9586524963378906
training step: 55469, total_loss: 4.386214256286621
training step: 55470, total_loss: 1.2484581470489502
training step: 55471, total_loss: 0.9664913415908813
training step: 55472, total_loss: 0.035026490688323975
training step: 55473, total_loss: 2.004547595977783
training step: 55474, total_loss: 3.1716861724853516
training step: 55475, total_loss: 3.1029605865478516
training step: 55476, total_loss: 2.272714138031006
training step: 55477, total_loss: 1.9782652854919434
training step: 55478, total_loss: 0.26287567615509033
training step: 55479, total_loss: 1.7237836122512817
training step: 55480, total_loss: 1.6275408267974854
training step: 55481, total_loss: 2.0989227294921875
training step: 55482, total_loss: 0.8637436628341675
training step: 55483, total_loss: 1.7052024602890015
training step: 55484, total_loss: 2.149613857269287
training step: 55485, total_loss: 3.0540826320648193
training step: 55486, total_loss: 2.15181827545166
training step: 55487, total_loss: 0.03804299607872963
training step: 55488, total_loss: 1.8442487716674805
training step: 55489, total_loss: 0.16556689143180847
training step: 55490, total_loss: 0.17182256281375885
training step: 55491, total_loss: 2.5495004653930664
training step: 55492, total_loss: 2.079040050506592
training step: 55493, total_loss: 1.53643000125885
training step: 55494, total_loss: 0.38330861926078796
training step: 55495, total_loss: 0.6882280707359314
training step: 55496, total_loss: 0.8433672189712524
training step: 55497, total_loss: 1.9659109115600586
training step: 55498, total_loss: 0.9036493301391602
training step: 55499, total_loss: 3.9920036792755127
training step: 55500, total_loss: 1.8953514099121094
training step: 55501, total_loss: 0.26933568716049194
training step: 55502, total_loss: 3.0909218788146973
training step: 55503, total_loss: 1.3387123346328735
training step: 55504, total_loss: 2.2289297580718994
training step: 55505, total_loss: 2.7895236015319824
training step: 55506, total_loss: 0.17127427458763123
training step: 55507, total_loss: 1.4804515838623047
training step: 55508, total_loss: 0.1540617048740387
training step: 55509, total_loss: 2.5464863777160645
training step: 55510, total_loss: 1.9343500137329102
training step: 55511, total_loss: 0.8905688524246216
training step: 55512, total_loss: 1.846407413482666
training step: 55513, total_loss: 3.0073084831237793
training step: 55514, total_loss: 1.9717276096343994
training step: 55515, total_loss: 1.8406126499176025
training step: 55516, total_loss: 4.3147687911987305
training step: 55517, total_loss: 0.9538572430610657
training step: 55518, total_loss: 1.321319580078125
training step: 55519, total_loss: 5.612323760986328
training step: 55520, total_loss: 0.2321321666240692
training step: 55521, total_loss: 0.2650507092475891
training step: 55522, total_loss: 0.6037266850471497
training step: 55523, total_loss: 1.457359790802002
training step: 55524, total_loss: 0.832946240901947
training step: 55525, total_loss: 1.2163068056106567
training step: 55526, total_loss: 2.6566083431243896
training step: 55527, total_loss: 3.4096438884735107
training step: 55528, total_loss: 1.7446043491363525
training step: 55529, total_loss: 1.6599175930023193
training step: 55530, total_loss: 0.873977780342102
training step: 55531, total_loss: 0.618803858757019
training step: 55532, total_loss: 2.8605453968048096
training step: 55533, total_loss: 1.6176989078521729
training step: 55534, total_loss: 0.003071961458772421
training step: 55535, total_loss: 1.974632978439331
training step: 55536, total_loss: 0.6766479015350342
training step: 55537, total_loss: 4.226556777954102
training step: 55538, total_loss: 0.7782989740371704
training step: 55539, total_loss: 3.340290069580078
training step: 55540, total_loss: 1.084072232246399
training step: 55541, total_loss: 1.2335376739501953
training step: 55542, total_loss: 2.5110843181610107
training step: 55543, total_loss: 1.324991226196289
training step: 55544, total_loss: 3.8421573638916016
training step: 55545, total_loss: 1.0314054489135742
training step: 55546, total_loss: 1.4634835720062256
training step: 55547, total_loss: 2.91581392288208
training step: 55548, total_loss: 0.4531172513961792
training step: 55549, total_loss: 2.945436954498291
training step: 55550, total_loss: 1.7598767280578613
training step: 55551, total_loss: 0.4189707338809967
training step: 55552, total_loss: 0.7510790824890137
training step: 55553, total_loss: 2.087444305419922
training step: 55554, total_loss: 1.1958595514297485
training step: 55555, total_loss: 2.0189623832702637
training step: 55556, total_loss: 1.299491047859192
training step: 55557, total_loss: 0.17125579714775085
training step: 55558, total_loss: 0.9191931486129761
training step: 55559, total_loss: 1.0353120565414429
training step: 55560, total_loss: 3.596682548522949
training step: 55561, total_loss: 2.3878066539764404
training step: 55562, total_loss: 1.0233986377716064
training step: 55563, total_loss: 1.334610939025879
training step: 55564, total_loss: 1.5121970176696777
training step: 55565, total_loss: 0.7511627078056335
training step: 55566, total_loss: 2.765838623046875
training step: 55567, total_loss: 1.363145112991333
training step: 55568, total_loss: 0.3344978094100952
training step: 55569, total_loss: 1.2883692979812622
training step: 55570, total_loss: 0.9407166242599487
training step: 55571, total_loss: 1.2997336387634277
training step: 55572, total_loss: 1.143357753753662
training step: 55573, total_loss: 0.06945998966693878
training step: 55574, total_loss: 2.449592113494873
training step: 55575, total_loss: 1.6633696556091309
training step: 55576, total_loss: 0.8718451857566833
training step: 55577, total_loss: 1.812309741973877
training step: 55578, total_loss: 1.2636479139328003
training step: 55579, total_loss: 0.16294732689857483
training step: 55580, total_loss: 1.6589012145996094
training step: 55581, total_loss: 2.621549129486084
training step: 55582, total_loss: 3.0358521938323975
training step: 55583, total_loss: 1.422605276107788
training step: 55584, total_loss: 2.3377737998962402
training step: 55585, total_loss: 1.4606437683105469
training step: 55586, total_loss: 0.8082646727561951
training step: 55587, total_loss: 0.8502098321914673
training step: 55588, total_loss: 0.5447741746902466
training step: 55589, total_loss: 0.38543030619621277
training step: 55590, total_loss: 1.6078767776489258
training step: 55591, total_loss: 3.44692325592041
training step: 55592, total_loss: 0.9649826884269714
training step: 55593, total_loss: 0.000977048184722662
training step: 55594, total_loss: 0.7746076583862305
training step: 55595, total_loss: 0.17827776074409485
training step: 55596, total_loss: 3.0041303634643555
training step: 55597, total_loss: 2.7786755561828613
training step: 55598, total_loss: 0.7807015180587769
training step: 55599, total_loss: 0.03352638706564903
training step: 55600, total_loss: 2.903998851776123
training step: 55601, total_loss: 0.621604323387146
training step: 55602, total_loss: 0.6652892231941223
training step: 55603, total_loss: 0.8588348627090454
training step: 55604, total_loss: 0.676160454750061
training step: 55605, total_loss: 0.0003881866578012705
training step: 55606, total_loss: 5.030777454376221
training step: 55607, total_loss: 3.651189088821411
training step: 55608, total_loss: 1.5514334440231323
training step: 55609, total_loss: 3.0822558403015137
training step: 55610, total_loss: 1.087040901184082
training step: 55611, total_loss: 0.0818188339471817
training step: 55612, total_loss: 3.1106152534484863
training step: 55613, total_loss: 1.9496150016784668
training step: 55614, total_loss: 3.444425344467163
training step: 55615, total_loss: 2.011519432067871
training step: 55616, total_loss: 3.9210140705108643
training step: 55617, total_loss: 0.0257997028529644
training step: 55618, total_loss: 1.7728286981582642
training step: 55619, total_loss: 2.3595657348632812
training step: 55620, total_loss: 0.30434393882751465
training step: 55621, total_loss: 0.580544114112854
training step: 55622, total_loss: 2.9869651794433594
training step: 55623, total_loss: 0.04727061837911606
training step: 55624, total_loss: 3.162844657897949
training step: 55625, total_loss: 1.0503807067871094
training step: 55626, total_loss: 1.4686145782470703
training step: 55627, total_loss: 2.6113193035125732
training step: 55628, total_loss: 0.0003322373959235847
training step: 55629, total_loss: 2.459101676940918
training step: 55630, total_loss: 2.187063217163086
training step: 55631, total_loss: 2.4438083171844482
training step: 55632, total_loss: 4.956854820251465
training step: 55633, total_loss: 4.434261322021484
training step: 55634, total_loss: 3.9005680084228516
training step: 55635, total_loss: 0.33022671937942505
training step: 55636, total_loss: 1.8397102355957031
training step: 55637, total_loss: 0.2083742320537567
training step: 55638, total_loss: 0.047189563512802124
training step: 55639, total_loss: 1.5509202480316162
training step: 55640, total_loss: 2.471160650253296
training step: 55641, total_loss: 1.700227975845337
training step: 55642, total_loss: 3.425830364227295
training step: 55643, total_loss: 1.0384056568145752
training step: 55644, total_loss: 3.994424819946289
training step: 55645, total_loss: 0.9774969220161438
training step: 55646, total_loss: 3.9455199241638184
training step: 55647, total_loss: 1.2987229824066162
training step: 55648, total_loss: 2.1910653114318848
training step: 55649, total_loss: 3.5628161430358887
training step: 55650, total_loss: 0.5369786024093628
training step: 55651, total_loss: 1.1181678771972656
training step: 55652, total_loss: 1.7315703630447388
training step: 55653, total_loss: 2.0177109241485596
training step: 55654, total_loss: 1.8741447925567627
training step: 55655, total_loss: 2.9862780570983887
training step: 55656, total_loss: 1.9316807985305786
training step: 55657, total_loss: 0.5927373170852661
training step: 55658, total_loss: 1.1677601337432861
training step: 55659, total_loss: 1.363520860671997
training step: 55660, total_loss: 1.1953423023223877
training step: 55661, total_loss: 2.8302154541015625
training step: 55662, total_loss: 0.6428578495979309
training step: 55663, total_loss: 0.0004804746131412685
training step: 55664, total_loss: 0.538093626499176
training step: 55665, total_loss: 1.9472558498382568
training step: 55666, total_loss: 0.5690088272094727
training step: 55667, total_loss: 3.0790634155273438
training step: 55668, total_loss: 0.9622766971588135
training step: 55669, total_loss: 2.4049017429351807
training step: 55670, total_loss: 0.9085825085639954
training step: 55671, total_loss: 0.0003486167988739908
training step: 55672, total_loss: 0.39027541875839233
training step: 55673, total_loss: 3.699120283126831
training step: 55674, total_loss: 2.2811081409454346
training step: 55675, total_loss: 1.6418614387512207
training step: 55676, total_loss: 3.8062217235565186
training step: 55677, total_loss: 0.5790958404541016
training step: 55678, total_loss: 2.6385421752929688
training step: 55679, total_loss: 0.7973172664642334
training step: 55680, total_loss: 1.3899904489517212
training step: 55681, total_loss: 4.4102067947387695
training step: 55682, total_loss: 3.7781949043273926
training step: 55683, total_loss: 0.043175991624593735
training step: 55684, total_loss: 1.5654011964797974
training step: 55685, total_loss: 4.14375638961792
training step: 55686, total_loss: 2.7342729568481445
training step: 55687, total_loss: 0.0018844293663278222
training step: 55688, total_loss: 0.49619606137275696
training step: 55689, total_loss: 1.5128649473190308
training step: 55690, total_loss: 1.4642690420150757
training step: 55691, total_loss: 2.328981876373291
training step: 55692, total_loss: 3.404010772705078
training step: 55693, total_loss: 1.253418207168579
training step: 55694, total_loss: 0.994970977306366
training step: 55695, total_loss: 0.892018735408783
training step: 55696, total_loss: 0.42088866233825684
training step: 55697, total_loss: 0.7010487914085388
training step: 55698, total_loss: 0.2823050618171692
training step: 55699, total_loss: 2.0169107913970947
training step: 55700, total_loss: 1.1506013870239258
training step: 55701, total_loss: 0.1213979497551918
training step: 55702, total_loss: 2.5943918228149414
training step: 55703, total_loss: 2.880415439605713
training step: 55704, total_loss: 3.6926136016845703
training step: 55705, total_loss: 1.9553711414337158
training step: 55706, total_loss: 2.0381202697753906
training step: 55707, total_loss: 2.314356803894043
training step: 55708, total_loss: 0.6976339817047119
training step: 55709, total_loss: 1.8234922885894775
training step: 55710, total_loss: 3.0669124126434326
training step: 55711, total_loss: 1.8476767539978027
training step: 55712, total_loss: 0.8119294047355652
training step: 55713, total_loss: 0.5329896211624146
training step: 55714, total_loss: 1.750933289527893
training step: 55715, total_loss: 2.0068130493164062
training step: 55716, total_loss: 3.9966073036193848
training step: 55717, total_loss: 1.7082533836364746
training step: 55718, total_loss: 1.8772696256637573
training step: 55719, total_loss: 1.5312395095825195
training step: 55720, total_loss: 1.4289159774780273
training step: 55721, total_loss: 0.4465975761413574
training step: 55722, total_loss: 2.8642213344573975
training step: 55723, total_loss: 0.9983856081962585
training step: 55724, total_loss: 2.968478202819824
training step: 55725, total_loss: 4.674918174743652
training step: 55726, total_loss: 0.5369377732276917
training step: 55727, total_loss: 1.4673372507095337
training step: 55728, total_loss: 0.4605151414871216
training step: 55729, total_loss: 0.9559071660041809
training step: 55730, total_loss: 2.0722975730895996
training step: 55731, total_loss: 3.091029644012451
training step: 55732, total_loss: 3.829528570175171
training step: 55733, total_loss: 0.030956830829381943
training step: 55734, total_loss: 0.916144073009491
training step: 55735, total_loss: 0.5356945991516113
training step: 55736, total_loss: 1.2907754182815552
training step: 55737, total_loss: 0.007158748805522919
training step: 55738, total_loss: 3.272261619567871
training step: 55739, total_loss: 0.4041188955307007
training step: 55740, total_loss: 0.3884367346763611
training step: 55741, total_loss: 2.3570075035095215
training step: 55742, total_loss: 1.3027006387710571
training step: 55743, total_loss: 3.6857261657714844
training step: 55744, total_loss: 3.424269676208496
training step: 55745, total_loss: 0.2082815170288086
training step: 55746, total_loss: 0.5048857927322388
training step: 55747, total_loss: 0.1106804758310318
training step: 55748, total_loss: 1.0469269752502441
training step: 55749, total_loss: 2.903275966644287
training step: 55750, total_loss: 1.2133369445800781
training step: 55751, total_loss: 4.698887825012207
training step: 55752, total_loss: 2.620532989501953
training step: 55753, total_loss: 1.6433653831481934
training step: 55754, total_loss: 0.3356972634792328
training step: 55755, total_loss: 2.9324162006378174
training step: 55756, total_loss: 2.855201244354248
training step: 55757, total_loss: 1.2615783214569092
training step: 55758, total_loss: 2.1555163860321045
training step: 55759, total_loss: 2.4545626640319824
training step: 55760, total_loss: 1.2510839700698853
training step: 55761, total_loss: 4.931457996368408
training step: 55762, total_loss: 1.0989092588424683
training step: 55763, total_loss: 0.013472369872033596
training step: 55764, total_loss: 0.9907376170158386
training step: 55765, total_loss: 2.2417824268341064
training step: 55766, total_loss: 1.6980352401733398
training step: 55767, total_loss: 3.9526681900024414
training step: 55768, total_loss: 4.328738212585449
training step: 55769, total_loss: 3.3374972343444824
training step: 55770, total_loss: 0.6472052335739136
training step: 55771, total_loss: 0.08987079560756683
training step: 55772, total_loss: 1.3915717601776123
training step: 55773, total_loss: 3.8385210037231445
training step: 55774, total_loss: 1.6756733655929565
training step: 55775, total_loss: 1.7227685451507568
training step: 55776, total_loss: 0.8656674027442932
training step: 55777, total_loss: 0.6225205063819885
training step: 55778, total_loss: 0.12879766523838043
training step: 55779, total_loss: 0.9290589094161987
training step: 55780, total_loss: 1.2927017211914062
training step: 55781, total_loss: 0.20823685824871063
training step: 55782, total_loss: 1.1400146484375
training step: 55783, total_loss: 1.450033187866211
training step: 55784, total_loss: 1.7913689613342285
training step: 55785, total_loss: 5.37464714050293
training step: 55786, total_loss: 1.1395599842071533
training step: 55787, total_loss: 2.76690673828125
training step: 55788, total_loss: 1.5522009134292603
training step: 55789, total_loss: 0.0459226556122303
training step: 55790, total_loss: 1.3918181657791138
training step: 55791, total_loss: 2.5581765174865723
training step: 55792, total_loss: 0.024620123207569122
training step: 55793, total_loss: 0.47342532873153687
training step: 55794, total_loss: 0.799028754234314
training step: 55795, total_loss: 1.1844096183776855
training step: 55796, total_loss: 1.5694981813430786
training step: 55797, total_loss: 3.6605165004730225
training step: 55798, total_loss: 0.9371448755264282
training step: 55799, total_loss: 0.6683756709098816
training step: 55800, total_loss: 1.983489990234375
training step: 55801, total_loss: 0.22544647753238678
training step: 55802, total_loss: 2.2203211784362793
training step: 55803, total_loss: 1.4785747528076172
training step: 55804, total_loss: 0.4906248450279236
training step: 55805, total_loss: 0.48186564445495605
training step: 55806, total_loss: 1.57035493850708
training step: 55807, total_loss: 4.191654205322266
training step: 55808, total_loss: 1.6874698400497437
training step: 55809, total_loss: 0.16639775037765503
training step: 55810, total_loss: 1.8345434665679932
training step: 55811, total_loss: 1.2409210205078125
training step: 55812, total_loss: 0.13256695866584778
training step: 55813, total_loss: 0.2672941982746124
training step: 55814, total_loss: 0.8791390061378479
training step: 55815, total_loss: 0.7227648496627808
training step: 55816, total_loss: 1.9646579027175903
training step: 55817, total_loss: 0.1209113597869873
training step: 55818, total_loss: 0.6635671854019165
training step: 55819, total_loss: 2.2349777221679688
training step: 55820, total_loss: 2.0200247764587402
training step: 55821, total_loss: 0.13874661922454834
training step: 55822, total_loss: 1.875929355621338
training step: 55823, total_loss: 0.12478914111852646
training step: 55824, total_loss: 2.3663225173950195
training step: 55825, total_loss: 1.3755840063095093
training step: 55826, total_loss: 2.0569348335266113
training step: 55827, total_loss: 2.570841073989868
training step: 55828, total_loss: 3.215452194213867
training step: 55829, total_loss: 0.20074044167995453
training step: 55830, total_loss: 0.9684561491012573
training step: 55831, total_loss: 3.0405449867248535
training step: 55832, total_loss: 1.7325084209442139
training step: 55833, total_loss: 2.3426144123077393
training step: 55834, total_loss: 2.4784693717956543
training step: 55835, total_loss: 0.30195915699005127
training step: 55836, total_loss: 1.8574490547180176
training step: 55837, total_loss: 1.6654289960861206
training step: 55838, total_loss: 0.8603681325912476
training step: 55839, total_loss: 2.054046154022217
training step: 55840, total_loss: 1.6213152408599854
training step: 55841, total_loss: 2.079596519470215
training step: 55842, total_loss: 2.1706507205963135
training step: 55843, total_loss: 1.6433979272842407
training step: 55844, total_loss: 1.6206207275390625
training step: 55845, total_loss: 1.1122052669525146
training step: 55846, total_loss: 0.07104161381721497
training step: 55847, total_loss: 1.9504375457763672
training step: 55848, total_loss: 0.6770207285881042
training step: 55849, total_loss: 2.322112560272217
training step: 55850, total_loss: 1.9451754093170166
training step: 55851, total_loss: 0.5911741256713867
training step: 55852, total_loss: 0.541663408279419
training step: 55853, total_loss: 1.816489338874817
training step: 55854, total_loss: 0.8048572540283203
training step: 55855, total_loss: 0.8746521472930908
training step: 55856, total_loss: 0.6122338175773621
training step: 55857, total_loss: 2.97221040725708
training step: 55858, total_loss: 1.4117456674575806
training step: 55859, total_loss: 3.1769890785217285
training step: 55860, total_loss: 1.9483835697174072
training step: 55861, total_loss: 0.7577818632125854
training step: 55862, total_loss: 2.3037397861480713
training step: 55863, total_loss: 1.290087342262268
training step: 55864, total_loss: 0.604478120803833
training step: 55865, total_loss: 2.6029281616210938
training step: 55866, total_loss: 4.460393905639648
training step: 55867, total_loss: 2.1728739738464355
training step: 55868, total_loss: 2.3920278549194336
training step: 55869, total_loss: 2.61413311958313
training step: 55870, total_loss: 2.1105709075927734
training step: 55871, total_loss: 2.70896577835083
training step: 55872, total_loss: 0.5321311950683594
training step: 55873, total_loss: 0.7060097455978394
training step: 55874, total_loss: 0.020445389673113823
training step: 55875, total_loss: 2.2116811275482178
training step: 55876, total_loss: 2.1763038635253906
training step: 55877, total_loss: 1.5193148851394653
training step: 55878, total_loss: 0.007432051468640566
training step: 55879, total_loss: 1.4457194805145264
training step: 55880, total_loss: 1.8372538089752197
training step: 55881, total_loss: 1.329026699066162
training step: 55882, total_loss: 1.1709526777267456
training step: 55883, total_loss: 2.4925713539123535
training step: 55884, total_loss: 1.6385849714279175
training step: 55885, total_loss: 0.9099941253662109
training step: 55886, total_loss: 2.677957057952881
training step: 55887, total_loss: 1.3081779479980469
training step: 55888, total_loss: 1.6299188137054443
training step: 55889, total_loss: 0.9213358163833618
training step: 55890, total_loss: 0.3367450535297394
training step: 55891, total_loss: 2.8974053859710693
training step: 55892, total_loss: 2.2629003524780273
training step: 55893, total_loss: 2.627819061279297
training step: 55894, total_loss: 6.271193504333496
training step: 55895, total_loss: 0.525353729724884
training step: 55896, total_loss: 1.225447654724121
training step: 55897, total_loss: 0.4999179542064667
training step: 55898, total_loss: 2.014770269393921
training step: 55899, total_loss: 0.46127164363861084
training step: 55900, total_loss: 2.789039134979248
training step: 55901, total_loss: 0.5044360160827637
training step: 55902, total_loss: 0.21670860052108765
training step: 55903, total_loss: 0.03367466479539871
training step: 55904, total_loss: 0.8512777090072632
training step: 55905, total_loss: 1.299080491065979
training step: 55906, total_loss: 0.10022284835577011
training step: 55907, total_loss: 3.0382492542266846
training step: 55908, total_loss: 0.2822610139846802
training step: 55909, total_loss: 2.846137762069702
training step: 55910, total_loss: 4.4709248542785645
training step: 55911, total_loss: 1.973724365234375
training step: 55912, total_loss: 2.2415781021118164
training step: 55913, total_loss: 3.5168426036834717
training step: 55914, total_loss: 1.0670905113220215
training step: 55915, total_loss: 1.071370005607605
training step: 55916, total_loss: 0.8416584730148315
training step: 55917, total_loss: 4.544459342956543
training step: 55918, total_loss: 2.324517011642456
training step: 55919, total_loss: 1.6072664260864258
training step: 55920, total_loss: 2.563291072845459
training step: 55921, total_loss: 0.00773263443261385
training step: 55922, total_loss: 0.09754367172718048
training step: 55923, total_loss: 3.2283575534820557
training step: 55924, total_loss: 2.7423582077026367
training step: 55925, total_loss: 2.3609540462493896
training step: 55926, total_loss: 1.0988309383392334
training step: 55927, total_loss: 0.39394909143447876
training step: 55928, total_loss: 1.6598836183547974
training step: 55929, total_loss: 1.6003711223602295
training step: 55930, total_loss: 1.802771806716919
training step: 55931, total_loss: 0.6563428640365601
training step: 55932, total_loss: 0.0003358143148943782
training step: 55933, total_loss: 2.3176629543304443
training step: 55934, total_loss: 2.0243585109710693
training step: 55935, total_loss: 1.926384687423706
training step: 55936, total_loss: 0.29856210947036743
training step: 55937, total_loss: 1.4803005456924438
training step: 55938, total_loss: 0.4881938099861145
training step: 55939, total_loss: 2.427335500717163
training step: 55940, total_loss: 0.007264604326337576INFO:tensorflow:Writing predictions to: test_output/predictions_56000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_56000.json

training step: 55941, total_loss: 4.655965805053711
training step: 55942, total_loss: 0.38132166862487793
training step: 55943, total_loss: 3.2522544860839844
training step: 55944, total_loss: 1.320215106010437
training step: 55945, total_loss: 0.8730559349060059
training step: 55946, total_loss: 2.0273337364196777
training step: 55947, total_loss: 2.129887342453003
training step: 55948, total_loss: 2.0616326332092285
training step: 55949, total_loss: 1.2213456630706787
training step: 55950, total_loss: 3.165635585784912
training step: 55951, total_loss: 2.4458961486816406
training step: 55952, total_loss: 0.030927423387765884
training step: 55953, total_loss: 2.8799855709075928
training step: 55954, total_loss: 1.0661635398864746
training step: 55955, total_loss: 0.2918453514575958
training step: 55956, total_loss: 0.4399898648262024
training step: 55957, total_loss: 1.4172617197036743
training step: 55958, total_loss: 1.3422743082046509
training step: 55959, total_loss: 1.9056413173675537
training step: 55960, total_loss: 1.1876335144042969
training step: 55961, total_loss: 0.6850805282592773
training step: 55962, total_loss: 2.8855581283569336
training step: 55963, total_loss: 2.6182126998901367
training step: 55964, total_loss: 1.0066052675247192
training step: 55965, total_loss: 0.005377363879233599
training step: 55966, total_loss: 3.435485363006592
training step: 55967, total_loss: 7.347601890563965
training step: 55968, total_loss: 0.8950838446617126
training step: 55969, total_loss: 1.8356845378875732
training step: 55970, total_loss: 2.1326656341552734
training step: 55971, total_loss: 1.8008390665054321
training step: 55972, total_loss: 3.2774999141693115
training step: 55973, total_loss: 1.1632575988769531
training step: 55974, total_loss: 1.7792401313781738
training step: 55975, total_loss: 2.7411766052246094
training step: 55976, total_loss: 3.254730224609375
training step: 55977, total_loss: 3.116748094558716
training step: 55978, total_loss: 2.040942668914795
training step: 55979, total_loss: 1.0924803018569946
training step: 55980, total_loss: 0.5884621739387512
training step: 55981, total_loss: 1.7387235164642334
training step: 55982, total_loss: 2.653172016143799
training step: 55983, total_loss: 0.047500915825366974
training step: 55984, total_loss: 0.2936454713344574
training step: 55985, total_loss: 1.6216543912887573
training step: 55986, total_loss: 0.7481410503387451
training step: 55987, total_loss: 4.196765899658203
training step: 55988, total_loss: 0.0010270702186971903
training step: 55989, total_loss: 2.6384103298187256
training step: 55990, total_loss: 0.18054482340812683
training step: 55991, total_loss: 2.645082473754883
training step: 55992, total_loss: 2.67753267288208
training step: 55993, total_loss: 1.1022989749908447
training step: 55994, total_loss: 1.8198716640472412
training step: 55995, total_loss: 0.9579504728317261
training step: 55996, total_loss: 3.3768205642700195
training step: 55997, total_loss: 3.062103033065796
training step: 55998, total_loss: 1.9421319961547852
training step: 55999, total_loss: 0.8836071491241455
training step: 56000, total_loss: 1.679262399673462
epoch finished! shuffle=False
evaluation: 20000, total_loss: 1.8292869329452515, f1: 57.89654478535877, followup: 36.08702266849232, yesno: 73.46721436178305, heq: 52.95907500380344, dheq: 3.7

Model saved in path test_output//model_56000.ckpt
training step: 56001, total_loss: 1.3964022397994995
training step: 56002, total_loss: 1.7118077278137207
training step: 56003, total_loss: 1.971245288848877
training step: 56004, total_loss: 1.4136037826538086
training step: 56005, total_loss: 1.287091851234436
training step: 56006, total_loss: 0.0727262943983078
training step: 56007, total_loss: 0.40390944480895996
training step: 56008, total_loss: 1.3000071048736572
training step: 56009, total_loss: 0.3450177311897278
training step: 56010, total_loss: 1.2595429420471191
training step: 56011, total_loss: 1.312188744544983
training step: 56012, total_loss: 2.017282485961914
training step: 56013, total_loss: 0.08769672363996506
training step: 56014, total_loss: 1.1193040609359741
training step: 56015, total_loss: 1.0036561489105225
training step: 56016, total_loss: 0.6197788715362549
training step: 56017, total_loss: 2.2317886352539062
training step: 56018, total_loss: 0.46280401945114136
training step: 56019, total_loss: 2.3047780990600586
training step: 56020, total_loss: 0.13129988312721252
training step: 56021, total_loss: 0.24033626914024353
training step: 56022, total_loss: 1.3879672288894653
training step: 56023, total_loss: 0.936390221118927
training step: 56024, total_loss: 0.17920330166816711
training step: 56025, total_loss: 1.1852604150772095
training step: 56026, total_loss: 0.18711522221565247
training step: 56027, total_loss: 0.4611649811267853
training step: 56028, total_loss: 1.733906865119934
training step: 56029, total_loss: 1.5741488933563232
training step: 56030, total_loss: 0.27987009286880493
training step: 56031, total_loss: 0.3109297752380371
training step: 56032, total_loss: 0.7444289326667786
training step: 56033, total_loss: 3.5146713256835938
training step: 56034, total_loss: 1.6873016357421875
training step: 56035, total_loss: 1.8864037990570068
training step: 56036, total_loss: 2.044926643371582
training step: 56037, total_loss: 2.5131568908691406
training step: 56038, total_loss: 4.084833145141602
training step: 56039, total_loss: 1.047874927520752
training step: 56040, total_loss: 1.07704758644104
training step: 56041, total_loss: 1.8985775709152222
training step: 56042, total_loss: 1.5722386837005615
training step: 56043, total_loss: 0.05154028534889221
training step: 56044, total_loss: 0.056044936180114746
training step: 56045, total_loss: 0.02030598372220993
training step: 56046, total_loss: 2.325831413269043
training step: 56047, total_loss: 0.5292198657989502
training step: 56048, total_loss: 1.3626360893249512
training step: 56049, total_loss: 2.0089964866638184
training step: 56050, total_loss: 0.43484818935394287
training step: 56051, total_loss: 1.5411863327026367
training step: 56052, total_loss: 3.171600341796875
training step: 56053, total_loss: 2.320014715194702
training step: 56054, total_loss: 0.6064172983169556
training step: 56055, total_loss: 0.20698659121990204
training step: 56056, total_loss: 1.0491920709609985
training step: 56057, total_loss: 1.7501537799835205
training step: 56058, total_loss: 1.412862777709961
training step: 56059, total_loss: 1.0160572528839111
training step: 56060, total_loss: 3.234745502471924
training step: 56061, total_loss: 2.1044082641601562
training step: 56062, total_loss: 2.6585025787353516
training step: 56063, total_loss: 0.8974045515060425
training step: 56064, total_loss: 0.7746292352676392
training step: 56065, total_loss: 4.351190567016602
training step: 56066, total_loss: 0.7175120115280151
training step: 56067, total_loss: 2.7989649772644043
training step: 56068, total_loss: 1.5440089702606201
training step: 56069, total_loss: 1.1462957859039307
training step: 56070, total_loss: 1.7792651653289795
training step: 56071, total_loss: 0.29679086804389954
training step: 56072, total_loss: 1.0075193643569946
training step: 56073, total_loss: 2.606058120727539
training step: 56074, total_loss: 3.494291305541992
training step: 56075, total_loss: 0.7716066837310791
training step: 56076, total_loss: 1.1283257007598877
training step: 56077, total_loss: 1.103867530822754
training step: 56078, total_loss: 1.3309950828552246
training step: 56079, total_loss: 0.5356033444404602
training step: 56080, total_loss: 1.8964903354644775
training step: 56081, total_loss: 1.472570538520813
training step: 56082, total_loss: 0.04616638645529747
training step: 56083, total_loss: 0.029572710394859314
training step: 56084, total_loss: 0.8600791692733765
training step: 56085, total_loss: 1.760793924331665
training step: 56086, total_loss: 3.1108040809631348
training step: 56087, total_loss: 0.6134803891181946
training step: 56088, total_loss: 1.3795619010925293
training step: 56089, total_loss: 1.6945158243179321
training step: 56090, total_loss: 0.9021505117416382
training step: 56091, total_loss: 0.13955092430114746
training step: 56092, total_loss: 1.8947681188583374
training step: 56093, total_loss: 2.276624917984009
training step: 56094, total_loss: 1.0909652709960938
training step: 56095, total_loss: 1.1416441202163696
training step: 56096, total_loss: 1.3791639804840088
training step: 56097, total_loss: 1.6341781616210938
training step: 56098, total_loss: 0.9070982336997986
training step: 56099, total_loss: 0.010547510348260403
training step: 56100, total_loss: 0.2917141914367676
training step: 56101, total_loss: 2.077685832977295
training step: 56102, total_loss: 2.0612270832061768
training step: 56103, total_loss: 0.7780925631523132
training step: 56104, total_loss: 2.3491597175598145
training step: 56105, total_loss: 3.272836446762085
training step: 56106, total_loss: 4.032862186431885
training step: 56107, total_loss: 1.694305181503296
training step: 56108, total_loss: 0.096312016248703
training step: 56109, total_loss: 1.0268418788909912
training step: 56110, total_loss: 3.5385775566101074
training step: 56111, total_loss: 3.2159976959228516
training step: 56112, total_loss: 0.4059201180934906
training step: 56113, total_loss: 1.9225070476531982
training step: 56114, total_loss: 1.763411521911621
training step: 56115, total_loss: 1.0090302228927612
training step: 56116, total_loss: 2.4744229316711426
training step: 56117, total_loss: 1.888338565826416
training step: 56118, total_loss: 0.5686111450195312
training step: 56119, total_loss: 2.417215347290039
training step: 56120, total_loss: 1.0867522954940796
training step: 56121, total_loss: 2.3322787284851074
training step: 56122, total_loss: 3.7154154777526855
training step: 56123, total_loss: 2.1025073528289795
training step: 56124, total_loss: 3.201923131942749
training step: 56125, total_loss: 0.8406800031661987
training step: 56126, total_loss: 1.4733184576034546
training step: 56127, total_loss: 1.7701572179794312
training step: 56128, total_loss: 1.4850478172302246
training step: 56129, total_loss: 1.6498796939849854
training step: 56130, total_loss: 3.276829242706299
training step: 56131, total_loss: 1.2095415592193604
training step: 56132, total_loss: 1.6576303243637085
training step: 56133, total_loss: 2.753143787384033
training step: 56134, total_loss: 4.4563164710998535
training step: 56135, total_loss: 0.3149045705795288
training step: 56136, total_loss: 2.6949024200439453
training step: 56137, total_loss: 0.5502791404724121
training step: 56138, total_loss: 3.539968490600586
training step: 56139, total_loss: 2.781374216079712
training step: 56140, total_loss: 2.2795019149780273
training step: 56141, total_loss: 0.2156280130147934
training step: 56142, total_loss: 1.9300861358642578
training step: 56143, total_loss: 3.5189990997314453
training step: 56144, total_loss: 2.3710825443267822
training step: 56145, total_loss: 2.0806446075439453
training step: 56146, total_loss: 1.8033417463302612
training step: 56147, total_loss: 0.6242371797561646
training step: 56148, total_loss: 1.7208181619644165
training step: 56149, total_loss: 1.435577392578125
training step: 56150, total_loss: 1.355284571647644
training step: 56151, total_loss: 1.167070984840393
training step: 56152, total_loss: 1.0523008108139038
training step: 56153, total_loss: 0.7608035802841187
training step: 56154, total_loss: 1.4277284145355225
training step: 56155, total_loss: 0.5538733005523682
training step: 56156, total_loss: 0.24715642631053925
training step: 56157, total_loss: 1.011979103088379
training step: 56158, total_loss: 1.4749993085861206
training step: 56159, total_loss: 0.3903191387653351
training step: 56160, total_loss: 1.7352960109710693
training step: 56161, total_loss: 1.3499895334243774
training step: 56162, total_loss: 4.654397964477539
training step: 56163, total_loss: 1.5597368478775024
training step: 56164, total_loss: 2.4424643516540527
training step: 56165, total_loss: 0.019080262631177902
training step: 56166, total_loss: 0.04559651389718056
training step: 56167, total_loss: 2.5101749897003174
training step: 56168, total_loss: 1.6619071960449219
training step: 56169, total_loss: 3.169689655303955
training step: 56170, total_loss: 2.0919342041015625
training step: 56171, total_loss: 1.6807167530059814
training step: 56172, total_loss: 2.307447910308838
training step: 56173, total_loss: 0.6794537305831909
training step: 56174, total_loss: 0.0354791134595871
training step: 56175, total_loss: 2.8656747341156006
training step: 56176, total_loss: 5.947962760925293
training step: 56177, total_loss: 1.7671926021575928
training step: 56178, total_loss: 1.7574670314788818
training step: 56179, total_loss: 0.4616745710372925
training step: 56180, total_loss: 1.1009615659713745
training step: 56181, total_loss: 1.7793235778808594
training step: 56182, total_loss: 0.11113917827606201
training step: 56183, total_loss: 0.08850491791963577
training step: 56184, total_loss: 1.2853283882141113
training step: 56185, total_loss: 1.7239701747894287
training step: 56186, total_loss: 2.042510986328125
training step: 56187, total_loss: 0.0008091404451988637
training step: 56188, total_loss: 0.6499409079551697
training step: 56189, total_loss: 0.00010376572754466906
training step: 56190, total_loss: 0.9576401710510254
training step: 56191, total_loss: 2.294736862182617
training step: 56192, total_loss: 3.4149365425109863
training step: 56193, total_loss: 1.9891533851623535
training step: 56194, total_loss: 1.8904799222946167
training step: 56195, total_loss: 1.586012840270996
training step: 56196, total_loss: 0.4798378348350525
training step: 56197, total_loss: 0.05483759194612503
training step: 56198, total_loss: 0.7945406436920166
training step: 56199, total_loss: 2.5195975303649902
training step: 56200, total_loss: 0.8399124145507812
training step: 56201, total_loss: 1.0440436601638794
training step: 56202, total_loss: 2.318939208984375
training step: 56203, total_loss: 0.1653270721435547
training step: 56204, total_loss: 2.2859110832214355
training step: 56205, total_loss: 3.0273399353027344
training step: 56206, total_loss: 4.417125701904297
training step: 56207, total_loss: 0.46057629585266113
training step: 56208, total_loss: 2.0249156951904297
training step: 56209, total_loss: 3.4673871994018555
training step: 56210, total_loss: 0.1675620973110199
training step: 56211, total_loss: 4.090562343597412
training step: 56212, total_loss: 0.791253387928009
training step: 56213, total_loss: 2.6384835243225098
training step: 56214, total_loss: 1.370485782623291
training step: 56215, total_loss: 1.3173905611038208
training step: 56216, total_loss: 0.9323586821556091
training step: 56217, total_loss: 0.05069196969270706
training step: 56218, total_loss: 3.282100200653076
training step: 56219, total_loss: 1.621853232383728
training step: 56220, total_loss: 3.499838352203369
training step: 56221, total_loss: 1.7830533981323242
training step: 56222, total_loss: 2.209660291671753
training step: 56223, total_loss: 0.0897904634475708
training step: 56224, total_loss: 0.4331034719944
training step: 56225, total_loss: 0.45283058285713196
training step: 56226, total_loss: 1.262850046157837
training step: 56227, total_loss: 2.0453295707702637
training step: 56228, total_loss: 1.846022129058838
training step: 56229, total_loss: 0.7987092733383179
training step: 56230, total_loss: 2.555603504180908
training step: 56231, total_loss: 0.1795048713684082
training step: 56232, total_loss: 1.9132599830627441
training step: 56233, total_loss: 0.8515552282333374
training step: 56234, total_loss: 0.9382394552230835
training step: 56235, total_loss: 0.05215853080153465
training step: 56236, total_loss: 2.1132595539093018
training step: 56237, total_loss: 1.8023293018341064
training step: 56238, total_loss: 1.197336196899414
training step: 56239, total_loss: 1.5932836532592773
training step: 56240, total_loss: 1.852508783340454
training step: 56241, total_loss: 4.068204879760742
training step: 56242, total_loss: 1.1236975193023682
training step: 56243, total_loss: 9.28901481628418
training step: 56244, total_loss: 2.160723924636841
training step: 56245, total_loss: 0.9573366641998291
training step: 56246, total_loss: 0.8252331018447876
training step: 56247, total_loss: 1.6632747650146484
training step: 56248, total_loss: 3.03332781791687
training step: 56249, total_loss: 3.615694999694824
training step: 56250, total_loss: 2.0527710914611816
training step: 56251, total_loss: 0.9491598010063171
training step: 56252, total_loss: 1.2563459873199463
training step: 56253, total_loss: 0.5437253713607788
training step: 56254, total_loss: 2.8348913192749023
training step: 56255, total_loss: 0.24261781573295593
training step: 56256, total_loss: 3.982968807220459
training step: 56257, total_loss: 2.413069725036621
training step: 56258, total_loss: 0.7797998785972595
training step: 56259, total_loss: 1.4907381534576416
training step: 56260, total_loss: 2.8373489379882812
training step: 56261, total_loss: 2.5115513801574707
training step: 56262, total_loss: 0.0032087289728224277
training step: 56263, total_loss: 1.2475444078445435
training step: 56264, total_loss: 1.0929778814315796
training step: 56265, total_loss: 1.4523627758026123
training step: 56266, total_loss: 2.2544403076171875
training step: 56267, total_loss: 3.0024573802948
training step: 56268, total_loss: 2.47597599029541
training step: 56269, total_loss: 0.41912519931793213
training step: 56270, total_loss: 1.015750527381897
training step: 56271, total_loss: 2.409095287322998
training step: 56272, total_loss: 3.4268007278442383
training step: 56273, total_loss: 0.7149044275283813
training step: 56274, total_loss: 1.0859999656677246
training step: 56275, total_loss: 0.7745211124420166
training step: 56276, total_loss: 1.1752785444259644
training step: 56277, total_loss: 3.013432741165161
training step: 56278, total_loss: 3.7504615783691406
training step: 56279, total_loss: 3.1575541496276855
training step: 56280, total_loss: 1.2647311687469482
training step: 56281, total_loss: 0.855032205581665
training step: 56282, total_loss: 3.3749570846557617
training step: 56283, total_loss: 1.9309656620025635
training step: 56284, total_loss: 2.0038278102874756
training step: 56285, total_loss: 0.5307851433753967
training step: 56286, total_loss: 0.2156563699245453
training step: 56287, total_loss: 1.1387152671813965
training step: 56288, total_loss: 0.1800251603126526
training step: 56289, total_loss: 2.2992045879364014
training step: 56290, total_loss: 1.0550786256790161
training step: 56291, total_loss: 1.7042059898376465
training step: 56292, total_loss: 0.3200308084487915
training step: 56293, total_loss: 1.362729787826538
training step: 56294, total_loss: 2.7522552013397217
training step: 56295, total_loss: 1.3018261194229126
training step: 56296, total_loss: 0.4586264193058014
training step: 56297, total_loss: 0.3081437051296234
training step: 56298, total_loss: 3.2520697116851807
training step: 56299, total_loss: 0.18172964453697205
training step: 56300, total_loss: 0.2865332067012787
training step: 56301, total_loss: 0.8950448632240295
training step: 56302, total_loss: 2.3436226844787598
training step: 56303, total_loss: 2.0134847164154053
training step: 56304, total_loss: 0.6857379674911499
training step: 56305, total_loss: 3.9051759243011475
training step: 56306, total_loss: 0.8099595308303833
training step: 56307, total_loss: 1.356170892715454
training step: 56308, total_loss: 0.9722959995269775
training step: 56309, total_loss: 2.1581692695617676
training step: 56310, total_loss: 2.2323925495147705
training step: 56311, total_loss: 4.282893180847168
training step: 56312, total_loss: 1.4069112539291382
training step: 56313, total_loss: 0.4314282238483429
training step: 56314, total_loss: 2.207601547241211
training step: 56315, total_loss: 2.128606081008911
training step: 56316, total_loss: 0.020779017359018326
training step: 56317, total_loss: 2.027966022491455
training step: 56318, total_loss: 1.2321771383285522
training step: 56319, total_loss: 0.5952197313308716
training step: 56320, total_loss: 0.6637505292892456
training step: 56321, total_loss: 1.5622514486312866
training step: 56322, total_loss: 1.555735468864441
training step: 56323, total_loss: 0.07897614687681198
training step: 56324, total_loss: 1.5245521068572998
training step: 56325, total_loss: 0.99991375207901
training step: 56326, total_loss: 0.39948153495788574
training step: 56327, total_loss: 0.19608241319656372
training step: 56328, total_loss: 0.3543839454650879
training step: 56329, total_loss: 2.7802677154541016
training step: 56330, total_loss: 3.3527939319610596
training step: 56331, total_loss: 3.351562023162842
training step: 56332, total_loss: 4.197265625
training step: 56333, total_loss: 0.4111276865005493
training step: 56334, total_loss: 0.7265173196792603
training step: 56335, total_loss: 1.1369059085845947
training step: 56336, total_loss: 0.38286077976226807
training step: 56337, total_loss: 0.36766746640205383
training step: 56338, total_loss: 2.6471171379089355
training step: 56339, total_loss: 0.4340262711048126
training step: 56340, total_loss: 0.4952737092971802
training step: 56341, total_loss: 3.1111717224121094
training step: 56342, total_loss: 4.205883979797363
training step: 56343, total_loss: 3.431232452392578
training step: 56344, total_loss: 0.7295137643814087
training step: 56345, total_loss: 0.8985421061515808
training step: 56346, total_loss: 1.5021032094955444
training step: 56347, total_loss: 4.398164749145508
training step: 56348, total_loss: 4.773760795593262
training step: 56349, total_loss: 1.1267709732055664
training step: 56350, total_loss: 1.5673048496246338
training step: 56351, total_loss: 1.9071658849716187
training step: 56352, total_loss: 1.9677488803863525
training step: 56353, total_loss: 0.41353023052215576
training step: 56354, total_loss: 0.6226823329925537
training step: 56355, total_loss: 0.3582204580307007
training step: 56356, total_loss: 2.162670135498047
training step: 56357, total_loss: 0.7331209182739258
training step: 56358, total_loss: 1.5351262092590332
training step: 56359, total_loss: 0.4239533245563507
training step: 56360, total_loss: 1.2200520038604736
training step: 56361, total_loss: 2.2160027027130127
training step: 56362, total_loss: 0.2437417060136795
training step: 56363, total_loss: 2.2401511669158936
training step: 56364, total_loss: 1.7896615266799927
training step: 56365, total_loss: 4.123736381530762
training step: 56366, total_loss: 0.9438307285308838
training step: 56367, total_loss: 2.5388174057006836
training step: 56368, total_loss: 2.6609110832214355
training step: 56369, total_loss: 1.5014442205429077
training step: 56370, total_loss: 0.7805097699165344
training step: 56371, total_loss: 0.3463364541530609
training step: 56372, total_loss: 2.5609536170959473
training step: 56373, total_loss: 3.0315072536468506
training step: 56374, total_loss: 0.4849691092967987
training step: 56375, total_loss: 0.9204260110855103
training step: 56376, total_loss: 2.523449420928955
training step: 56377, total_loss: 3.7453646659851074
training step: 56378, total_loss: 2.2977294921875
training step: 56379, total_loss: 1.4349514245986938
training step: 56380, total_loss: 0.24324554204940796
training step: 56381, total_loss: 2.293137550354004
training step: 56382, total_loss: 1.0245850086212158
training step: 56383, total_loss: 3.5943920612335205
training step: 56384, total_loss: 1.3731446266174316
training step: 56385, total_loss: 1.3888885974884033
training step: 56386, total_loss: 2.3477680683135986
training step: 56387, total_loss: 1.6247057914733887
training step: 56388, total_loss: 3.399941921234131
training step: 56389, total_loss: 1.477118730545044
training step: 56390, total_loss: 1.4515750408172607
training step: 56391, total_loss: 1.2409236431121826
training step: 56392, total_loss: 0.33410871028900146
training step: 56393, total_loss: 2.935262680053711
training step: 56394, total_loss: 3.5866196155548096
training step: 56395, total_loss: 0.07899413257837296
training step: 56396, total_loss: 1.0806599855422974
training step: 56397, total_loss: 2.3247509002685547
training step: 56398, total_loss: 1.7248070240020752
training step: 56399, total_loss: 3.25343656539917
training step: 56400, total_loss: 1.0024008750915527
training step: 56401, total_loss: 5.889089584350586
training step: 56402, total_loss: 0.8494596481323242
training step: 56403, total_loss: 2.879528522491455
training step: 56404, total_loss: 0.6373469829559326
training step: 56405, total_loss: 1.47084379196167
training step: 56406, total_loss: 3.4490818977355957
training step: 56407, total_loss: 0.34756410121917725
training step: 56408, total_loss: 1.2097264528274536
training step: 56409, total_loss: 2.817887306213379
training step: 56410, total_loss: 0.4244137406349182
training step: 56411, total_loss: 0.5793572664260864
training step: 56412, total_loss: 0.4431857168674469
training step: 56413, total_loss: 0.4878436326980591
training step: 56414, total_loss: 1.767785668373108
training step: 56415, total_loss: 1.812408447265625
training step: 56416, total_loss: 1.679296612739563
training step: 56417, total_loss: 0.028894122689962387
training step: 56418, total_loss: 1.1111263036727905
training step: 56419, total_loss: 1.2651647329330444
training step: 56420, total_loss: 0.935892641544342
training step: 56421, total_loss: 1.3272013664245605
training step: 56422, total_loss: 0.5983855128288269
training step: 56423, total_loss: 0.0004992277827113867
training step: 56424, total_loss: 0.0051690866239368916
training step: 56425, total_loss: 3.0632364749908447
training step: 56426, total_loss: 0.6424058675765991
training step: 56427, total_loss: 0.0571831613779068
training step: 56428, total_loss: 0.6515251398086548
training step: 56429, total_loss: 0.6978672742843628
training step: 56430, total_loss: 2.6982228755950928
training step: 56431, total_loss: 1.3116172552108765
training step: 56432, total_loss: 1.1343963146209717
training step: 56433, total_loss: 0.8239797353744507
training step: 56434, total_loss: 1.3030762672424316
training step: 56435, total_loss: 1.7179512977600098
training step: 56436, total_loss: 0.15926684439182281
training step: 56437, total_loss: 1.3734327554702759
training step: 56438, total_loss: 1.9774391651153564
training step: 56439, total_loss: 0.10949227213859558
training step: 56440, total_loss: 0.40267908573150635
training step: 56441, total_loss: 2.4963302612304688
training step: 56442, total_loss: 6.817839622497559
training step: 56443, total_loss: 1.3959391117095947
training step: 56444, total_loss: 1.236147165298462
training step: 56445, total_loss: 1.4259343147277832
training step: 56446, total_loss: 2.7121572494506836
training step: 56447, total_loss: 0.24102568626403809
training step: 56448, total_loss: 2.72493577003479
training step: 56449, total_loss: 4.4633893966674805
training step: 56450, total_loss: 0.511508047580719
training step: 56451, total_loss: 4.473991394042969
training step: 56452, total_loss: 1.3822652101516724
training step: 56453, total_loss: 1.3024319410324097
training step: 56454, total_loss: 3.192896842956543
training step: 56455, total_loss: 1.9882142543792725
training step: 56456, total_loss: 0.10446102917194366
training step: 56457, total_loss: 2.0873360633850098
training step: 56458, total_loss: 0.5800352692604065
training step: 56459, total_loss: 0.5229899883270264
training step: 56460, total_loss: 1.1754376888275146
training step: 56461, total_loss: 0.23698821663856506
training step: 56462, total_loss: 1.0002259016036987
training step: 56463, total_loss: 0.05718114972114563
training step: 56464, total_loss: 0.006570941768586636
training step: 56465, total_loss: 1.8475769758224487
training step: 56466, total_loss: 0.9355509281158447
training step: 56467, total_loss: 1.5649163722991943
training step: 56468, total_loss: 1.9257266521453857
training step: 56469, total_loss: 2.5031397342681885
training step: 56470, total_loss: 0.9218192100524902
training step: 56471, total_loss: 1.0436663627624512
training step: 56472, total_loss: 0.6153207421302795
training step: 56473, total_loss: 1.7563233375549316
training step: 56474, total_loss: 2.3247077465057373
training step: 56475, total_loss: 0.06903328001499176
training step: 56476, total_loss: 1.046697735786438
training step: 56477, total_loss: 0.20131619274616241
training step: 56478, total_loss: 0.9331930875778198
training step: 56479, total_loss: 2.013274669647217
training step: 56480, total_loss: 1.8866608142852783
training step: 56481, total_loss: 0.653323769569397
training step: 56482, total_loss: 1.7574193477630615
training step: 56483, total_loss: 0.6672307252883911
training step: 56484, total_loss: 1.4185818433761597
training step: 56485, total_loss: 0.9290328025817871
training step: 56486, total_loss: 0.29249078035354614
training step: 56487, total_loss: 2.364114761352539
training step: 56488, total_loss: 1.2677478790283203
training step: 56489, total_loss: 0.29570063948631287
training step: 56490, total_loss: 1.9571728706359863
training step: 56491, total_loss: 1.025005578994751
training step: 56492, total_loss: 2.4331464767456055
training step: 56493, total_loss: 3.983569622039795
training step: 56494, total_loss: 1.1934021711349487
training step: 56495, total_loss: 1.7771146297454834
training step: 56496, total_loss: 1.4651604890823364
training step: 56497, total_loss: 2.8106839656829834
training step: 56498, total_loss: 1.6466448307037354
training step: 56499, total_loss: 0.3793726861476898
training step: 56500, total_loss: 2.262589931488037
training step: 56501, total_loss: 2.1848411560058594
training step: 56502, total_loss: 0.3256676495075226
training step: 56503, total_loss: 1.2625540494918823
training step: 56504, total_loss: 3.1585278511047363
training step: 56505, total_loss: 1.024966835975647
training step: 56506, total_loss: 2.4069759845733643
training step: 56507, total_loss: 2.762068271636963
training step: 56508, total_loss: 0.6718684434890747
training step: 56509, total_loss: 0.23146545886993408
training step: 56510, total_loss: 0.1846308410167694
training step: 56511, total_loss: 0.6639174818992615
training step: 56512, total_loss: 2.272353172302246
training step: 56513, total_loss: 1.1031831502914429
training step: 56514, total_loss: 2.572964668273926
training step: 56515, total_loss: 1.130782127380371
training step: 56516, total_loss: 0.02139974944293499
training step: 56517, total_loss: 3.121537685394287
training step: 56518, total_loss: 2.835181713104248
training step: 56519, total_loss: 0.5716917514801025
training step: 56520, total_loss: 1.9500267505645752
training step: 56521, total_loss: 3.092951774597168
training step: 56522, total_loss: 0.728021502494812
training step: 56523, total_loss: 1.1507563591003418
training step: 56524, total_loss: 1.7275177240371704
training step: 56525, total_loss: 2.0041604042053223
training step: 56526, total_loss: 5.9348931312561035
training step: 56527, total_loss: 1.3320595026016235
training step: 56528, total_loss: 1.3064494132995605
training step: 56529, total_loss: 2.4735970497131348
training step: 56530, total_loss: 3.1427173614501953
training step: 56531, total_loss: 3.163180351257324
training step: 56532, total_loss: 1.121942162513733
training step: 56533, total_loss: 1.9477342367172241
training step: 56534, total_loss: 3.8669159412384033
training step: 56535, total_loss: 0.3535371422767639
training step: 56536, total_loss: 1.7811123132705688
training step: 56537, total_loss: 1.614170789718628
training step: 56538, total_loss: 2.082333564758301
training step: 56539, total_loss: 0.9488055109977722
training step: 56540, total_loss: 1.6190474033355713
training step: 56541, total_loss: 0.7872565984725952
training step: 56542, total_loss: 1.2394665479660034
training step: 56543, total_loss: 0.05306481570005417
training step: 56544, total_loss: 0.529003918170929
training step: 56545, total_loss: 0.3826312720775604
training step: 56546, total_loss: 0.7860920429229736
training step: 56547, total_loss: 2.991933822631836
training step: 56548, total_loss: 1.5561647415161133
training step: 56549, total_loss: 2.848604679107666
training step: 56550, total_loss: 1.7799835205078125
training step: 56551, total_loss: 2.295652151107788
training step: 56552, total_loss: 1.0253045558929443
training step: 56553, total_loss: 1.6968977451324463
training step: 56554, total_loss: 1.6361079216003418
training step: 56555, total_loss: 2.751922845840454
training step: 56556, total_loss: 0.7167165279388428
training step: 56557, total_loss: 1.2224220037460327
training step: 56558, total_loss: 2.3707516193389893
training step: 56559, total_loss: 5.546013355255127
training step: 56560, total_loss: 1.4843164682388306
training step: 56561, total_loss: 1.2092593908309937
training step: 56562, total_loss: 0.6159991025924683
training step: 56563, total_loss: 1.181141972541809
training step: 56564, total_loss: 1.7507460117340088
training step: 56565, total_loss: 2.164269208908081
training step: 56566, total_loss: 4.659344673156738
training step: 56567, total_loss: 0.5580377578735352
training step: 56568, total_loss: 0.13387323915958405
training step: 56569, total_loss: 0.28158488869667053
training step: 56570, total_loss: 0.3655414581298828
training step: 56571, total_loss: 1.2442938089370728
training step: 56572, total_loss: 3.1033575534820557
training step: 56573, total_loss: 2.524643898010254
training step: 56574, total_loss: 0.7164018750190735
training step: 56575, total_loss: 3.6569719314575195
training step: 56576, total_loss: 1.18474280834198
training step: 56577, total_loss: 0.4559974670410156
training step: 56578, total_loss: 1.504021406173706
training step: 56579, total_loss: 1.9837892055511475
training step: 56580, total_loss: 3.4823226928710938
training step: 56581, total_loss: 2.0389559268951416
training step: 56582, total_loss: 0.8424657583236694
training step: 56583, total_loss: 1.5315035581588745
training step: 56584, total_loss: 2.7417140007019043
training step: 56585, total_loss: 0.7113215327262878
training step: 56586, total_loss: 0.4388490319252014
training step: 56587, total_loss: 0.0035954818595200777
training step: 56588, total_loss: 0.2740126848220825
training step: 56589, total_loss: 0.6331556439399719
training step: 56590, total_loss: 1.5888688564300537
training step: 56591, total_loss: 2.163681983947754
training step: 56592, total_loss: 0.8701364994049072
training step: 56593, total_loss: 0.19861911237239838
training step: 56594, total_loss: 0.6836413741111755
training step: 56595, total_loss: 2.072695732116699
training step: 56596, total_loss: 2.1481986045837402
training step: 56597, total_loss: 2.6747517585754395
training step: 56598, total_loss: 5.028769016265869
training step: 56599, total_loss: 1.2631537914276123
training step: 56600, total_loss: 1.03446364402771
training step: 56601, total_loss: 1.31089448928833
training step: 56602, total_loss: 0.6693490743637085
training step: 56603, total_loss: 1.440201997756958
training step: 56604, total_loss: 0.010404804721474648
training step: 56605, total_loss: 1.3412208557128906
training step: 56606, total_loss: 2.315171480178833
training step: 56607, total_loss: 1.9616189002990723
training step: 56608, total_loss: 1.1358524560928345
training step: 56609, total_loss: 2.3657524585723877
training step: 56610, total_loss: 2.297980546951294
training step: 56611, total_loss: 2.650085926055908
training step: 56612, total_loss: 3.1632513999938965
training step: 56613, total_loss: 1.384910225868225
training step: 56614, total_loss: 0.32122015953063965
training step: 56615, total_loss: 2.011814832687378
training step: 56616, total_loss: 6.370807647705078
training step: 56617, total_loss: 1.708040714263916
training step: 56618, total_loss: 2.6224663257598877
training step: 56619, total_loss: 0.08259325474500656
training step: 56620, total_loss: 0.9692414999008179
training step: 56621, total_loss: 0.8775972127914429
training step: 56622, total_loss: 0.5010340213775635
training step: 56623, total_loss: 1.337812066078186
training step: 56624, total_loss: 1.5770260095596313
training step: 56625, total_loss: 0.6715816259384155
training step: 56626, total_loss: 2.465930938720703
training step: 56627, total_loss: 1.918442964553833
training step: 56628, total_loss: 1.960251808166504
training step: 56629, total_loss: 1.6110669374465942
training step: 56630, total_loss: 3.3747973442077637
training step: 56631, total_loss: 0.10745875537395477
training step: 56632, total_loss: 3.882495403289795
training step: 56633, total_loss: 3.6918349266052246
training step: 56634, total_loss: 3.3765151500701904
training step: 56635, total_loss: 2.012651205062866
training step: 56636, total_loss: 1.7353122234344482
training step: 56637, total_loss: 0.9220329523086548
training step: 56638, total_loss: 1.933794379234314
training step: 56639, total_loss: 0.02013169601559639
training step: 56640, total_loss: 0.0012622462818399072
training step: 56641, total_loss: 0.6950434446334839
training step: 56642, total_loss: 0.17420139908790588
training step: 56643, total_loss: 1.5927209854125977
training step: 56644, total_loss: 1.1699141263961792
training step: 56645, total_loss: 2.02805233001709
training step: 56646, total_loss: 0.20523706078529358
training step: 56647, total_loss: 0.12680712342262268
training step: 56648, total_loss: 2.596724510192871
training step: 56649, total_loss: 2.227627754211426
training step: 56650, total_loss: 2.317476749420166
training step: 56651, total_loss: 2.5849971771240234
training step: 56652, total_loss: 1.356508731842041
training step: 56653, total_loss: 2.2124569416046143
training step: 56654, total_loss: 0.8595018982887268
training step: 56655, total_loss: 2.7111153602600098
training step: 56656, total_loss: 2.8759336471557617
training step: 56657, total_loss: 0.01907624863088131
training step: 56658, total_loss: 0.9572089910507202
training step: 56659, total_loss: 1.5168049335479736
training step: 56660, total_loss: 1.0800769329071045
training step: 56661, total_loss: 3.8592681884765625
training step: 56662, total_loss: 0.7249338626861572
training step: 56663, total_loss: 2.960347890853882
training step: 56664, total_loss: 0.3768795132637024
training step: 56665, total_loss: 4.983214378356934
training step: 56666, total_loss: 4.561995029449463
training step: 56667, total_loss: 3.175747871398926
training step: 56668, total_loss: 1.2560198307037354
training step: 56669, total_loss: 2.8879637718200684
training step: 56670, total_loss: 1.8467895984649658
training step: 56671, total_loss: 1.2228894233703613
training step: 56672, total_loss: 0.7773466110229492
training step: 56673, total_loss: 1.80854070186615
training step: 56674, total_loss: 2.350419521331787
training step: 56675, total_loss: 0.1237596720457077
training step: 56676, total_loss: 1.33988356590271
training step: 56677, total_loss: 0.7517281770706177
training step: 56678, total_loss: 0.3842620253562927
training step: 56679, total_loss: 1.875921368598938
training step: 56680, total_loss: 0.8150625228881836
training step: 56681, total_loss: 0.2619868218898773
training step: 56682, total_loss: 0.6539928913116455
training step: 56683, total_loss: 0.628944993019104
training step: 56684, total_loss: 0.36994093656539917
training step: 56685, total_loss: 8.076102676568553e-05
training step: 56686, total_loss: 2.1235833168029785
training step: 56687, total_loss: 3.022143840789795
training step: 56688, total_loss: 1.8178701400756836
training step: 56689, total_loss: 1.384840488433838
training step: 56690, total_loss: 1.7458252906799316
training step: 56691, total_loss: 3.2945165634155273
training step: 56692, total_loss: 2.1833231449127197
training step: 56693, total_loss: 1.661165714263916
training step: 56694, total_loss: 5.038636207580566
training step: 56695, total_loss: 4.075499057769775
training step: 56696, total_loss: 0.9931156635284424
training step: 56697, total_loss: 2.5801515579223633
training step: 56698, total_loss: 1.2316969633102417
training step: 56699, total_loss: 1.4100229740142822
training step: 56700, total_loss: 0.0002072805946227163
training step: 56701, total_loss: 3.9723644256591797
training step: 56702, total_loss: 4.586432456970215
training step: 56703, total_loss: 3.7452940940856934
training step: 56704, total_loss: 0.9507861137390137
training step: 56705, total_loss: 0.42078572511672974
training step: 56706, total_loss: 2.3538098335266113
training step: 56707, total_loss: 1.264418601989746
training step: 56708, total_loss: 0.37536656856536865
training step: 56709, total_loss: 3.7092857360839844
training step: 56710, total_loss: 0.09805749356746674
training step: 56711, total_loss: 1.944779872894287
training step: 56712, total_loss: 1.873164415359497
training step: 56713, total_loss: 2.8180980682373047
training step: 56714, total_loss: 1.2161164283752441
training step: 56715, total_loss: 2.7936348915100098
training step: 56716, total_loss: 2.4695825576782227
training step: 56717, total_loss: 0.8337936401367188
training step: 56718, total_loss: 1.6984686851501465
training step: 56719, total_loss: 2.0245249271392822
training step: 56720, total_loss: 5.317134857177734
training step: 56721, total_loss: 1.9660444259643555
training step: 56722, total_loss: 0.34782689809799194
training step: 56723, total_loss: 1.4622125625610352
training step: 56724, total_loss: 4.252418518066406
training step: 56725, total_loss: 1.5556838512420654
training step: 56726, total_loss: 0.4955213665962219
training step: 56727, total_loss: 3.077951192855835
training step: 56728, total_loss: 4.597258567810059
training step: 56729, total_loss: 1.0938231945037842
training step: 56730, total_loss: 0.77260822057724
training step: 56731, total_loss: 4.488028526306152
training step: 56732, total_loss: 0.6563141345977783
training step: 56733, total_loss: 2.489795684814453
training step: 56734, total_loss: 0.47711074352264404
training step: 56735, total_loss: 0.3145453631877899
training step: 56736, total_loss: 0.0186467282474041
training step: 56737, total_loss: 2.765108108520508
training step: 56738, total_loss: 0.7764968872070312
training step: 56739, total_loss: 1.0648614168167114
training step: 56740, total_loss: 1.9785895347595215
training step: 56741, total_loss: 1.1482462882995605
training step: 56742, total_loss: 1.6929643154144287
training step: 56743, total_loss: 1.1908540725708008
training step: 56744, total_loss: 2.0463476181030273
training step: 56745, total_loss: 2.815023899078369
training step: 56746, total_loss: 3.276611328125
training step: 56747, total_loss: 3.4817967414855957
training step: 56748, total_loss: 0.9157351851463318
training step: 56749, total_loss: 4.143808364868164
training step: 56750, total_loss: 1.4780347347259521
training step: 56751, total_loss: 1.447948932647705
training step: 56752, total_loss: 5.789608001708984
training step: 56753, total_loss: 1.578627586364746
training step: 56754, total_loss: 1.27919602394104
training step: 56755, total_loss: 1.057849645614624
training step: 56756, total_loss: 1.8261291980743408
training step: 56757, total_loss: 1.3254382610321045
training step: 56758, total_loss: 4.843803405761719
training step: 56759, total_loss: 0.9682120084762573
training step: 56760, total_loss: 3.1224160194396973
training step: 56761, total_loss: 0.0010349752847105265
training step: 56762, total_loss: 0.800940215587616
training step: 56763, total_loss: 1.345515489578247
training step: 56764, total_loss: 0.5693373680114746
training step: 56765, total_loss: 2.6280529499053955
training step: 56766, total_loss: 0.6078957915306091
training step: 56767, total_loss: 0.17988666892051697
training step: 56768, total_loss: 0.15154339373111725
training step: 56769, total_loss: 0.05110066384077072
training step: 56770, total_loss: 0.33342161774635315
training step: 56771, total_loss: 1.0167388916015625
training step: 56772, total_loss: 1.8642988204956055
training step: 56773, total_loss: 0.9442101716995239
training step: 56774, total_loss: 2.8593802452087402
training step: 56775, total_loss: 0.6404820680618286
training step: 56776, total_loss: 0.45272210240364075
training step: 56777, total_loss: 1.2262029647827148
training step: 56778, total_loss: 1.3525258302688599
training step: 56779, total_loss: 0.3347921073436737
training step: 56780, total_loss: 0.36863768100738525
training step: 56781, total_loss: 2.7500433921813965
training step: 56782, total_loss: 0.9240090250968933
training step: 56783, total_loss: 1.6893048286437988
training step: 56784, total_loss: 1.2152736186981201
training step: 56785, total_loss: 0.20244747400283813
training step: 56786, total_loss: 0.9022655487060547
training step: 56787, total_loss: 1.6492112874984741
training step: 56788, total_loss: 1.2072341442108154
training step: 56789, total_loss: 0.6923888325691223
training step: 56790, total_loss: 0.8247828483581543
training step: 56791, total_loss: 0.26361143589019775
training step: 56792, total_loss: 0.8742250800132751
training step: 56793, total_loss: 0.8319769501686096
training step: 56794, total_loss: 1.0807363986968994
training step: 56795, total_loss: 2.186896800994873
training step: 56796, total_loss: 4.6132402420043945
training step: 56797, total_loss: 0.7827070951461792
training step: 56798, total_loss: 1.0196176767349243
training step: 56799, total_loss: 2.6953563690185547
training step: 56800, total_loss: 0.7251768112182617
training step: 56801, total_loss: 0.9968016147613525
training step: 56802, total_loss: 1.0721256732940674
training step: 56803, total_loss: 1.29184091091156
training step: 56804, total_loss: 0.20898982882499695
training step: 56805, total_loss: 1.5811872482299805
training step: 56806, total_loss: 0.477097749710083
training step: 56807, total_loss: 1.2298765182495117
training step: 56808, total_loss: 3.3047285079956055
training step: 56809, total_loss: 0.6858750581741333
training step: 56810, total_loss: 2.239122152328491
training step: 56811, total_loss: 2.2777271270751953
training step: 56812, total_loss: 0.4731072783470154
training step: 56813, total_loss: 1.08445405960083
training step: 56814, total_loss: 0.5939508080482483
training step: 56815, total_loss: 0.10872003436088562
training step: 56816, total_loss: 2.7702150344848633
training step: 56817, total_loss: 0.0061502959579229355
training step: 56818, total_loss: 1.3802082538604736
training step: 56819, total_loss: 0.07156962156295776
training step: 56820, total_loss: 0.2781130075454712
training step: 56821, total_loss: 2.6511785984039307
training step: 56822, total_loss: 2.072232723236084
training step: 56823, total_loss: 1.2597157955169678
training step: 56824, total_loss: 6.456206321716309
training step: 56825, total_loss: 1.0421220064163208
training step: 56826, total_loss: 0.8071719408035278
training step: 56827, total_loss: 2.593604326248169
training step: 56828, total_loss: 1.41361403465271
training step: 56829, total_loss: 0.9504573345184326
training step: 56830, total_loss: 0.8990007638931274
training step: 56831, total_loss: 2.497403621673584
training step: 56832, total_loss: 0.04502641409635544
training step: 56833, total_loss: 0.02628585696220398
training step: 56834, total_loss: 3.6060500144958496
training step: 56835, total_loss: 2.227163553237915
training step: 56836, total_loss: 1.325969934463501
training step: 56837, total_loss: 0.12384390830993652
training step: 56838, total_loss: 1.6052061319351196
training step: 56839, total_loss: 1.1603038311004639
training step: 56840, total_loss: 0.3320273756980896
training step: 56841, total_loss: 0.021264126524329185
training step: 56842, total_loss: 1.0428922176361084
training step: 56843, total_loss: 3.4461669921875
training step: 56844, total_loss: 2.4596657752990723
training step: 56845, total_loss: 0.19730131328105927
training step: 56846, total_loss: 0.24295315146446228
training step: 56847, total_loss: 0.5411108732223511
training step: 56848, total_loss: 0.27348294854164124
training step: 56849, total_loss: 1.0979106426239014
training step: 56850, total_loss: 0.30627620220184326
training step: 56851, total_loss: 1.7069385051727295
training step: 56852, total_loss: 3.3085179328918457
training step: 56853, total_loss: 1.9564048051834106
training step: 56854, total_loss: 0.5847311019897461
training step: 56855, total_loss: 1.8145673274993896
training step: 56856, total_loss: 0.3035777509212494
training step: 56857, total_loss: 0.12815901637077332
training step: 56858, total_loss: 2.2870442867279053
training step: 56859, total_loss: 0.9949696063995361
training step: 56860, total_loss: 0.4642804265022278
training step: 56861, total_loss: 2.933401107788086
training step: 56862, total_loss: 0.13093465566635132
training step: 56863, total_loss: 2.6746702194213867
training step: 56864, total_loss: 1.8865097761154175
training step: 56865, total_loss: 1.2345463037490845
training step: 56866, total_loss: 2.927704334259033
training step: 56867, total_loss: 0.35339969396591187
training step: 56868, total_loss: 1.5190781354904175
training step: 56869, total_loss: 0.010162297636270523INFO:tensorflow:Writing predictions to: test_output/predictions_57000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_57000.json

training step: 56870, total_loss: 1.705773115158081
training step: 56871, total_loss: 1.6541881561279297
training step: 56872, total_loss: 1.8327654600143433
training step: 56873, total_loss: 2.337371349334717
training step: 56874, total_loss: 0.46686115860939026
training step: 56875, total_loss: 1.60493803024292
training step: 56876, total_loss: 4.827486038208008
training step: 56877, total_loss: 2.5102357864379883
training step: 56878, total_loss: 1.411211371421814
training step: 56879, total_loss: 0.9967369437217712
training step: 56880, total_loss: 0.5170214176177979
training step: 56881, total_loss: 3.5960144996643066
training step: 56882, total_loss: 1.2853877544403076
training step: 56883, total_loss: 0.5748375654220581
training step: 56884, total_loss: 2.60493803024292
training step: 56885, total_loss: 2.3135008811950684
training step: 56886, total_loss: 3.129236936569214
training step: 56887, total_loss: 0.802579402923584
training step: 56888, total_loss: 1.4399784803390503
training step: 56889, total_loss: 2.7497565746307373
training step: 56890, total_loss: 0.6996594667434692
training step: 56891, total_loss: 1.505634069442749
training step: 56892, total_loss: 2.5954363346099854
training step: 56893, total_loss: 2.7549800872802734
training step: 56894, total_loss: 0.24084238708019257
training step: 56895, total_loss: 0.31628671288490295
training step: 56896, total_loss: 2.461658239364624
training step: 56897, total_loss: 2.0023417472839355
training step: 56898, total_loss: 1.8020586967468262
training step: 56899, total_loss: 2.0872316360473633
training step: 56900, total_loss: 5.417844772338867
training step: 56901, total_loss: 2.434439182281494
training step: 56902, total_loss: 0.4827256500720978
training step: 56903, total_loss: 1.8992472887039185
training step: 56904, total_loss: 1.2614178657531738
training step: 56905, total_loss: 1.1213047504425049
training step: 56906, total_loss: 1.9608728885650635
training step: 56907, total_loss: 0.19278846681118011
training step: 56908, total_loss: 0.29179278016090393
training step: 56909, total_loss: 1.1036299467086792
training step: 56910, total_loss: 2.5388617515563965
training step: 56911, total_loss: 3.310060739517212
training step: 56912, total_loss: 0.6599247455596924
training step: 56913, total_loss: 3.215441942214966
training step: 56914, total_loss: 0.004107649903744459
training step: 56915, total_loss: 3.3753485679626465
training step: 56916, total_loss: 1.038750171661377
training step: 56917, total_loss: 2.006622791290283
training step: 56918, total_loss: 1.724796175956726
training step: 56919, total_loss: 1.2266888618469238
training step: 56920, total_loss: 2.993441343307495
training step: 56921, total_loss: 0.04440725967288017
training step: 56922, total_loss: 0.0008261175244115293
training step: 56923, total_loss: 1.6116595268249512
training step: 56924, total_loss: 2.477874755859375
training step: 56925, total_loss: 2.9051671028137207
training step: 56926, total_loss: 1.0337847471237183
training step: 56927, total_loss: 1.013256549835205
training step: 56928, total_loss: 3.586893081665039
training step: 56929, total_loss: 3.3695008754730225
training step: 56930, total_loss: 0.390713095664978
training step: 56931, total_loss: 1.7451813220977783
training step: 56932, total_loss: 1.821580171585083
training step: 56933, total_loss: 2.4719715118408203
training step: 56934, total_loss: 0.03922802582383156
training step: 56935, total_loss: 0.4918014109134674
training step: 56936, total_loss: 2.1445963382720947
training step: 56937, total_loss: 2.985750913619995
training step: 56938, total_loss: 0.7659987807273865
training step: 56939, total_loss: 3.7055859565734863
training step: 56940, total_loss: 2.7972235679626465
training step: 56941, total_loss: 1.1050045490264893
training step: 56942, total_loss: 2.984283685684204
training step: 56943, total_loss: 1.8190972805023193
training step: 56944, total_loss: 0.5733326077461243
training step: 56945, total_loss: 0.08650242537260056
training step: 56946, total_loss: 0.5327050685882568
training step: 56947, total_loss: 0.6726762652397156
training step: 56948, total_loss: 0.9559483528137207
training step: 56949, total_loss: 0.13878238201141357
training step: 56950, total_loss: 0.42772749066352844
training step: 56951, total_loss: 2.5890097618103027
training step: 56952, total_loss: 1.9962890148162842
training step: 56953, total_loss: 1.057581901550293
training step: 56954, total_loss: 2.690612316131592
training step: 56955, total_loss: 1.940218210220337
training step: 56956, total_loss: 2.2954349517822266
training step: 56957, total_loss: 2.4548964500427246
training step: 56958, total_loss: 1.6265994310379028
training step: 56959, total_loss: 4.548411846160889
training step: 56960, total_loss: 0.6802031993865967
training step: 56961, total_loss: 0.4609319567680359
training step: 56962, total_loss: 0.05658278241753578
training step: 56963, total_loss: 3.594705104827881
training step: 56964, total_loss: 1.9855058193206787
training step: 56965, total_loss: 2.0517468452453613
training step: 56966, total_loss: 0.9473659992218018
training step: 56967, total_loss: 1.5404995679855347
training step: 56968, total_loss: 1.1209732294082642
training step: 56969, total_loss: 0.09551634639501572
training step: 56970, total_loss: 1.6896002292633057
training step: 56971, total_loss: 0.35706934332847595
training step: 56972, total_loss: 1.1575472354888916
training step: 56973, total_loss: 2.14796781539917
training step: 56974, total_loss: 3.4718379974365234
training step: 56975, total_loss: 1.863998293876648
training step: 56976, total_loss: 3.6690683364868164
training step: 56977, total_loss: 0.22281578183174133
training step: 56978, total_loss: 2.4607415199279785
training step: 56979, total_loss: 1.8942766189575195
training step: 56980, total_loss: 1.152115821838379
training step: 56981, total_loss: 0.7084537744522095
training step: 56982, total_loss: 4.197503089904785
training step: 56983, total_loss: 2.8732457160949707
training step: 56984, total_loss: 2.7602033615112305
training step: 56985, total_loss: 2.386852741241455
training step: 56986, total_loss: 4.326785087585449
training step: 56987, total_loss: 3.9272685050964355
training step: 56988, total_loss: 0.0973627120256424
training step: 56989, total_loss: 3.4176130294799805
training step: 56990, total_loss: 1.71044921875
training step: 56991, total_loss: 7.046857833862305
training step: 56992, total_loss: 0.19297491014003754
training step: 56993, total_loss: 0.58807772397995
training step: 56994, total_loss: 4.121580123901367
training step: 56995, total_loss: 0.5353085398674011
training step: 56996, total_loss: 1.6247457265853882
training step: 56997, total_loss: 0.2072761356830597
training step: 56998, total_loss: 2.847626209259033
training step: 56999, total_loss: 4.5602617263793945
training step: 57000, total_loss: 0.577261209487915
epoch finished! shuffle=False
evaluation: 21000, total_loss: 1.8305577039718628, f1: 58.17336508257638, followup: 39.52533089913282, yesno: 74.0453369846341, heq: 53.29377757492774, dheq: 3.6

Model saved in path test_output//model_57000.ckpt
training step: 57001, total_loss: 5.131108283996582
training step: 57002, total_loss: 4.3941450119018555
training step: 57003, total_loss: 1.980386734008789
training step: 57004, total_loss: 0.021979285404086113
training step: 57005, total_loss: 0.48037299513816833
training step: 57006, total_loss: 1.1190578937530518
training step: 57007, total_loss: 1.0705510377883911
training step: 57008, total_loss: 2.2719719409942627
training step: 57009, total_loss: 7.599447250366211
training step: 57010, total_loss: 1.8196299076080322
training step: 57011, total_loss: 1.6420621871948242
training step: 57012, total_loss: 0.4077056646347046
training step: 57013, total_loss: 1.556145191192627
training step: 57014, total_loss: 0.653172492980957
training step: 57015, total_loss: 1.8601940870285034
training step: 57016, total_loss: 2.6391868591308594
training step: 57017, total_loss: 1.532466173171997
training step: 57018, total_loss: 3.602694034576416
training step: 57019, total_loss: 0.8949365615844727
training step: 57020, total_loss: 1.1894221305847168
training step: 57021, total_loss: 1.1513192653656006
training step: 57022, total_loss: 0.4230957329273224
training step: 57023, total_loss: 1.1028391122817993
training step: 57024, total_loss: 1.8109450340270996
training step: 57025, total_loss: 0.418634831905365
training step: 57026, total_loss: 0.7279130220413208
training step: 57027, total_loss: 0.9540360569953918
training step: 57028, total_loss: 3.3488237857818604
training step: 57029, total_loss: 0.37937021255493164
training step: 57030, total_loss: 0.3350255787372589
training step: 57031, total_loss: 2.76693058013916
training step: 57032, total_loss: 0.050912342965602875
training step: 57033, total_loss: 0.02712078206241131
training step: 57034, total_loss: 3.0356295108795166
training step: 57035, total_loss: 1.969820261001587
training step: 57036, total_loss: 2.1589455604553223
training step: 57037, total_loss: 0.911137580871582
training step: 57038, total_loss: 1.770654559135437
training step: 57039, total_loss: 0.040148090571165085
training step: 57040, total_loss: 4.444875717163086
training step: 57041, total_loss: 2.1669135093688965
training step: 57042, total_loss: 5.27671480178833
training step: 57043, total_loss: 0.6795881390571594
training step: 57044, total_loss: 0.40064460039138794
training step: 57045, total_loss: 1.829309344291687
training step: 57046, total_loss: 1.2698513269424438
training step: 57047, total_loss: 0.9863224625587463
training step: 57048, total_loss: 0.6654035449028015
training step: 57049, total_loss: 0.5019555687904358
training step: 57050, total_loss: 0.5378754734992981
training step: 57051, total_loss: 0.4765547513961792
training step: 57052, total_loss: 1.209814190864563
training step: 57053, total_loss: 2.9703378677368164
training step: 57054, total_loss: 2.85172700881958
training step: 57055, total_loss: 4.05164098739624
training step: 57056, total_loss: 1.0294101238250732
training step: 57057, total_loss: 0.44322264194488525
training step: 57058, total_loss: 4.27571964263916
training step: 57059, total_loss: 1.2119864225387573
training step: 57060, total_loss: 2.8889522552490234
training step: 57061, total_loss: 1.2726173400878906
training step: 57062, total_loss: 0.9741664528846741
training step: 57063, total_loss: 1.6541547775268555
training step: 57064, total_loss: 0.9612464904785156
training step: 57065, total_loss: 0.12349627912044525
training step: 57066, total_loss: 0.23779819905757904
training step: 57067, total_loss: 1.926969051361084
training step: 57068, total_loss: 2.237379550933838
training step: 57069, total_loss: 1.7764554023742676
training step: 57070, total_loss: 2.117331027984619
training step: 57071, total_loss: 1.5443823337554932
training step: 57072, total_loss: 0.18784672021865845
training step: 57073, total_loss: 1.912858247756958
training step: 57074, total_loss: 1.6250604391098022
training step: 57075, total_loss: 5.3058905601501465
training step: 57076, total_loss: 2.057713508605957
training step: 57077, total_loss: 1.48957097530365
training step: 57078, total_loss: 2.028552293777466
training step: 57079, total_loss: 1.135127067565918
training step: 57080, total_loss: 1.349290132522583
training step: 57081, total_loss: 0.07339855283498764
training step: 57082, total_loss: 0.4038066267967224
training step: 57083, total_loss: 0.8315997123718262
training step: 57084, total_loss: 0.7503483891487122
training step: 57085, total_loss: 0.09441827982664108
training step: 57086, total_loss: 0.5502375960350037
training step: 57087, total_loss: 4.029229640960693
training step: 57088, total_loss: 4.497918128967285
training step: 57089, total_loss: 2.9541661739349365
training step: 57090, total_loss: 0.3646417260169983
training step: 57091, total_loss: 0.5387132167816162
training step: 57092, total_loss: 1.7151687145233154
training step: 57093, total_loss: 0.6269890069961548
training step: 57094, total_loss: 1.760330080986023
training step: 57095, total_loss: 1.332255482673645
training step: 57096, total_loss: 0.5285778045654297
training step: 57097, total_loss: 0.8379231095314026
training step: 57098, total_loss: 0.1292494535446167
training step: 57099, total_loss: 1.095083236694336
training step: 57100, total_loss: 1.1377897262573242
training step: 57101, total_loss: 1.5778112411499023
training step: 57102, total_loss: 1.7302316427230835
training step: 57103, total_loss: 1.982309103012085
training step: 57104, total_loss: 3.798755645751953
training step: 57105, total_loss: 1.9334166049957275
training step: 57106, total_loss: 0.02039770595729351
training step: 57107, total_loss: 1.8948047161102295
training step: 57108, total_loss: 0.740462064743042
training step: 57109, total_loss: 0.7385253310203552
training step: 57110, total_loss: 2.0114684104919434
training step: 57111, total_loss: 1.5890982151031494
training step: 57112, total_loss: 4.0555877685546875
training step: 57113, total_loss: 1.7306249141693115
training step: 57114, total_loss: 0.7272042036056519
training step: 57115, total_loss: 3.172567367553711
training step: 57116, total_loss: 1.1355193853378296
training step: 57117, total_loss: 1.6762959957122803
training step: 57118, total_loss: 2.4956259727478027
training step: 57119, total_loss: 2.967463493347168
training step: 57120, total_loss: 3.2324063777923584
training step: 57121, total_loss: 0.5290690660476685
training step: 57122, total_loss: 1.42274808883667
training step: 57123, total_loss: 1.7454733848571777
training step: 57124, total_loss: 1.7172764539718628
training step: 57125, total_loss: 1.813299298286438
training step: 57126, total_loss: 2.73451828956604
training step: 57127, total_loss: 4.02393913269043
training step: 57128, total_loss: 0.7496833801269531
training step: 57129, total_loss: 3.2410521507263184
training step: 57130, total_loss: 1.0608946084976196
training step: 57131, total_loss: 0.594508945941925
training step: 57132, total_loss: 1.120336651802063
training step: 57133, total_loss: 1.175707459449768
training step: 57134, total_loss: 1.743948221206665
training step: 57135, total_loss: 1.7000681161880493
training step: 57136, total_loss: 0.38766607642173767
training step: 57137, total_loss: 1.7770981788635254
training step: 57138, total_loss: 5.946178913116455
training step: 57139, total_loss: 0.0630846917629242
training step: 57140, total_loss: 1.6041412353515625
training step: 57141, total_loss: 1.385202407836914
training step: 57142, total_loss: 1.4398061037063599
training step: 57143, total_loss: 2.8445968627929688
training step: 57144, total_loss: 0.44528836011886597
training step: 57145, total_loss: 2.060892105102539
training step: 57146, total_loss: 0.657923698425293
training step: 57147, total_loss: 2.4757657051086426
training step: 57148, total_loss: 2.6445212364196777
training step: 57149, total_loss: 3.969799757003784
training step: 57150, total_loss: 2.3074965476989746
training step: 57151, total_loss: 4.293895244598389
training step: 57152, total_loss: 2.1719961166381836
training step: 57153, total_loss: 2.171018600463867
training step: 57154, total_loss: 2.807260274887085
training step: 57155, total_loss: 1.6761085987091064
training step: 57156, total_loss: 0.7155282497406006
training step: 57157, total_loss: 1.0362910032272339
training step: 57158, total_loss: 0.8139715790748596
training step: 57159, total_loss: 0.34278035163879395
training step: 57160, total_loss: 2.818795680999756
training step: 57161, total_loss: 1.7417768239974976
training step: 57162, total_loss: 0.27904385328292847
training step: 57163, total_loss: 0.06934447586536407
training step: 57164, total_loss: 0.6465840339660645
training step: 57165, total_loss: 0.9148457050323486
training step: 57166, total_loss: 1.8946651220321655
training step: 57167, total_loss: 1.210653305053711
training step: 57168, total_loss: 1.7889820337295532
training step: 57169, total_loss: 0.37618720531463623
training step: 57170, total_loss: 0.32517197728157043
training step: 57171, total_loss: 3.5639007091522217
training step: 57172, total_loss: 1.6218738555908203
training step: 57173, total_loss: 1.9623796939849854
training step: 57174, total_loss: 2.941019296646118
training step: 57175, total_loss: 0.0988205224275589
training step: 57176, total_loss: 4.186774730682373
training step: 57177, total_loss: 1.7747740745544434
training step: 57178, total_loss: 2.936556577682495
training step: 57179, total_loss: 0.3311827480792999
training step: 57180, total_loss: 0.32689645886421204
training step: 57181, total_loss: 2.863618850708008
training step: 57182, total_loss: 3.021688461303711
training step: 57183, total_loss: 0.29052606225013733
training step: 57184, total_loss: 0.33556824922561646
training step: 57185, total_loss: 1.2877578735351562
training step: 57186, total_loss: 1.6545559167861938
training step: 57187, total_loss: 2.042360782623291
training step: 57188, total_loss: 3.54599666595459
training step: 57189, total_loss: 1.5519299507141113
training step: 57190, total_loss: 3.2317943572998047
training step: 57191, total_loss: 1.3559303283691406
training step: 57192, total_loss: 0.5319823622703552
training step: 57193, total_loss: 0.7378057241439819
training step: 57194, total_loss: 1.3504987955093384
training step: 57195, total_loss: 2.1006250381469727
training step: 57196, total_loss: 0.01483975350856781
training step: 57197, total_loss: 0.8549207448959351
training step: 57198, total_loss: 0.020854538306593895
training step: 57199, total_loss: 1.3796857595443726
training step: 57200, total_loss: 0.2173168808221817
training step: 57201, total_loss: 1.5422624349594116
training step: 57202, total_loss: 1.9121019840240479
training step: 57203, total_loss: 0.46079134941101074
training step: 57204, total_loss: 1.7024753093719482
training step: 57205, total_loss: 1.0372543334960938
training step: 57206, total_loss: 0.23207788169384003
training step: 57207, total_loss: 3.0953569412231445
training step: 57208, total_loss: 0.33939534425735474
training step: 57209, total_loss: 1.8694443702697754
training step: 57210, total_loss: 1.4875189065933228
training step: 57211, total_loss: 2.39786958694458
training step: 57212, total_loss: 1.1559453010559082
training step: 57213, total_loss: 1.84147310256958
training step: 57214, total_loss: 2.4081015586853027
training step: 57215, total_loss: 2.621337890625
training step: 57216, total_loss: 1.7657417058944702
training step: 57217, total_loss: 0.3693651556968689
training step: 57218, total_loss: 1.1076886653900146
training step: 57219, total_loss: 2.8974671363830566
training step: 57220, total_loss: 4.28427267074585
training step: 57221, total_loss: 2.2412304878234863
training step: 57222, total_loss: 3.131520986557007
training step: 57223, total_loss: 0.0955767035484314
training step: 57224, total_loss: 0.6703219413757324
training step: 57225, total_loss: 1.1278992891311646
training step: 57226, total_loss: 1.7635759115219116
training step: 57227, total_loss: 0.6197953224182129
training step: 57228, total_loss: 1.5589128732681274
training step: 57229, total_loss: 4.873111248016357
training step: 57230, total_loss: 1.043044924736023
training step: 57231, total_loss: 2.0062031745910645
training step: 57232, total_loss: 0.524482250213623
training step: 57233, total_loss: 0.5596567392349243
training step: 57234, total_loss: 0.1634463518857956
training step: 57235, total_loss: 4.541171550750732
training step: 57236, total_loss: 0.6751868724822998
training step: 57237, total_loss: 1.7288429737091064
training step: 57238, total_loss: 4.518630504608154
training step: 57239, total_loss: 2.1271305084228516
training step: 57240, total_loss: 2.921985149383545
training step: 57241, total_loss: 3.9902617931365967
training step: 57242, total_loss: 2.8356480598449707
training step: 57243, total_loss: 2.023245334625244
training step: 57244, total_loss: 1.1611523628234863
training step: 57245, total_loss: 1.0446672439575195
training step: 57246, total_loss: 1.1084953546524048
training step: 57247, total_loss: 1.0844848155975342
training step: 57248, total_loss: 1.0504605770111084
training step: 57249, total_loss: 1.450909972190857
training step: 57250, total_loss: 1.1801905632019043
training step: 57251, total_loss: 0.17549966275691986
training step: 57252, total_loss: 0.671404242515564
training step: 57253, total_loss: 1.7363779544830322
training step: 57254, total_loss: 0.061553023755550385
training step: 57255, total_loss: 1.1534550189971924
training step: 57256, total_loss: 3.911297082901001
training step: 57257, total_loss: 0.9922245740890503
training step: 57258, total_loss: 0.3825171887874603
training step: 57259, total_loss: 1.4760446548461914
training step: 57260, total_loss: 0.7029155492782593
training step: 57261, total_loss: 0.020451582968235016
training step: 57262, total_loss: 0.6168567538261414
training step: 57263, total_loss: 1.044121265411377
training step: 57264, total_loss: 5.133614540100098
training step: 57265, total_loss: 3.068016529083252
training step: 57266, total_loss: 1.057175636291504
training step: 57267, total_loss: 0.9235899448394775
training step: 57268, total_loss: 0.9867544174194336
training step: 57269, total_loss: 0.26496046781539917
training step: 57270, total_loss: 2.1746511459350586
training step: 57271, total_loss: 0.02271380089223385
training step: 57272, total_loss: 1.1823192834854126
training step: 57273, total_loss: 0.0037157139740884304
training step: 57274, total_loss: 1.1888364553451538
training step: 57275, total_loss: 2.7476963996887207
training step: 57276, total_loss: 2.6622400283813477
training step: 57277, total_loss: 0.6118217706680298
training step: 57278, total_loss: 0.3746500015258789
training step: 57279, total_loss: 1.1092780828475952
training step: 57280, total_loss: 1.5706461668014526
training step: 57281, total_loss: 3.455564498901367
training step: 57282, total_loss: 2.820378541946411
training step: 57283, total_loss: 1.6513307094573975
training step: 57284, total_loss: 3.207458019256592
training step: 57285, total_loss: 0.6361351609230042
training step: 57286, total_loss: 0.19623586535453796
training step: 57287, total_loss: 1.642212152481079
training step: 57288, total_loss: 4.639589309692383
training step: 57289, total_loss: 2.1833913326263428
training step: 57290, total_loss: 1.1178901195526123
training step: 57291, total_loss: 2.0424282550811768
training step: 57292, total_loss: 1.4741171598434448
training step: 57293, total_loss: 0.4447605311870575
training step: 57294, total_loss: 2.6757302284240723
training step: 57295, total_loss: 3.133753776550293
training step: 57296, total_loss: 1.1188414096832275
training step: 57297, total_loss: 0.7197182178497314
training step: 57298, total_loss: 1.7421669960021973
training step: 57299, total_loss: 0.0005903588607907295
training step: 57300, total_loss: 0.7789556980133057
training step: 57301, total_loss: 4.365086078643799
training step: 57302, total_loss: 0.0032743560150265694
training step: 57303, total_loss: 1.7139201164245605
training step: 57304, total_loss: 3.1106343269348145
training step: 57305, total_loss: 1.6031608581542969
training step: 57306, total_loss: 0.11536287516355515
training step: 57307, total_loss: 1.7698545455932617
training step: 57308, total_loss: 1.3946839570999146
training step: 57309, total_loss: 3.415475845336914
training step: 57310, total_loss: 3.027675151824951
training step: 57311, total_loss: 3.9654154777526855
training step: 57312, total_loss: 1.327798843383789
training step: 57313, total_loss: 0.9991430044174194
training step: 57314, total_loss: 3.5034615993499756
training step: 57315, total_loss: 0.8133252859115601
training step: 57316, total_loss: 1.5238450765609741
training step: 57317, total_loss: 0.9865320920944214
training step: 57318, total_loss: 0.21171142160892487
training step: 57319, total_loss: 0.4849185049533844
training step: 57320, total_loss: 2.150113821029663
training step: 57321, total_loss: 3.231487274169922
training step: 57322, total_loss: 0.7543686628341675
training step: 57323, total_loss: 2.4585766792297363
training step: 57324, total_loss: 1.4937901496887207
training step: 57325, total_loss: 1.842179298400879
training step: 57326, total_loss: 0.13476844131946564
training step: 57327, total_loss: 1.4403574466705322
training step: 57328, total_loss: 1.82313871383667
training step: 57329, total_loss: 0.6325183510780334
training step: 57330, total_loss: 1.519263505935669
training step: 57331, total_loss: 2.077993392944336
training step: 57332, total_loss: 1.7220946550369263
training step: 57333, total_loss: 2.6445236206054688
training step: 57334, total_loss: 3.0584988594055176
training step: 57335, total_loss: 0.9571360349655151
training step: 57336, total_loss: 0.6494846343994141
training step: 57337, total_loss: 0.25854337215423584
training step: 57338, total_loss: 0.1538187563419342
training step: 57339, total_loss: 0.2092830091714859
training step: 57340, total_loss: 1.548513412475586
training step: 57341, total_loss: 1.1043825149536133
training step: 57342, total_loss: 2.2457518577575684
training step: 57343, total_loss: 3.598123550415039
training step: 57344, total_loss: 1.8374443054199219
training step: 57345, total_loss: 1.6538524627685547
training step: 57346, total_loss: 1.408063530921936
training step: 57347, total_loss: 2.7598185539245605
training step: 57348, total_loss: 1.1440913677215576
training step: 57349, total_loss: 1.3552699089050293
training step: 57350, total_loss: 6.473004341125488
training step: 57351, total_loss: 4.918662071228027
training step: 57352, total_loss: 1.336413025856018
training step: 57353, total_loss: 1.7077962160110474
training step: 57354, total_loss: 0.9441393613815308
training step: 57355, total_loss: 2.0255796909332275
training step: 57356, total_loss: 1.819272518157959
training step: 57357, total_loss: 1.7982769012451172
training step: 57358, total_loss: 0.0044281864538788795
training step: 57359, total_loss: 4.152584075927734
training step: 57360, total_loss: 0.8432830572128296
training step: 57361, total_loss: 2.6606740951538086
training step: 57362, total_loss: 2.0869009494781494
training step: 57363, total_loss: 0.4181375801563263
training step: 57364, total_loss: 1.4181976318359375
training step: 57365, total_loss: 0.40529370307922363
training step: 57366, total_loss: 4.630727767944336
training step: 57367, total_loss: 0.30773720145225525
training step: 57368, total_loss: 0.8494246602058411
training step: 57369, total_loss: 2.5237059593200684
training step: 57370, total_loss: 1.4876513481140137
training step: 57371, total_loss: 1.3873876333236694
training step: 57372, total_loss: 0.09153137356042862
training step: 57373, total_loss: 3.6678898334503174
training step: 57374, total_loss: 1.2138763666152954
training step: 57375, total_loss: 0.9299520254135132
training step: 57376, total_loss: 1.5357811450958252
training step: 57377, total_loss: 0.462760865688324
training step: 57378, total_loss: 2.1931252479553223
training step: 57379, total_loss: 1.249604344367981
training step: 57380, total_loss: 2.162940502166748
training step: 57381, total_loss: 0.27711355686187744
training step: 57382, total_loss: 3.0770437717437744
training step: 57383, total_loss: 1.2953248023986816
training step: 57384, total_loss: 2.2682604789733887
training step: 57385, total_loss: 1.040994644165039
training step: 57386, total_loss: 0.6293115019798279
training step: 57387, total_loss: 1.6235005855560303
training step: 57388, total_loss: 3.1281039714813232
training step: 57389, total_loss: 1.8100227117538452
training step: 57390, total_loss: 0.48232319951057434
training step: 57391, total_loss: 1.0001990795135498
training step: 57392, total_loss: 2.6843342781066895
training step: 57393, total_loss: 0.6378517746925354
training step: 57394, total_loss: 1.7432775497436523
training step: 57395, total_loss: 0.6695480346679688
training step: 57396, total_loss: 2.8319530487060547
training step: 57397, total_loss: 2.1588778495788574
training step: 57398, total_loss: 1.3542516231536865
training step: 57399, total_loss: 3.976705551147461
training step: 57400, total_loss: 0.0016261331038549542
training step: 57401, total_loss: 1.4669241905212402
training step: 57402, total_loss: 0.5812863111495972
training step: 57403, total_loss: 1.5661463737487793
training step: 57404, total_loss: 0.18521210551261902
training step: 57405, total_loss: 0.09528005123138428
training step: 57406, total_loss: 1.6524100303649902
training step: 57407, total_loss: 1.632440447807312
training step: 57408, total_loss: 0.8308044672012329
training step: 57409, total_loss: 1.7659318447113037
training step: 57410, total_loss: 1.1082050800323486
training step: 57411, total_loss: 4.696308612823486
training step: 57412, total_loss: 1.1935898065567017
training step: 57413, total_loss: 1.2564693689346313
training step: 57414, total_loss: 2.17413067817688
training step: 57415, total_loss: 0.6765056252479553
training step: 57416, total_loss: 3.6719157695770264
training step: 57417, total_loss: 1.488208532333374
training step: 57418, total_loss: 0.43879434466362
training step: 57419, total_loss: 1.143298625946045
training step: 57420, total_loss: 4.476344108581543
training step: 57421, total_loss: 1.555528163909912
training step: 57422, total_loss: 5.943452835083008
training step: 57423, total_loss: 3.5069408416748047
training step: 57424, total_loss: 1.9194955825805664
training step: 57425, total_loss: 3.148857355117798
training step: 57426, total_loss: 0.20896795392036438
training step: 57427, total_loss: 1.6983933448791504
training step: 57428, total_loss: 1.2592086791992188
training step: 57429, total_loss: 0.9789870381355286
training step: 57430, total_loss: 1.7777318954467773
training step: 57431, total_loss: 2.254661798477173
training step: 57432, total_loss: 1.201802134513855
training step: 57433, total_loss: 3.6447906494140625
training step: 57434, total_loss: 1.3217954635620117
training step: 57435, total_loss: 1.3605674505233765
training step: 57436, total_loss: 0.41094034910202026
training step: 57437, total_loss: 6.508606020361185e-05
training step: 57438, total_loss: 0.24712803959846497
training step: 57439, total_loss: 1.8067190647125244
training step: 57440, total_loss: 0.37646475434303284
training step: 57441, total_loss: 2.1651010513305664
training step: 57442, total_loss: 1.5504305362701416
training step: 57443, total_loss: 1.6527907848358154
training step: 57444, total_loss: 0.0026339227333664894
training step: 57445, total_loss: 2.0785555839538574
training step: 57446, total_loss: 2.0061986446380615
training step: 57447, total_loss: 0.023529164493083954
training step: 57448, total_loss: 0.9255557060241699
training step: 57449, total_loss: 3.4716310501098633
training step: 57450, total_loss: 2.0592527389526367
training step: 57451, total_loss: 3.9257423877716064
training step: 57452, total_loss: 0.9525896906852722
training step: 57453, total_loss: 1.63508939743042
training step: 57454, total_loss: 2.1417484283447266
training step: 57455, total_loss: 1.2569079399108887
training step: 57456, total_loss: 1.8743691444396973
training step: 57457, total_loss: 0.1538705974817276
training step: 57458, total_loss: 3.5928492546081543
training step: 57459, total_loss: 1.4535026550292969
training step: 57460, total_loss: 2.9877243041992188
training step: 57461, total_loss: 2.087679386138916
training step: 57462, total_loss: 1.999213457107544
training step: 57463, total_loss: 1.4960898160934448
training step: 57464, total_loss: 3.8870792388916016
training step: 57465, total_loss: 1.049952745437622
training step: 57466, total_loss: 1.1308634281158447
training step: 57467, total_loss: 5.851529121398926
training step: 57468, total_loss: 0.3143260180950165
training step: 57469, total_loss: 3.494554042816162
training step: 57470, total_loss: 1.622178077697754
training step: 57471, total_loss: 2.594911813735962
training step: 57472, total_loss: 0.07272006571292877
training step: 57473, total_loss: 2.286330223083496
training step: 57474, total_loss: 2.1326234340667725
training step: 57475, total_loss: 1.9171638488769531
training step: 57476, total_loss: 0.16499997675418854
training step: 57477, total_loss: 2.1622533798217773
training step: 57478, total_loss: 1.8191450834274292
training step: 57479, total_loss: 6.631162643432617
training step: 57480, total_loss: 0.06251277774572372
training step: 57481, total_loss: 0.30980372428894043
training step: 57482, total_loss: 1.6266975402832031
training step: 57483, total_loss: 0.10938630253076553
training step: 57484, total_loss: 3.057835102081299
training step: 57485, total_loss: 1.6244066953659058
training step: 57486, total_loss: 2.3040075302124023
training step: 57487, total_loss: 3.357800245285034
training step: 57488, total_loss: 1.0333341360092163
training step: 57489, total_loss: 0.25043344497680664
training step: 57490, total_loss: 2.075320243835449
training step: 57491, total_loss: 1.4735363721847534
training step: 57492, total_loss: 1.881949543952942
training step: 57493, total_loss: 0.32167941331863403
training step: 57494, total_loss: 1.5678455829620361
training step: 57495, total_loss: 0.48992544412612915
training step: 57496, total_loss: 1.0141215324401855
training step: 57497, total_loss: 1.2281336784362793
training step: 57498, total_loss: 3.0888218879699707
training step: 57499, total_loss: 1.6159054040908813
training step: 57500, total_loss: 1.0993897914886475
training step: 57501, total_loss: 4.440856456756592
training step: 57502, total_loss: 0.16866156458854675
training step: 57503, total_loss: 0.9132093787193298
training step: 57504, total_loss: 0.2631760537624359
training step: 57505, total_loss: 0.6664516925811768
training step: 57506, total_loss: 0.9631786346435547
training step: 57507, total_loss: 0.46757960319519043
training step: 57508, total_loss: 0.2446487843990326
training step: 57509, total_loss: 0.9506499767303467
training step: 57510, total_loss: 3.252821207046509
training step: 57511, total_loss: 1.7901597023010254
training step: 57512, total_loss: 2.445565700531006
training step: 57513, total_loss: 0.031177448108792305
training step: 57514, total_loss: 1.141160488128662
training step: 57515, total_loss: 2.0390517711639404
training step: 57516, total_loss: 2.7054436206817627
training step: 57517, total_loss: 1.2195426225662231
training step: 57518, total_loss: 1.534024953842163
training step: 57519, total_loss: 0.5783354043960571
training step: 57520, total_loss: 0.3405565321445465
training step: 57521, total_loss: 0.4414740204811096
training step: 57522, total_loss: 0.3460656404495239
training step: 57523, total_loss: 0.053535185754299164
training step: 57524, total_loss: 0.4139382839202881
training step: 57525, total_loss: 2.302877902984619
training step: 57526, total_loss: 3.8644580841064453
training step: 57527, total_loss: 1.6084295511245728
training step: 57528, total_loss: 5.283494472503662
training step: 57529, total_loss: 1.5450440645217896
training step: 57530, total_loss: 3.422131061553955
training step: 57531, total_loss: 0.8163416981697083
training step: 57532, total_loss: 2.0893797874450684
training step: 57533, total_loss: 2.1408798694610596
training step: 57534, total_loss: 2.7819535732269287
training step: 57535, total_loss: 1.926224708557129
training step: 57536, total_loss: 2.8202905654907227
training step: 57537, total_loss: 2.5620369911193848
training step: 57538, total_loss: 2.632966995239258
training step: 57539, total_loss: 0.9260115623474121
training step: 57540, total_loss: 1.3284754753112793
training step: 57541, total_loss: 2.3374643325805664
training step: 57542, total_loss: 3.106639862060547
training step: 57543, total_loss: 2.2561850547790527
training step: 57544, total_loss: 2.6160366535186768
training step: 57545, total_loss: 3.4484448432922363
training step: 57546, total_loss: 2.642268180847168
training step: 57547, total_loss: 2.202288866043091
training step: 57548, total_loss: 2.3112268447875977
training step: 57549, total_loss: 1.637285590171814
training step: 57550, total_loss: 1.9482495784759521
training step: 57551, total_loss: 1.3760321140289307
training step: 57552, total_loss: 0.8939114809036255
training step: 57553, total_loss: 2.732180595397949
training step: 57554, total_loss: 4.070571422576904
training step: 57555, total_loss: 0.7977489233016968
training step: 57556, total_loss: 0.8558716773986816
training step: 57557, total_loss: 1.7076175212860107
training step: 57558, total_loss: 1.5733554363250732
training step: 57559, total_loss: 1.4484622478485107
training step: 57560, total_loss: 1.2642909288406372
training step: 57561, total_loss: 2.8100061416625977
training step: 57562, total_loss: 0.19859781861305237
training step: 57563, total_loss: 2.141960620880127
training step: 57564, total_loss: 2.366722345352173
training step: 57565, total_loss: 0.9807860255241394
training step: 57566, total_loss: 1.1033051013946533
training step: 57567, total_loss: 1.2458890676498413
training step: 57568, total_loss: 2.862799644470215
training step: 57569, total_loss: 0.2945813536643982
training step: 57570, total_loss: 5.133917808532715
training step: 57571, total_loss: 1.741011142730713
training step: 57572, total_loss: 2.2906546592712402
training step: 57573, total_loss: 3.334852457046509
training step: 57574, total_loss: 0.6300397515296936
training step: 57575, total_loss: 0.6795631647109985
training step: 57576, total_loss: 2.2468924522399902
training step: 57577, total_loss: 0.05914710462093353
training step: 57578, total_loss: 0.006337936967611313
training step: 57579, total_loss: 0.488492876291275
training step: 57580, total_loss: 1.1021032333374023
training step: 57581, total_loss: 0.1229366660118103
training step: 57582, total_loss: 0.6344417333602905
training step: 57583, total_loss: 0.8844071626663208
training step: 57584, total_loss: 2.4299302101135254
training step: 57585, total_loss: 0.024496130645275116
training step: 57586, total_loss: 1.3660545349121094
training step: 57587, total_loss: 1.1359615325927734
training step: 57588, total_loss: 2.3201866149902344
training step: 57589, total_loss: 2.3746261596679688
training step: 57590, total_loss: 0.32897770404815674
training step: 57591, total_loss: 0.3920043110847473
training step: 57592, total_loss: 1.6741018295288086
training step: 57593, total_loss: 3.886275291442871
training step: 57594, total_loss: 1.0705037117004395
training step: 57595, total_loss: 0.2450345903635025
training step: 57596, total_loss: 1.4761345386505127
training step: 57597, total_loss: 0.005443404894322157
training step: 57598, total_loss: 1.5378727912902832
training step: 57599, total_loss: 1.3084471225738525
training step: 57600, total_loss: 2.22891902923584
training step: 57601, total_loss: 1.467974066734314
training step: 57602, total_loss: 5.914624214172363
training step: 57603, total_loss: 2.025420904159546
training step: 57604, total_loss: 3.3523483276367188
training step: 57605, total_loss: 3.998711109161377
training step: 57606, total_loss: 1.0089094638824463
training step: 57607, total_loss: 1.8089067935943604
training step: 57608, total_loss: 1.4387574195861816
training step: 57609, total_loss: 1.4781267642974854
training step: 57610, total_loss: 0.21083569526672363
training step: 57611, total_loss: 2.095743179321289
training step: 57612, total_loss: 2.0958118438720703
training step: 57613, total_loss: 3.229496955871582
training step: 57614, total_loss: 1.2474309206008911
training step: 57615, total_loss: 1.9515316486358643
training step: 57616, total_loss: 1.943809151649475
training step: 57617, total_loss: 3.135101556777954
training step: 57618, total_loss: 3.6956326961517334
training step: 57619, total_loss: 2.8506619930267334
training step: 57620, total_loss: 2.5064408779144287
training step: 57621, total_loss: 1.0111826658248901
training step: 57622, total_loss: 1.287834882736206
training step: 57623, total_loss: 1.3248209953308105
training step: 57624, total_loss: 4.5409393310546875
training step: 57625, total_loss: 1.3756284713745117
training step: 57626, total_loss: 4.4642815589904785
training step: 57627, total_loss: 2.8105039596557617
training step: 57628, total_loss: 0.34826749563217163
training step: 57629, total_loss: 3.107208251953125
training step: 57630, total_loss: 2.220644950866699
training step: 57631, total_loss: 2.8324477672576904
training step: 57632, total_loss: 1.3930420875549316
training step: 57633, total_loss: 1.6388436555862427
training step: 57634, total_loss: 1.2447984218597412
training step: 57635, total_loss: 1.0350265502929688
training step: 57636, total_loss: 0.09761379659175873
training step: 57637, total_loss: 0.006004249211400747
training step: 57638, total_loss: 0.6867562532424927
training step: 57639, total_loss: 0.9807599782943726
training step: 57640, total_loss: 0.005267568863928318
training step: 57641, total_loss: 0.6056003570556641
training step: 57642, total_loss: 0.1354486048221588
training step: 57643, total_loss: 3.0165791511535645
training step: 57644, total_loss: 2.333162784576416
training step: 57645, total_loss: 0.658068060874939
training step: 57646, total_loss: 1.2006266117095947
training step: 57647, total_loss: 2.2735323905944824
training step: 57648, total_loss: 1.9386404752731323
training step: 57649, total_loss: 1.7369003295898438
training step: 57650, total_loss: 1.4518241882324219
training step: 57651, total_loss: 1.8737602233886719
training step: 57652, total_loss: 0.2281726896762848
training step: 57653, total_loss: 0.10110408812761307
training step: 57654, total_loss: 2.243821859359741
training step: 57655, total_loss: 0.5450458526611328
training step: 57656, total_loss: 0.45699653029441833
training step: 57657, total_loss: 0.44930410385131836
training step: 57658, total_loss: 0.3249574899673462
training step: 57659, total_loss: 2.8399946689605713
training step: 57660, total_loss: 0.6852083206176758
training step: 57661, total_loss: 0.7911372780799866
training step: 57662, total_loss: 1.686875581741333
training step: 57663, total_loss: 0.9508482217788696
training step: 57664, total_loss: 4.078157424926758
training step: 57665, total_loss: 1.8508894443511963
training step: 57666, total_loss: 0.1398947387933731
training step: 57667, total_loss: 2.319160223007202
training step: 57668, total_loss: 2.0079331398010254
training step: 57669, total_loss: 1.2728652954101562
training step: 57670, total_loss: 2.2780470848083496
training step: 57671, total_loss: 0.9607322812080383
training step: 57672, total_loss: 0.9789482355117798
training step: 57673, total_loss: 1.6049280166625977
training step: 57674, total_loss: 2.145620107650757
training step: 57675, total_loss: 2.158825159072876
training step: 57676, total_loss: 0.006803298834711313
training step: 57677, total_loss: 0.1532973200082779
training step: 57678, total_loss: 0.4186059534549713
training step: 57679, total_loss: 2.6491878032684326
training step: 57680, total_loss: 0.4073587954044342
training step: 57681, total_loss: 0.8371766209602356
training step: 57682, total_loss: 4.282707691192627
training step: 57683, total_loss: 1.7801398038864136
training step: 57684, total_loss: 2.556922435760498
training step: 57685, total_loss: 0.03615687042474747
training step: 57686, total_loss: 0.601836085319519
training step: 57687, total_loss: 4.442965030670166
training step: 57688, total_loss: 3.7123961448669434
training step: 57689, total_loss: 2.91093111038208
training step: 57690, total_loss: 0.016927916556596756
training step: 57691, total_loss: 0.7474873065948486
training step: 57692, total_loss: 1.8640410900115967
training step: 57693, total_loss: 2.885468006134033
training step: 57694, total_loss: 0.2888835668563843
training step: 57695, total_loss: 0.4123539924621582
training step: 57696, total_loss: 2.8861677646636963
training step: 57697, total_loss: 0.805074155330658
training step: 57698, total_loss: 0.009607784450054169
training step: 57699, total_loss: 3.131274700164795
training step: 57700, total_loss: 1.8766851425170898
training step: 57701, total_loss: 0.795267641544342
training step: 57702, total_loss: 1.2474712133407593
training step: 57703, total_loss: 0.005601270589977503
training step: 57704, total_loss: 0.6184519529342651
training step: 57705, total_loss: 1.258486032485962
training step: 57706, total_loss: 0.3323631286621094
training step: 57707, total_loss: 2.0666422843933105
training step: 57708, total_loss: 0.2390068620443344
training step: 57709, total_loss: 1.865283489227295
training step: 57710, total_loss: 0.549057126045227
training step: 57711, total_loss: 0.9106839895248413
training step: 57712, total_loss: 1.907766580581665
training step: 57713, total_loss: 2.4283714294433594
training step: 57714, total_loss: 1.7777881622314453
training step: 57715, total_loss: 0.7114015817642212
training step: 57716, total_loss: 0.6307989954948425
training step: 57717, total_loss: 3.3054208755493164
training step: 57718, total_loss: 1.2849587202072144
training step: 57719, total_loss: 0.01746729016304016
training step: 57720, total_loss: 1.7331427335739136
training step: 57721, total_loss: 1.9306716918945312
training step: 57722, total_loss: 0.5354055762290955
training step: 57723, total_loss: 0.3598988950252533
training step: 57724, total_loss: 1.4509918689727783
training step: 57725, total_loss: 0.31514620780944824
training step: 57726, total_loss: 5.300568103790283
training step: 57727, total_loss: 1.3334369659423828
training step: 57728, total_loss: 0.7582364082336426
training step: 57729, total_loss: 0.02005246840417385
training step: 57730, total_loss: 1.784844160079956
training step: 57731, total_loss: 2.5207152366638184
training step: 57732, total_loss: 0.3013257384300232
training step: 57733, total_loss: 3.261951446533203
training step: 57734, total_loss: 0.35301950573921204
training step: 57735, total_loss: 0.15878546237945557
training step: 57736, total_loss: 0.5376903414726257
training step: 57737, total_loss: 0.7017538547515869
training step: 57738, total_loss: 1.0699448585510254
training step: 57739, total_loss: 0.9050188064575195
training step: 57740, total_loss: 0.7496158480644226
training step: 57741, total_loss: 1.716525673866272
training step: 57742, total_loss: 0.01778990775346756
training step: 57743, total_loss: 0.3373788595199585
training step: 57744, total_loss: 0.5724606513977051
training step: 57745, total_loss: 0.38852447271347046
training step: 57746, total_loss: 3.8106942176818848
training step: 57747, total_loss: 1.5094966888427734
training step: 57748, total_loss: 0.8834178447723389
training step: 57749, total_loss: 0.9144805669784546
training step: 57750, total_loss: 0.788769543170929
training step: 57751, total_loss: 2.162106990814209
training step: 57752, total_loss: 0.29383331537246704
training step: 57753, total_loss: 1.8216667175292969
training step: 57754, total_loss: 1.16121506690979
training step: 57755, total_loss: 2.1010818481445312
training step: 57756, total_loss: 2.248368978500366
training step: 57757, total_loss: 1.8396204710006714
training step: 57758, total_loss: 2.816751003265381
training step: 57759, total_loss: 0.15420210361480713
training step: 57760, total_loss: 1.0891733169555664
training step: 57761, total_loss: 3.37626314163208
training step: 57762, total_loss: 2.0666542053222656
training step: 57763, total_loss: 0.5230438709259033
training step: 57764, total_loss: 0.3607100546360016
training step: 57765, total_loss: 1.5004321336746216
training step: 57766, total_loss: 5.506829261779785
training step: 57767, total_loss: 2.0887022018432617
training step: 57768, total_loss: 0.12728163599967957
training step: 57769, total_loss: 6.042332172393799
training step: 57770, total_loss: 1.7858973741531372
training step: 57771, total_loss: 0.6383807063102722
training step: 57772, total_loss: 3.236436367034912
training step: 57773, total_loss: 1.8487639427185059
training step: 57774, total_loss: 1.4846432209014893
training step: 57775, total_loss: 1.2936840057373047
training step: 57776, total_loss: 2.6436753273010254
training step: 57777, total_loss: 0.7215022444725037
training step: 57778, total_loss: 0.12544575333595276
training step: 57779, total_loss: 0.9075488448143005
training step: 57780, total_loss: 1.2459931373596191
training step: 57781, total_loss: 0.2893844246864319
training step: 57782, total_loss: 1.5082554817199707
training step: 57783, total_loss: 1.177842617034912
training step: 57784, total_loss: 1.0541787147521973
training step: 57785, total_loss: 1.4718983173370361
training step: 57786, total_loss: 1.4942327737808228
training step: 57787, total_loss: 1.814342975616455
training step: 57788, total_loss: 1.5615506172180176
training step: 57789, total_loss: 1.444385290145874
training step: 57790, total_loss: 3.334319591522217
training step: 57791, total_loss: 2.5175395011901855
training step: 57792, total_loss: 2.6001486778259277
training step: 57793, total_loss: 2.1000561714172363
training step: 57794, total_loss: 0.9640271067619324
training step: 57795, total_loss: 4.782686710357666
training step: 57796, total_loss: 1.4047788381576538
training step: 57797, total_loss: 2.059937000274658
training step: 57798, total_loss: 0.8691052794456482
training step: 57799, total_loss: 2.3302559852600098
training step: 57800, total_loss: 0.010859627276659012
training step: 57801, total_loss: 0.24450835585594177
training step: 57802, total_loss: 0.28604695200920105
training step: 57803, total_loss: 0.0007265412714332342
training step: 57804, total_loss: 1.4126982688903809
training step: 57805, total_loss: 2.3630166053771973
training step: 57806, total_loss: 1.1329162120819092
training step: 57807, total_loss: 0.9031452536582947
training step: 57808, total_loss: 0.7416495084762573
training step: 57809, total_loss: 1.4922549724578857
training step: 57810, total_loss: 1.5658725500106812
training step: 57811, total_loss: 2.7225043773651123
training step: 57812, total_loss: 2.2323811054229736
training step: 57813, total_loss: 2.7867634296417236
training step: 57814, total_loss: 0.3068416714668274
training step: 57815, total_loss: 1.560725450515747
training step: 57816, total_loss: 0.19617098569869995
training step: 57817, total_loss: 1.476538896560669
training step: 57818, total_loss: 0.32542890310287476
training step: 57819, total_loss: 1.6829843521118164
training step: 57820, total_loss: 1.1563544273376465
training step: 57821, total_loss: 2.0698282718658447
training step: 57822, total_loss: 1.7931621074676514
training step: 57823, total_loss: 6.061098098754883
training step: 57824, total_loss: 3.4456400871276855
training step: 57825, total_loss: 3.347212553024292
training step: 57826, total_loss: 0.9926056861877441
training step: 57827, total_loss: 1.6829116344451904
training step: 57828, total_loss: 0.11241870373487473
training step: 57829, total_loss: 2.795912027359009
training step: 57830, total_loss: 3.0336575508117676
training step: 57831, total_loss: 0.13315579295158386
training step: 57832, total_loss: 0.1891118288040161
training step: 57833, total_loss: 2.602689743041992
training step: 57834, total_loss: 1.1292451620101929
training step: 57835, total_loss: 0.5762255787849426
training step: 57836, total_loss: 3.634861469268799
training step: 57837, total_loss: 4.024714946746826
training step: 57838, total_loss: 0.7454425692558289
training step: 57839, total_loss: 1.8403286933898926
training step: 57840, total_loss: 0.948157548904419
training step: 57841, total_loss: 0.3334691524505615
training step: 57842, total_loss: 2.9763340950012207
training step: 57843, total_loss: 2.2682197093963623
training step: 57844, total_loss: 2.178088903427124
training step: 57845, total_loss: 0.48366984724998474
training step: 57846, total_loss: 1.8458380699157715
training step: 57847, total_loss: 1.4545152187347412
training step: 57848, total_loss: 1.1870968341827393
training step: 57849, total_loss: 3.8030104637145996
training step: 57850, total_loss: 0.08505500853061676
training step: 57851, total_loss: 3.769822597503662
training step: 57852, total_loss: 2.4888916015625
training step: 57853, total_loss: 0.3306252360343933
training step: 57854, total_loss: 1.8165323734283447
training step: 57855, total_loss: 1.9115442037582397
training step: 57856, total_loss: 0.13014018535614014
training step: 57857, total_loss: 0.08135659992694855
training step: 57858, total_loss: 0.23925301432609558
training step: 57859, total_loss: 0.4963681697845459
training step: 57860, total_loss: 1.8887035846710205
training step: 57861, total_loss: 0.9547005891799927
training step: 57862, total_loss: 1.592729091644287
training step: 57863, total_loss: 0.08853509277105331
training step: 57864, total_loss: 1.817720651626587
training step: 57865, total_loss: 1.0795023441314697
training step: 57866, total_loss: 1.8472082614898682
training step: 57867, total_loss: 1.7034902572631836
training step: 57868, total_loss: 1.425897240638733
training step: 57869, total_loss: 0.060739099979400635
training step: 57870, total_loss: 1.3139396905899048
training step: 57871, total_loss: 0.21669985353946686
training step: 57872, total_loss: 3.438778877258301
training step: 57873, total_loss: 4.346631050109863
training step: 57874, total_loss: 0.12708744406700134
training step: 57875, total_loss: 3.206040382385254
training step: 57876, total_loss: 0.07428230345249176
training step: 57877, total_loss: 1.3659961223602295
training step: 57878, total_loss: 1.750523567199707
training step: 57879, total_loss: 2.868218421936035
training step: 57880, total_loss: 1.0540969371795654
training step: 57881, total_loss: 0.20184341073036194
training step: 57882, total_loss: 3.4768285751342773
training step: 57883, total_loss: 1.5794427394866943
training step: 57884, total_loss: 1.3756258487701416
training step: 57885, total_loss: 0.4371322989463806
training step: 57886, total_loss: 0.49425461888313293
training step: 57887, total_loss: 0.20238152146339417
training step: 57888, total_loss: 0.6030280590057373
training step: 57889, total_loss: 0.5303189754486084
training step: 57890, total_loss: 0.4104323089122772
training step: 57891, total_loss: 3.2582926750183105
training step: 57892, total_loss: 0.5376687049865723
training step: 57893, total_loss: 3.504974365234375
training step: 57894, total_loss: 0.025865726172924042
training step: 57895, total_loss: 0.3553043007850647
training step: 57896, total_loss: 1.92232084274292
training step: 57897, total_loss: 1.5712865591049194
training step: 57898, total_loss: 0.9850613474845886
training step: 57899, total_loss: 2.7822232246398926
training step: 57900, total_loss: 2.805079460144043
training step: 57901, total_loss: 2.0323073863983154
training step: 57902, total_loss: 2.408909320831299
training step: 57903, total_loss: 0.024740464985370636
training step: 57904, total_loss: 0.9910043478012085
training step: 57905, total_loss: 1.3803811073303223
training step: 57906, total_loss: 0.057950519025325775
training step: 57907, total_loss: 2.824887275695801
training step: 57908, total_loss: 1.4376907348632812
training step: 57909, total_loss: 2.497631311416626
training step: 57910, total_loss: 0.826687216758728
training step: 57911, total_loss: 0.8416248559951782
training step: 57912, total_loss: 2.6961426734924316
training step: 57913, total_loss: 1.81759512424469
training step: 57914, total_loss: 2.4378855228424072
training step: 57915, total_loss: 0.05319578945636749
training step: 57916, total_loss: 0.3240393400192261
training step: 57917, total_loss: 0.31898176670074463
training step: 57918, total_loss: 2.3142647743225098
training step: 57919, total_loss: 1.406612515449524
training step: 57920, total_loss: 1.0842702388763428
training step: 57921, total_loss: 2.530832529067993
training step: 57922, total_loss: 1.4784467220306396
training step: 57923, total_loss: 1.0806262493133545
training step: 57924, total_loss: 0.129705011844635
training step: 57925, total_loss: 1.522017002105713
training step: 57926, total_loss: 3.751183032989502
training step: 57927, total_loss: 1.6702880859375
training step: 57928, total_loss: 0.9620465636253357
training step: 57929, total_loss: 1.714949131011963
training step: 57930, total_loss: 1.2857139110565186
training step: 57931, total_loss: 3.2910380363464355
training step: 57932, total_loss: 2.1302216053009033
training step: 57933, total_loss: 1.7203409671783447
training step: 57934, total_loss: 0.47590070962905884
training step: 57935, total_loss: 1.1000639200210571
training step: 57936, total_loss: 5.807096481323242
training step: 57937, total_loss: 1.2597979307174683
training step: 57938, total_loss: 1.9744144678115845
training step: 57939, total_loss: 3.1800074577331543
training step: 57940, total_loss: 2.9565773010253906
training step: 57941, total_loss: 3.9572062492370605
training step: 57942, total_loss: 3.910679817199707
training step: 57943, total_loss: 0.8324750065803528
training step: 57944, total_loss: 0.45094922184944153
training step: 57945, total_loss: 5.9327473640441895
training step: 57946, total_loss: 0.14711758494377136
training step: 57947, total_loss: 0.8730106353759766
training step: 57948, total_loss: 1.7191927433013916
training step: 57949, total_loss: 2.3594491481781006
training step: 57950, total_loss: 1.6542026996612549
training step: 57951, total_loss: 1.1356778144836426INFO:tensorflow:Writing predictions to: test_output/predictions_58000.json
INFO:tensorflow:Writing nbest to: test_output/nbest_predictions_58000.json

training step: 57952, total_loss: 0.1493944227695465
training step: 57953, total_loss: 3.2389535903930664
training step: 57954, total_loss: 2.2116286754608154
training step: 57955, total_loss: 2.157136917114258
training step: 57956, total_loss: 0.10845893621444702
training step: 57957, total_loss: 4.014803409576416
training step: 57958, total_loss: 2.568809986114502
training step: 57959, total_loss: 0.09308929741382599
training step: 57960, total_loss: 1.7367184162139893
training step: 57961, total_loss: 0.675230860710144
training step: 57962, total_loss: 0.2763064205646515
training step: 57963, total_loss: 1.9762498140335083
training step: 57964, total_loss: 0.8586047291755676
training step: 57965, total_loss: 1.0331746339797974
training step: 57966, total_loss: 3.305156707763672
training step: 57967, total_loss: 1.713309407234192
training step: 57968, total_loss: 2.0956454277038574
training step: 57969, total_loss: 0.44976839423179626
training step: 57970, total_loss: 1.9729804992675781
training step: 57971, total_loss: 4.385607719421387
training step: 57972, total_loss: 1.3367379903793335
training step: 57973, total_loss: 1.0562900304794312
training step: 57974, total_loss: 1.2920910120010376
training step: 57975, total_loss: 2.4020419120788574
training step: 57976, total_loss: 2.0285744667053223
training step: 57977, total_loss: 1.6632440090179443
training step: 57978, total_loss: 0.8656351566314697
training step: 57979, total_loss: 0.6324129104614258
training step: 57980, total_loss: 0.7260897755622864
training step: 57981, total_loss: 1.083292007446289
training step: 57982, total_loss: 2.3684144020080566
training step: 57983, total_loss: 0.2748678922653198
training step: 57984, total_loss: 2.1373541355133057
training step: 57985, total_loss: 1.4254696369171143
training step: 57986, total_loss: 1.8690249919891357
training step: 57987, total_loss: 1.9425501823425293
training step: 57988, total_loss: 1.984895944595337
training step: 57989, total_loss: 1.4581429958343506
training step: 57990, total_loss: 0.9648635387420654
training step: 57991, total_loss: 1.6770780086517334
training step: 57992, total_loss: 1.7200214862823486
training step: 57993, total_loss: 3.38016414642334
training step: 57994, total_loss: 2.031740427017212
training step: 57995, total_loss: 0.7866568565368652
training step: 57996, total_loss: 0.5692057609558105
training step: 57997, total_loss: 1.4221285581588745
training step: 57998, total_loss: 2.2353854179382324
training step: 57999, total_loss: 1.3714091777801514
training step: 58000, total_loss: 4.843984603881836
epoch finished! shuffle=False
evaluation: 22000, total_loss: 1.8208928108215332, f1: 57.35163934935475, followup: 39.3123383538719, yesno: 73.31507682945383, heq: 52.54830366651453, dheq: 3.3

Model saved in path test_output//model_58000.ckpt
